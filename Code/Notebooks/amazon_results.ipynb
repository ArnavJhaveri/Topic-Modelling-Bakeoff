{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "BERTopic\n",
      "Run: 1\n",
      "Linear model score: 0.284825\n",
      "Linear model RMSE: 1.1976948175902427\n",
      "Linear model R2: 0.28706706554319295\n",
      "Linear model MAE: 0.9991154128697017\n",
      "\n",
      "Logistic model: 0.31805\n",
      "Rf model: nan\n",
      "Old Logistic model: 0.799995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# amazon\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "dataset = 'amazon' # folder and dataset name\n",
    "# cols = ['gender', 'age', 'politics'] # outcome columns\n",
    "cols = ['label_x'] # outcome columns\n",
    "outcome = 'label'\n",
    "\n",
    "logit = True\n",
    "\n",
    "df = pd.read_csv(\"data/\" + dataset + \".csv\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# for model in ['GensimLDA', 'BERTopic', 'NMF', 'Mallet_LDA', 'CTM']:\n",
    "for model in ['BERTopic']:\n",
    "    print(model)\n",
    "    scores = []\n",
    "    scores_logit = []\n",
    "    scores_rf = []\n",
    "    scores_old = []\n",
    "\n",
    "    rmses = []\n",
    "    r2s = []\n",
    "    maes = []\n",
    "\n",
    "    for run in range(1, 2):\n",
    "\n",
    "        print(\"Run: \" + str(run))\n",
    "\n",
    "        # loading in distributions that were saved during topic extraction\n",
    "        test_distribution = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_topic_distribution_test.pkl', 'rb'))\n",
    "        train_distribution = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_topic_distribution_train.pkl', 'rb'))\n",
    "        train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "        test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "        topics = []\n",
    "        with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                topic_list = [item.strip() for item in row if item.strip()]\n",
    "                topics.append(topic_list)\n",
    "\n",
    "        temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "        distribution = np.concatenate([train_distribution, test_distribution]) # concatenating train and test distributions\n",
    "\n",
    "        merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id', 'message'] + cols]\n",
    "        merged.columns = ['message_id', 'message', 'label']\n",
    "\n",
    "        X = distribution\n",
    "        y = merged[outcome].reset_index(drop=True) # the outcome we care about\n",
    "\n",
    "        # # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "        X_train = X[:round(0.80 * len(X))]\n",
    "        X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "        y_train = y[:round(0.80 * len(X))]\n",
    "        y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "        lr_model = LinearRegression().fit(X_train, y_train) \n",
    "        y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "        pred_rounded = []\n",
    "\n",
    "        for i in np.round(y_pred_test):\n",
    "            min = y_test.min()\n",
    "            max = y_test.max()\n",
    "            if i >= min and i <= max:\n",
    "                pred_rounded.append(i)\n",
    "            elif i > max:\n",
    "                pred_rounded.append(max)\n",
    "            else:\n",
    "                pred_rounded.append(min)\n",
    "\n",
    "        score = sklearn.metrics.accuracy_score(pred_rounded, list(y_test))\n",
    "        scores.append(score)\n",
    "\n",
    "        rmse_test = mean_squared_error(y_true=y_test, y_pred=y_pred_test, squared=False)\n",
    "        r2_test = r2_score(y_true=y_test, y_pred=y_pred_test)\n",
    "        mae_test = mean_absolute_error(y_true=y_test, y_pred=y_pred_test)\n",
    "\n",
    "        rmses.append(rmse_test)\n",
    "        r2s.append(r2_test)\n",
    "        maes.append(mae_test)\n",
    "\n",
    "        logit_model = LogisticRegression().fit(X_train, y_train)\n",
    "        pred_logit = logit_model.predict(X_test)\n",
    "        score_logit = sklearn.metrics.accuracy_score(pred_logit, list(y_test))\n",
    "        scores_logit.append(score_logit)\n",
    "\n",
    "        # rf = RandomForestClassifier(random_state = 42).fit(X_train, y_train)\n",
    "        # pred_rf = rf.predict(X_test)\n",
    "        # score_rf = sklearn.metrics.accuracy_score(pred_rf, list(y_test))\n",
    "        # scores_rf.append(score_rf)\n",
    "\n",
    "        dummy_train = pd.get_dummies(train['label'], dtype=int)\n",
    "        dummy_test = pd.get_dummies(test['label'], dtype=int)\n",
    "        dummy_train.columns = [\"label \" + str(i) for i in range(len(dummy_train.columns))]\n",
    "        dummy_test.columns = [\"label \" + str(i) for i in range(len(dummy_test.columns))]\n",
    "\n",
    "        accuracies = []\n",
    "        r2s_old = []\n",
    "        for i, label_col in enumerate(list(dummy_train)):\n",
    "            lr_model_old = LogisticRegression().fit(X_train, dummy_train[label_col])\n",
    "            accuracies.append(lr_model_old.score(X_test, dummy_test[label_col]))\n",
    "            r2s_old.append(r2_score(dummy_test[label_col], lr_model_old.predict(X_test)))\n",
    "        \n",
    "        accuracies = [np.mean(accuracies)] + accuracies + [np.mean(r2s_old)]\n",
    "        scores_old.append(accuracies[0])\n",
    "        \n",
    "    print(\"Linear model score: \" + str(np.mean(scores)))\n",
    "    print(\"Linear model RMSE: \" + str(np.mean(rmses)))\n",
    "    print(\"Linear model R2: \" + str(np.mean(r2s)))\n",
    "    print(\"Linear model MAE: \" + str(np.mean(maes)))\n",
    "    print()\n",
    "    print(\"Logistic model: \" + str(np.mean(scores_logit)))\n",
    "    print(\"Rf model: \" + str(np.mean(scores_rf)))\n",
    "    print(\"Old Logistic model: \" + str(np.mean(scores_old)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 1.5376\n",
      "Epoch [20/300], Loss: 1.5059\n",
      "Epoch [30/300], Loss: 1.4882\n",
      "Epoch [40/300], Loss: 1.4772\n",
      "Epoch [50/300], Loss: 1.4696\n",
      "Epoch [60/300], Loss: 1.4641\n",
      "Epoch [70/300], Loss: 1.4599\n",
      "Epoch [80/300], Loss: 1.4565\n",
      "Epoch [90/300], Loss: 1.4538\n",
      "Epoch [100/300], Loss: 1.4514\n",
      "Epoch [110/300], Loss: 1.4494\n",
      "Epoch [120/300], Loss: 1.4476\n",
      "Epoch [130/300], Loss: 1.4460\n",
      "Epoch [140/300], Loss: 1.4445\n",
      "Epoch [150/300], Loss: 1.4432\n",
      "Epoch [160/300], Loss: 1.4419\n",
      "Epoch [170/300], Loss: 1.4407\n",
      "Epoch [180/300], Loss: 1.4396\n",
      "Epoch [190/300], Loss: 1.4385\n",
      "Epoch [200/300], Loss: 1.4375\n",
      "Epoch [210/300], Loss: 1.4365\n",
      "Epoch [220/300], Loss: 1.4356\n",
      "Epoch [230/300], Loss: 1.4347\n",
      "Epoch [240/300], Loss: 1.4338\n",
      "Epoch [250/300], Loss: 1.4329\n",
      "Epoch [260/300], Loss: 1.4321\n",
      "Epoch [270/300], Loss: 1.4313\n",
      "Epoch [280/300], Loss: 1.4306\n",
      "Epoch [290/300], Loss: 1.4298\n",
      "Epoch [300/300], Loss: 1.4291\n",
      "Train Accuracy Score: 0.41135\n",
      "Test Accuracy Score: 0.4144\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_epochs = 300\n",
    "lr = 0.003\n",
    "\n",
    "# Convert arrays to torch tensors\n",
    "X_train_tensor = torch.tensor(np.array(X_train).astype(np.float32))\n",
    "y_train_tensor = torch.tensor(np.array(y_train).astype(np.longlong))\n",
    "X_test_tensor = torch.tensor(np.array(X_test).astype(np.float32))\n",
    "y_test_tensor = torch.tensor(np.array(y_test).astype(np.longlong))\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, num_classes)  # First linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))  # Assuming y_train contains all classes\n",
    "model = LogisticRegressionModel(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # This includes softmax\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor)\n",
    "    _, predicted_train = torch.max(y_pred_train.data, 1)\n",
    "\n",
    "# Calculate accuracy\n",
    "score_logit_train = accuracy_score(predicted_train.numpy(), y_train)\n",
    "print(f'Train Accuracy Score: {score_logit_train}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_tensor)\n",
    "    _, predicted = torch.max(y_pred_test.data, 1)\n",
    "\n",
    "# Calculate accuracy\n",
    "score_logit = accuracy_score(predicted.numpy(), y_test)\n",
    "print(f'Test Accuracy Score: {score_logit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 4    8126\n",
       " 2    8081\n",
       " 0    8062\n",
       " 1    7960\n",
       " 3    7771\n",
       " Name: count, dtype: int64,\n",
       " label\n",
       " 3    32229\n",
       " 1    32040\n",
       " 0    31938\n",
       " 2    31919\n",
       " 4    31874\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts(), pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    10361\n",
       " 4     9842\n",
       " 3     7653\n",
       " 1     7009\n",
       " 2     5135\n",
       " Name: count, dtype: int64,\n",
       " 0    41465\n",
       " 4    39498\n",
       " 3    30700\n",
       " 1    28078\n",
       " 2    20259\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predicted).value_counts(), pd.Series(predicted_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
