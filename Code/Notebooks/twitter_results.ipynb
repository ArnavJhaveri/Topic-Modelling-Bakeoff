{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 topics (age)\n",
    "# Train RMSE: 9.095497197568372\n",
    "# Test RMSE: 10.460199129743534\n",
    "# R2 Train: 0.3698023817559233\n",
    "# R2 Test: 0.230573536354301\n",
    "\n",
    "# 2000 topics (age)\n",
    "# Train RMSE: 8.611894270144457\n",
    "# Test RMSE: 10.43693839611697\n",
    "# R2 Train: 0.4350353857658916\n",
    "# R2 Test: 0.2339917359910033\n",
    "\n",
    "# 1000 topics (isFemale)\n",
    "# Train Error: 0.17829181494661916\n",
    "# Test Error: 0.23755334281650076\n",
    "\n",
    "# 2000 topics (isFemale)\n",
    "# Train Error: 0.14270462633451952\n",
    "# Test Error: 0.2304409672830725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "\n",
      "BERTopic\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 524.6688842773438\n",
      "Epoch 51, Loss: 88.45242309570312\n",
      "Epoch 101, Loss: 72.90101623535156\n",
      "Epoch 151, Loss: 65.9138412475586\n",
      "Epoch 201, Loss: 61.97526168823242\n",
      "Epoch 251, Loss: 59.255821228027344\n",
      "Epoch 301, Loss: 57.12215042114258\n",
      "Epoch 351, Loss: 55.32066345214844\n",
      "Epoch 401, Loss: 53.732112884521484\n",
      "Epoch 451, Loss: 52.293758392333984\n",
      "Epoch 501, Loss: 50.96934127807617\n",
      "Epoch 551, Loss: 49.73602294921875\n",
      "Epoch 601, Loss: 48.578590393066406\n",
      "Epoch 651, Loss: 47.48653793334961\n",
      "Epoch 701, Loss: 46.452674865722656\n",
      "Epoch 751, Loss: 45.471519470214844\n",
      "Epoch 801, Loss: 44.53873062133789\n",
      "Epoch 851, Loss: 43.65091323852539\n",
      "Epoch 901, Loss: 42.80537796020508\n",
      "Epoch 951, Loss: 41.999977111816406\n",
      "Train RMSE: 8.611894270144457\n",
      "Test RMSE: 10.43693839611697\n",
      "R2 Train: 0.4350353857658916\n",
      "R2 Test: 0.2339917359910033\n",
      "\n",
      "\n",
      "BERTopic Average Train RMSE: 8.611894270144457\n",
      "BERTopic Average Test RMSE: 10.43693839611697\n",
      "BERTopic Average R2 Train: 0.4350353857658916\n",
      "BERTopic Average R2 Test: 0.2339917359910033\n",
      "\n",
      "politics\n",
      "\n",
      "BERTopic\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.09605073928833\n",
      "Epoch 51, Loss: 3.797977924346924\n",
      "Epoch 101, Loss: 3.6092982292175293\n",
      "Epoch 151, Loss: 3.4648385047912598\n",
      "Epoch 201, Loss: 3.3458285331726074\n",
      "Epoch 251, Loss: 3.243290424346924\n",
      "Epoch 301, Loss: 3.153444766998291\n",
      "Epoch 351, Loss: 3.0735819339752197\n",
      "Epoch 401, Loss: 3.0016865730285645\n",
      "Epoch 451, Loss: 2.9362504482269287\n",
      "Epoch 501, Loss: 2.8762145042419434\n",
      "Epoch 551, Loss: 2.820812463760376\n",
      "Epoch 601, Loss: 2.7694547176361084\n",
      "Epoch 651, Loss: 2.721669912338257\n",
      "Epoch 701, Loss: 2.6770777702331543\n",
      "Epoch 751, Loss: 2.635355234146118\n",
      "Epoch 801, Loss: 2.5962271690368652\n",
      "Epoch 851, Loss: 2.559454917907715\n",
      "Epoch 901, Loss: 2.5248260498046875\n",
      "Epoch 951, Loss: 2.492149829864502\n",
      "Train RMSE: 1.7211242156658042\n",
      "Test RMSE: 1.8389448388536138\n",
      "R2 Train: 0.05985558921383949\n",
      "R2 Test: 0.014951440756550416\n",
      "\n",
      "\n",
      "BERTopic Average Train RMSE: 1.7211242156658042\n",
      "BERTopic Average Test RMSE: 1.8389448388536138\n",
      "BERTopic Average R2 Train: 0.05985558921383949\n",
      "BERTopic Average R2 Test: 0.014951440756550416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# twitter\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# change this\n",
    "dataset = 'twitter' # folder and dataset name\n",
    "\n",
    "cols = ['age', 'politics']\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "# START\n",
    "\n",
    "train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "df = pd.read_csv('data/' + dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "\n",
    "merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "all_user_ids = list(user_ids.keys())\n",
    "\n",
    "labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "\n",
    "all_ys = []\n",
    "for key in user_ids.keys():\n",
    "    all_ys.append(labels_data[labels_data['user_id'] == key][cols].iloc[0])\n",
    "\n",
    "for outcome in cols:\n",
    "    print(outcome)\n",
    "    print()\n",
    "\n",
    "    y = [i[outcome] for i in all_ys]\n",
    "    \n",
    "    # REGRESSION\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for model in ['BERTopic']:\n",
    "        print(model)\n",
    "        \n",
    "        train_error_list = []\n",
    "        test_error_list = []\n",
    "\n",
    "        r2s_train = []\n",
    "        r2s_test = []\n",
    "\n",
    "        for run in range(1, 3):\n",
    "            print(\"Run: \" + str(run))\n",
    "            print()\n",
    "\n",
    "            # topics = []\n",
    "            # with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_2000.txt', 'r') as f:\n",
    "            #     reader = csv.reader(f)\n",
    "            #     for row in reader:\n",
    "            #         topic_list = [item.strip() for item in row if item.strip()]\n",
    "            #         topics.append(topic_list)\n",
    "\n",
    "            X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "            # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "            X_train = X[:round(0.80 * len(X))]\n",
    "            X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "            y_train = y[:round(0.80 * len(X))]\n",
    "            y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            X_test = np.array(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            torch.manual_seed(42) # for reproducibility\n",
    "\n",
    "            X_train_NN = torch.tensor(X_train, dtype = torch.float32)\n",
    "            y_train_NN = torch.tensor(y_train, dtype = torch.float32).reshape(-1, 1)\n",
    "            X_test_NN = torch.tensor(X_test, dtype = torch.float32)\n",
    "            y_test_NN = torch.tensor(y_test, dtype = torch.float32).reshape(-1, 1)\n",
    "\n",
    "            model_NN = nn.Sequential(\n",
    "                nn.Linear(X_train_NN.shape[1], 1),\n",
    "            )\n",
    "\n",
    "            loss_fn = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model_NN.parameters(), lr = lr)\n",
    "\n",
    "            n_epochs = 1000\n",
    "            batch_size = 32\n",
    "            batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "            best_mse = np.inf\n",
    "            best_weights = None\n",
    "            history = []\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                model_NN.train()\n",
    "                with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                    bar.set_description(f\"Epoch {epoch}\")\n",
    "                    for start in bar:\n",
    "\n",
    "                        X_batch = X_train_NN[start : start+batch_size]\n",
    "                        y_batch = y_train_NN[start : start+batch_size]\n",
    "\n",
    "                        y_pred = model_NN(X_batch)\n",
    "                        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                        bar.set_postfix(mse=float(loss))\n",
    "\n",
    "                model_NN.eval()\n",
    "                y_pred = model_NN(X_test_NN)\n",
    "                mse = loss_fn(y_pred, y_test_NN)\n",
    "                mse = float(mse)\n",
    "                history.append(mse)\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_weights = copy.deepcopy(model_NN.state_dict())\n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "            model_NN.load_state_dict(best_weights)\n",
    "\n",
    "            y_pred_train = model_NN(X_train_NN).detach().numpy()\n",
    "            y_pred_test = model_NN(X_test_NN).detach().numpy()\n",
    "\n",
    "            train_error = np.sqrt(mean_squared_error(y_pred_train, y_train))\n",
    "            test_error = np.sqrt(mean_squared_error(y_pred_test, y_test))\n",
    "\n",
    "            train_error_list.append(train_error)\n",
    "            test_error_list.append(test_error)\n",
    "\n",
    "            print(f'Train RMSE: {train_error}')\n",
    "            print(f'Test RMSE: {test_error}')\n",
    "\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "            r2s_train.append(r2_train)\n",
    "            r2s_test.append(r2_test)\n",
    "            \n",
    "            print(f'R2 Train: {r2_train}')\n",
    "            print(f'R2 Test: {r2_test}')\n",
    "\n",
    "            all_scores[model] = {\n",
    "                'Train RMSE': train_error_list,\n",
    "                'Test RMSE': test_error_list,\n",
    "                'R2 Train': r2s_train,\n",
    "                'R2 Test': r2s_test,\n",
    "\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': y_pred_train,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_pred_test\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    for m in all_scores.keys():\n",
    "        print(f'{m} Average Train RMSE: {np.mean(all_scores[m][\"Train RMSE\"])}')\n",
    "        print(f'{m} Average Test RMSE: {np.mean(all_scores[m][\"Test RMSE\"])}')\n",
    "        print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "        print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "        print()\n",
    "\n",
    "    # with open('all_results/' + dataset + '_all_scores_' + outcome + '_NN.pkl', 'wb') as f:\n",
    "    #     pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic\n",
      "Run: 2\n",
      "\n",
      "Epoch [10/500], Loss: 0.6279\n",
      "Epoch [20/500], Loss: 0.5996\n",
      "Epoch [30/500], Loss: 0.5756\n",
      "Epoch [40/500], Loss: 0.5548\n",
      "Epoch [50/500], Loss: 0.5366\n",
      "Epoch [60/500], Loss: 0.5206\n",
      "Epoch [70/500], Loss: 0.5064\n",
      "Epoch [80/500], Loss: 0.4937\n",
      "Epoch [90/500], Loss: 0.4822\n",
      "Epoch [100/500], Loss: 0.4718\n",
      "Epoch [110/500], Loss: 0.4624\n",
      "Epoch [120/500], Loss: 0.4538\n",
      "Epoch [130/500], Loss: 0.4459\n",
      "Epoch [140/500], Loss: 0.4386\n",
      "Epoch [150/500], Loss: 0.4320\n",
      "Epoch [160/500], Loss: 0.4258\n",
      "Epoch [170/500], Loss: 0.4201\n",
      "Epoch [180/500], Loss: 0.4147\n",
      "Epoch [190/500], Loss: 0.4098\n",
      "Epoch [200/500], Loss: 0.4051\n",
      "Epoch [210/500], Loss: 0.4008\n",
      "Epoch [220/500], Loss: 0.3967\n",
      "Epoch [230/500], Loss: 0.3929\n",
      "Epoch [240/500], Loss: 0.3892\n",
      "Epoch [250/500], Loss: 0.3858\n",
      "Epoch [260/500], Loss: 0.3826\n",
      "Epoch [270/500], Loss: 0.3795\n",
      "Epoch [280/500], Loss: 0.3765\n",
      "Epoch [290/500], Loss: 0.3737\n",
      "Epoch [300/500], Loss: 0.3711\n",
      "Epoch [310/500], Loss: 0.3685\n",
      "Epoch [320/500], Loss: 0.3660\n",
      "Epoch [330/500], Loss: 0.3637\n",
      "Epoch [340/500], Loss: 0.3614\n",
      "Epoch [350/500], Loss: 0.3593\n",
      "Epoch [360/500], Loss: 0.3572\n",
      "Epoch [370/500], Loss: 0.3552\n",
      "Epoch [380/500], Loss: 0.3532\n",
      "Epoch [390/500], Loss: 0.3513\n",
      "Epoch [400/500], Loss: 0.3495\n",
      "Epoch [410/500], Loss: 0.3478\n",
      "Epoch [420/500], Loss: 0.3460\n",
      "Epoch [430/500], Loss: 0.3444\n",
      "Epoch [440/500], Loss: 0.3428\n",
      "Epoch [450/500], Loss: 0.3412\n",
      "Epoch [460/500], Loss: 0.3397\n",
      "Epoch [470/500], Loss: 0.3382\n",
      "Epoch [480/500], Loss: 0.3367\n",
      "Epoch [490/500], Loss: 0.3353\n",
      "Epoch [500/500], Loss: 0.3339\n",
      "Train Error: 0.14270462633451952\n",
      "Test Error: 0.2304409672830725\n",
      "\n",
      "R2 Train: 0.36988610182836545\n",
      "R2 Test: -0.0589121338912133\n",
      "\n",
      "\n",
      "BERTopic Average Train Error: 0.14270462633451952\n",
      "BERTopic Average Test Error: 0.2304409672830725\n",
      "BERTopic Average R2 Train: 0.36988610182836545\n",
      "BERTopic Average R2 Test: -0.0589121338912133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# change this\n",
    "dataset = 'twitter' # folder and dataset name\n",
    "cols = ['gender'] # outcome columns\n",
    "outcome = 'gender'\n",
    "buckets = 4\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "df = pd.read_csv('data/' + dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "\n",
    "merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "all_user_ids = list(user_ids.keys())\n",
    "\n",
    "labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "y = [] # the outcome we care about\n",
    "\n",
    "for key in user_ids.keys():\n",
    "    y.append(labels_data[labels_data['user_id'] == key][outcome].iloc[0])\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "for model in ['BERTopic']:\n",
    "    print(model)\n",
    "\n",
    "    train_error_list = []\n",
    "    test_error_list = []\n",
    "\n",
    "    r2s_train = []\n",
    "    r2s_test = []\n",
    "\n",
    "    if model == 'GensimLDA':\n",
    "        lr = 0.5\n",
    "    elif model == 'BERTopic':\n",
    "        lr = 0.05\n",
    "    elif model == 'NMF':\n",
    "        lr = 0.05\n",
    "    elif model == 'Mallet_LDA':\n",
    "        lr = 0.1\n",
    "    else:\n",
    "        lr = 0.1\n",
    "\n",
    "    for run in range(1, 3):\n",
    "        print(\"Run: \" + str(run))\n",
    "        print()\n",
    "\n",
    "        # topics = []\n",
    "        # with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_2000.txt', 'r') as f:\n",
    "        #     reader = csv.reader(f)\n",
    "        #     for row in reader:\n",
    "        #         topic_list = [item.strip() for item in row if item.strip()]\n",
    "        #         topics.append(topic_list)\n",
    "\n",
    "        X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "        # # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "        X_train = X[:round(0.80 * len(X))]\n",
    "        X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "        y_train = y[:round(0.80 * len(X))]\n",
    "        y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        X_test = np.array(X_test)\n",
    "        y_train = np.array(y_train)\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "        train_users = all_user_ids[:round(0.80 * len(X))]\n",
    "        test_users = all_user_ids[round(0.80 * len(X)):]\n",
    "\n",
    "        # Convert arrays to torch tensors\n",
    "        X_train_tensor = torch.tensor(np.array(X_train).astype(np.float32))\n",
    "        y_train_tensor = torch.tensor(np.array(y_train).astype(np.longlong))  # Use long for classification\n",
    "        X_test_tensor = torch.tensor(np.array(X_test).astype(np.float32))\n",
    "        y_test_tensor = torch.tensor(np.array(y_test).astype(np.longlong))\n",
    "\n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Define the model\n",
    "        class LogisticRegressionModel(nn.Module):\n",
    "            def __init__(self, input_size, num_classes):\n",
    "                super(LogisticRegressionModel, self).__init__()\n",
    "                self.layer1 = nn.Linear(input_size, num_classes)\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.layer1(x)\n",
    "\n",
    "        input_size = X_train.shape[1]\n",
    "        num_classes = len(np.unique(y_train))  # Assuming y_train contains all classes\n",
    "        logit_model = LogisticRegressionModel(input_size, num_classes)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()  # This includes softmax\n",
    "        optimizer = optim.Adam(logit_model.parameters(), lr=lr)\n",
    "\n",
    "        loss_values = []\n",
    "        early_stopping_triggered = False\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = logit_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item() * inputs.size(0) \n",
    "            \n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            loss_values.append(epoch_loss)\n",
    "\n",
    "            if epoch >= 50:\n",
    "                # Calculate the percentage change in loss\n",
    "                loss_change = (loss_values[epoch - 50] - loss_values[epoch]) / loss_values[epoch - 50]\n",
    "                \n",
    "                # If change in loss is less than 1%, stop training\n",
    "                if abs(loss_change) < 0.005:\n",
    "                    print(f'Early stopping at epoch {epoch+1} due to minimal loss improvement.')\n",
    "                    early_stopping_triggered = True\n",
    "                    break\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Predict on the test set\n",
    "        logit_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = logit_model(X_train_tensor)\n",
    "            _, predicted_train = torch.max(y_pred_train.data, 1)\n",
    "\n",
    "            y_pred_test = logit_model(X_test_tensor)\n",
    "            _, predicted_test = torch.max(y_pred_test.data, 1)\n",
    "\n",
    "\n",
    "        train_error = 1 - accuracy_score(predicted_train, y_train)\n",
    "        test_error = 1 - accuracy_score(predicted_test, y_test)\n",
    "\n",
    "        train_error_list.append(train_error)\n",
    "        test_error_list.append(test_error)\n",
    "        \n",
    "        print(f'Train Error: {train_error}')\n",
    "        print(f'Test Error: {test_error}')\n",
    "\n",
    "        print()\n",
    "\n",
    "        r2_train = r2_score(y_train, predicted_train)\n",
    "        r2_test = r2_score(y_test, predicted_test)\n",
    "\n",
    "        r2s_train.append(r2_train)\n",
    "        r2s_test.append(r2_test)\n",
    "        \n",
    "        print(f'R2 Train: {r2_train}')\n",
    "        print(f'R2 Test: {r2_test}')\n",
    "\n",
    "        all_scores[model] = {\n",
    "            'Train Error': train_error_list,\n",
    "            'Test Error': test_error_list,\n",
    "            'R2 Train': r2s_train,\n",
    "            'R2 Test': r2s_test,\n",
    "\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': predicted_train.numpy(),\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': predicted_test.numpy()\n",
    "        }\n",
    "\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "for m in all_scores.keys():\n",
    "    print(f'{m} Average Train Error: {np.mean(all_scores[m][\"Train Error\"])}')\n",
    "    print(f'{m} Average Test Error: {np.mean(all_scores[m][\"Test Error\"])}')\n",
    "    print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "    print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "    print()\n",
    "\n",
    "# with open('all_results/' + dataset + '_all_scores_' + outcome + '.pkl', 'wb') as f:\n",
    "#      pickle.dump(all_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
