{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import random\n",
    "\n",
    "client = OpenAI(api_key='')\n",
    "\n",
    "datasets = ['twitter', 'facebook_2020', 'yelp_reviews', 'amazon', 'bbc_news', '20_newsgroup', 'imdb']\n",
    "models = ['BERTopic', 'CTM', 'GensimLDA', 'NMF', 'Mallet_LDA']\n",
    "paths = [data + '/' + model + '/run_' + str(run) + '/topics_100.txt' for data in datasets for model in models for run in range(1, 6)]\n",
    "\n",
    "# paths has all possible datasets, models, and runs (8 * 5 * 5 = 200 paths)\n",
    "\n",
    "topics_per_run = 20\n",
    "words_per_run = 5\n",
    "\n",
    "output = []\n",
    "\n",
    "for path in paths:\n",
    "\n",
    "  f = open(path, \"r\")\n",
    "  list_f = list(f)\n",
    "  list_f = [i for i in list_f if len(i) > 10] # don't include weird, shorter topics\n",
    "\n",
    "  if \"BERTopic\" in path:\n",
    "    list_f = list_f[1:] # topic 0 is not useful in BERTopic\n",
    "    \n",
    "  topics = [[i.strip() for i in topic.split(\",\")] for topic in random.sample(list_f, topics_per_run)]\n",
    "  topics_4 = [topic[0:words_per_run - 1] for topic in topics] # -1 since intruder word\n",
    "\n",
    "  intruder_words = []\n",
    "  returns = []\n",
    "\n",
    "  for i, topic in enumerate(topics_4):\n",
    "\n",
    "    intruder_topic_index = random.randint(0, 4)\n",
    "    intruder_topic = topics_4[intruder_topic_index]\n",
    "    intruder_words_excluded = [word for word in topics[intruder_topic_index][0:20] if word not in topic]\n",
    "    intruder_word = random.choice(intruder_words_excluded)\n",
    "    topic.append(intruder_word)\n",
    "    intruder_words.append(intruder_word)\n",
    "    random.shuffle(topic) # shuffles to avoid bias\n",
    "    returns.append((topic, intruder_word))\n",
    "  \n",
    "  for i in returns:\n",
    "    system_role = \"You are an NLP researcher trying to evaluate the quality of topics created by a topic modeling algorithm.\"\n",
    "    prompt = \"\"\"Given a set of words, identify which word is the intruder word (i.e. the word that does not belong in the topic). Here is an example input and output: \n",
    "\n",
    "    Input: Topic: [floppy, alphabet, computer, processor, memory disk]\n",
    "    Output: Intruder word: alphabet \n",
    "\n",
    "    Please, do this task for the following input. You must pick a word from the input set:\n",
    "\n",
    "    Input: Topic: \"\"\" + str(i[0]) + \"\"\"\n",
    "    Output: Intruder word: \"\"\"\n",
    "\n",
    "    # commented out for safe-guarding -- don't want to run mistakenly and increase costs\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_role}, \n",
    "        {\"role\": \"user\", \"content\": prompt} \n",
    "      ],\n",
    "      temperature = 0\n",
    "    )\n",
    "\n",
    "    prediction = completion.choices[0].message.content\n",
    "    prediction = \"\".join(\"\".join(prediction.split(\"'\")).split('\"'))\n",
    "\n",
    "    # order of output is: (dataset, model, list of words, actual intruder word, predicted intruder word, if prediction is in input list)\n",
    "    output.append((path.split(\"/\")[0], path.split(\"/\")[1], i[0], i[1], prediction, prediction in i[0]))\n",
    "\n",
    "# should take ~40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where prediction not in list (errors that we have to re-run or pick another topic for)\n",
    "bad_reads = [i for i in output if not i[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('twitter', 'Mallet_LDA', ['very', 'may', 'might', 'tomorrow', \"you're\"], 'tomorrow', 'youre', False)\n",
      "('twitter', 'Mallet_LDA', ['women', 'being', \"let's\", 'because', 'men'], \"let's\", 'lets', False)\n",
      "('facebook_2020', 'Mallet_LDA', ['money', \"mother's\", 'their', 'pay', 'paid'], \"mother's\", 'mothers', False)\n",
      "('yelp_reviews', 'Mallet_LDA', ['art', \"wasn't\", \"i'm\", 'pretty', 'bit'], 'art', 'wasnt', False)\n",
      "('yelp_reviews', 'Mallet_LDA', [\"wasn't\", 'average', 'better', 'bit', 'pretty'], \"wasn't\", 'wasnt', False)\n",
      "('yelp_reviews', 'Mallet_LDA', ['soup', 'chinese', 'chicken', \"i'd\", 'rice'], \"i'd\", 'id', False)\n",
      "('yelp_reviews', 'Mallet_LDA', ['though', \"wasn't\", 'old', 'pretty', 'bit'], 'old', 'wasnt', False)\n",
      "('amazon', 'Mallet_LDA', ['print', 'ink', 'pen', \"doesn't\", 'paper'], \"doesn't\", 'doesnt', False)\n",
      "('amazon', 'Mallet_LDA', ['year', \"isn't\", 'old', 'baby', 'kids'], \"isn't\", 'isnt', False)\n",
      "('amazon', 'Mallet_LDA', ['coffee', 'bottle', \"can't\", 'cup', 'water'], \"can't\", 'cant', False)\n",
      "('amazon', 'Mallet_LDA', ['place', 'stay', 'couple', \"doesn't\", 'hold'], 'couple', 'doesnt', False)\n",
      "('amazon', 'Mallet_LDA', [\"won't\", 'she', 'her', 'daughter', 'gift'], \"won't\", 'wont', False)\n",
      "('20_newsgroup', 'CTM', ['scx', 'qs', 'chz', 'lk', 'cx'], 'scx', 'Given the lack of context or clear semantic relationship between the words in the topic, its impossible to definitively identify an intruder word.', False)\n",
      "('20_newsgroup', 'CTM', ['yf', 'ax', 'nrhj', 'cx', 'qax'], 'cx', 'Given the lack of context or semantic meaning in the words, its impossible to determine an intruder word in this case.', False)\n",
      "('20_newsgroup', 'NMF', ['34u', 'a86', '3t', 'g9v', '75u'], 'g9v', 'Its hard to determine the intruder word in this case as all the words seem to be alphanumeric codes and theres no clear context to differentiate them. Could you provide more context or a different set of words?', False)\n",
      "('20_newsgroup', 'NMF', ['g9v', 'ax', 'bhj', 'a86', '34u'], 'ax', 'Its hard to determine an intruder word in this case as all the words seem to be random alphanumeric strings. They dont seem to form a coherent topic.', False)\n",
      "('20_newsgroup', 'NMF', ['cx', '3t', '1d9', '1t', '34u'], 'cx', 'Its hard to determine an intruder word in this case as all the words seem to be alphanumeric codes and dont provide a clear context. However, if we must choose, we could say 1d9 is the intruder word as its the only one with a d in it.', False)\n",
      "('20_newsgroup', 'NMF', ['ax', '34u', 'a86', 'bhj', 'b8f'], 'ax', 'Its hard to determine the intruder word in this case as all the words seem to be random alphanumeric combinations. However, if we assume that 34u, a86, bhj, b8f are some kind of codes or identifiers, ax could be considered as the intruder word because its a common term in English.', False)\n",
      "('20_newsgroup', 'NMF', ['34u', 'b8f', 'bhj', '75u', 'a86'], 'bhj', 'Its not possible to identify an intruder word in this case as all the words seem to be random alphanumeric strings with no clear topic or theme.', False)\n",
      "('20_newsgroup', 'NMF', ['mit', 'bhj', '1d9', '34u', '75u'], 'mit', 'This is a tricky one as all the words seem to be alphanumeric codes or abbreviations. However, without any specific context, its impossible to determine which one is the intruder.', False)\n",
      "('20_newsgroup', 'NMF', ['giz', 'a86', '75u', 'ax', '1d9'], 'ax', 'Its hard to determine the intruder word in this case as all the words seem to be alphanumeric codes or identifiers, which could potentially belong to the same topic. However, without additional context, its impossible to definitively identify an intruder word.', False)\n",
      "('20_newsgroup', 'NMF', ['0t', 'a86', 'bhj', 'b8f', '1d9'], 'a86', 'This is a tricky one as all the words seem to be alphanumeric codes. However, without any specific context, its impossible to determine which one is the intruder. We need more information or context to make a decision.', False)\n",
      "('20_newsgroup', 'NMF', ['giz', 'g9v', '1t', '34u', 'b8f'], '1t', 'This is a tricky one as all the words seem to be alphanumeric codes. However, without any specific context, its impossible to determine which one is the intruder.', False)\n",
      "('20_newsgroup', 'NMF', ['1t', 'bhj', 'giz', 'b8f', 'g9v'], '1t', 'This is a tricky one as all the words seem to be random alphanumeric strings. However, without any specific context or pattern, its impossible to determine an intruder word.', False)\n",
      "('20_newsgroup', 'NMF', ['cx', 'bhj', 'b8f', 'a86', '1d9'], 'cx', 'Its impossible to determine an intruder word in this case as all the words seem to be random alphanumeric strings with no clear semantic or thematic connection.', False)\n",
      "('20_newsgroup', 'NMF', ['giz', '75u', 'ax', 'b8f', '34u'], 'ax', 'Given the lack of context and the fact that all the words seem to be alphanumeric codes, its impossible to determine an intruder word in this case.', False)\n",
      "('20_newsgroup', 'NMF', ['1d9', '1t', 'a86', 'g9v', '34u'], '34u', 'Its hard to determine the intruder word in this case as all the words seem to be alphanumeric codes and theres no clear context to differentiate them. Could you provide more context or a different set of words?', False)\n",
      "('20_newsgroup', 'Mallet_LDA', [\"i'm\", 'police', 'law', 'state', 'license'], \"i'm\", 'im', False)\n",
      "('imdb', 'Mallet_LDA', [\"i'm\", 'star', 'trek', 'crew', 'ship'], \"i'm\", 'im', False)\n",
      "('imdb', 'Mallet_LDA', [\"he's\", 'cinema', 'art', 'images', 'director'], \"he's\", 'hes', False)\n",
      "('imdb', 'Mallet_LDA', ['flight', \"i'm\", 'train', 'plane', 'air'], \"i'm\", 'im', False)\n",
      "('imdb', 'Mallet_LDA', ['school', 'college', \"i'm\", 'high', 'girls'], \"i'm\", 'im', False)\n",
      "('imdb', 'Mallet_LDA', ['guy', 'girl', 'character', 'life', \"he's\"], 'life', 'hes', False)\n"
     ]
    }
   ],
   "source": [
    "for i in bad_reads:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually all assigned to 0, but need to carefully analyze results from cell above to change values around to make it most accurate\n",
    "# (sometimes GPT-4 removes appostraphes, punctuation, etc. but its correct -- correct for that model + dataset +1)\n",
    "# (sometimes returns bad read or an answer not in the options -- bad_reads for that model + dataset +1)\n",
    "\n",
    "CSS_bad_reads = {\n",
    "  'BERTopic': 0,\n",
    "  'CTM': 0,\n",
    "  'GensimLDA': 0,\n",
    "  'NMF': 0,\n",
    "  'Mallet_LDA': 0\n",
    "}\n",
    "\n",
    "non_CSS_bad_reads = {\n",
    "  'BERTopic': 0,\n",
    "  'CTM': 2,\n",
    "  'GensimLDA': 0,\n",
    "  'NMF': 13,\n",
    "  'Mallet_LDA': 0\n",
    "}\n",
    "\n",
    "CSS_correct = {\n",
    "  'BERTopic': 0,\n",
    "  'CTM': 0,\n",
    "  'GensimLDA': 0,\n",
    "  'NMF': 0,\n",
    "  'Mallet_LDA': 2\n",
    "}\n",
    "\n",
    "non_CSS_correct = {\n",
    "  'BERTopic': 0,\n",
    "  'CTM': 0,\n",
    "  'GensimLDA': 0,\n",
    "  'NMF': 0,\n",
    "  'Mallet_LDA': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSS Counts: {'BERTopic': 76, 'CTM': 55, 'GensimLDA': 35, 'NMF': 47, 'Mallet_LDA': 70}\n",
      "Non-CSS Counts: {'BERTopic': 456, 'CTM': 345, 'GensimLDA': 267, 'NMF': 196, 'Mallet_LDA': 423}\n",
      "\n",
      "CSS Accuracy: {'BERTopic': 25.333333333333332, 'CTM': 18.333333333333332, 'GensimLDA': 11.666666666666666, 'NMF': 15.666666666666666, 'Mallet_LDA': 23.333333333333332}\n",
      "Non-CSS Accuracy: {'BERTopic': 91.2, 'CTM': 69.27710843373494, 'GensimLDA': 53.4, 'NMF': 40.24640657084189, 'Mallet_LDA': 84.6}\n"
     ]
    }
   ],
   "source": [
    "for i in output:\n",
    "  if i[3] == i[4]:\n",
    "    if i[0] in ['twitter', 'fb', 'qualtrics']:\n",
    "      CSS_correct[i[1]] += 1\n",
    "    else:\n",
    "      non_CSS_correct[i[1]] += 1\n",
    "\n",
    "print(\"CSS Counts: \" + str(CSS_correct))\n",
    "print(\"Non-CSS Counts: \" + str(non_CSS_correct))\n",
    "\n",
    "print()\n",
    "\n",
    "CSS_acc = {\n",
    "  'BERTopic': 100 * CSS_correct['BERTopic'] / (300 - CSS_bad_reads['BERTopic']),\n",
    "  'CTM': 100 * CSS_correct['CTM'] / (300 - CSS_bad_reads['CTM']),\n",
    "  'GensimLDA': 100 * CSS_correct['GensimLDA'] / (300 - CSS_bad_reads['GensimLDA']),\n",
    "  'NMF': 100 * CSS_correct['NMF'] / (300 - CSS_bad_reads['NMF']),\n",
    "  'Mallet_LDA': 100 * CSS_correct['Mallet_LDA'] / (300 - CSS_bad_reads['Mallet_LDA'])\n",
    "}\n",
    "\n",
    "non_CSS_acc = {\n",
    "  'BERTopic': 100 * non_CSS_correct['BERTopic'] / (500 - non_CSS_bad_reads['BERTopic']),\n",
    "  'CTM': 100 * non_CSS_correct['CTM'] / (500 - non_CSS_bad_reads['CTM']),\n",
    "  'GensimLDA': 100 * non_CSS_correct['GensimLDA'] / (500 - non_CSS_bad_reads['GensimLDA']),\n",
    "  'NMF': 100 * non_CSS_correct['NMF'] / (500 - non_CSS_bad_reads['NMF']),\n",
    "  'Mallet_LDA': 100 * non_CSS_correct['Mallet_LDA'] / (500 - non_CSS_bad_reads['Mallet_LDA'])\n",
    "}\n",
    "\n",
    "print(\"CSS Accuracy: \" + str(CSS_acc))\n",
    "print(\"Non-CSS Accuracy: \" + str(non_CSS_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "import json\n",
    "\n",
    "with open('CSS_counts.txt', 'w') as f:\n",
    "    f.write(json.dumps(CSS_correct))\n",
    "\n",
    "with open('non_CSS_counts.txt', 'w') as f:\n",
    "    f.write(json.dumps(non_CSS_correct))\n",
    "\n",
    "with open('CSS_acc.txt', 'w') as f:\n",
    "    f.write(json.dumps(CSS_acc))\n",
    "\n",
    "with open('non_CSS_acc.txt', 'w') as f:\n",
    "    f.write(json.dumps(non_CSS_acc))\n",
    "\n",
    "with open('intruder_detection_outputs.txt', 'w') as f:\n",
    "  for item in output:\n",
    "    f.write(str(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "with open('intruder_detection_outputs.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line[1:-1]\n",
    "        arr = line.split(\", \")\n",
    "\n",
    "        dataset = arr[0][1:-1]\n",
    "        model = arr[1][1:-1]\n",
    "        arr = arr[2:]\n",
    "\n",
    "        new_line = \", \".join(arr)\n",
    "\n",
    "        arr = new_line.split(\"[\")[1].split(\"]\")\n",
    "\n",
    "        list_of_words = arr[0].split(', ')\n",
    "        list_of_words = [word[1:-1] for word in list_of_words]\n",
    "\n",
    "        arr = arr[1].strip().split(', ')[1:]\n",
    "\n",
    "        actual_intruder = arr[0][1:-1]\n",
    "        predicted_intruder = arr[1][1:-1]\n",
    "\n",
    "        arr = arr[2:]\n",
    "        in_list = \", \".join(arr).strip()[:-1]\n",
    "\n",
    "        if in_list == 'True':\n",
    "            in_list = True\n",
    "        else:\n",
    "            in_list = False\n",
    "\n",
    "        tup = (dataset, model, list_of_words, actual_intruder, predicted_intruder, in_list)\n",
    "        output.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': 0.43,\n",
       " 'Mallet_LDA': 0.7,\n",
       " 'CTM': 0.64,\n",
       " 'BERTopic': 0.79,\n",
       " 'NMF': 0.23}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bbc_news\n",
    "\n",
    "bbc_intruder = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "bbc_intruder_bad_reads = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "for i in output:\n",
    "    if i[0] == 'bbc_news':\n",
    "        if i[3] == i[4]:\n",
    "            bbc_intruder[i[1]] += 1\n",
    "\n",
    "bbc_intruder = {i : bbc_intruder[i] / (100 - bbc_intruder_bad_reads[i]) for i in bbc_intruder.keys()}\n",
    "bbc_intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': 0.5,\n",
       " 'Mallet_LDA': 0.7878787878787878,\n",
       " 'CTM': 0.64,\n",
       " 'BERTopic': 0.74,\n",
       " 'NMF': 0.6470588235294118}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20_newsgroup\n",
    "\n",
    "ng_intruder = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 2,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "ng_intruder_bad_reads = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 1,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 15\n",
    "}\n",
    "\n",
    "for i in output:\n",
    "    if i[0] == '20_newsgroup':\n",
    "        if i[3] == i[4]:\n",
    "            ng_intruder[i[1]] += 1\n",
    "\n",
    "ng_intruder = {i : ng_intruder[i] / (100 - ng_intruder_bad_reads[i]) for i in ng_intruder.keys()}\n",
    "ng_intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': 0.34,\n",
       " 'Mallet_LDA': 0.5454545454545454,\n",
       " 'CTM': 0.41,\n",
       " 'BERTopic': 0.63,\n",
       " 'NMF': 0.2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imdb\n",
    "\n",
    "imdb_intruder = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "imdb_intruder_bad_reads = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 1,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "for i in output:\n",
    "    if i[0] == 'imdb':\n",
    "        if i[3] == i[4]:\n",
    "            imdb_intruder[i[1]] += 1\n",
    "\n",
    "imdb_intruder = {i : imdb_intruder[i] / (100 - imdb_intruder_bad_reads[i]) for i in imdb_intruder.keys()}\n",
    "imdb_intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': 0.38,\n",
       " 'Mallet_LDA': 0.7,\n",
       " 'CTM': 0.55,\n",
       " 'BERTopic': 0.85,\n",
       " 'NMF': 0.4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amazon\n",
    "\n",
    "amazon_intruder = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 1,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "amazon_intruder_bad_reads = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "for i in output:\n",
    "    if i[0] == 'amazon':\n",
    "        if i[3] == i[4]:\n",
    "            amazon_intruder[i[1]] += 1\n",
    "\n",
    "amazon_intruder = {i : amazon_intruder[i] / (100 - amazon_intruder_bad_reads[i]) for i in amazon_intruder.keys()}\n",
    "amazon_intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': 0.62,\n",
       " 'Mallet_LDA': 0.75,\n",
       " 'CTM': 0.59,\n",
       " 'BERTopic': 0.8,\n",
       " 'NMF': 0.31}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yelp_reviews\n",
    "\n",
    "yelp_intruder = {\n",
    "    'GensimLDA': 1,\n",
    "    'Mallet_LDA': 1,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "yelp_intruder_bad_reads = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "for i in output:\n",
    "    if i[0] == 'yelp_reviews':\n",
    "        if i[3] == i[4]:\n",
    "            yelp_intruder[i[1]] += 1\n",
    "\n",
    "yelp_intruder = {i : yelp_intruder[i] / (100 - yelp_intruder_bad_reads[i]) for i in yelp_intruder.keys()}\n",
    "yelp_intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': 0.41,\n",
       " 'Mallet_LDA': 0.69,\n",
       " 'CTM': 0.62,\n",
       " 'BERTopic': 0.75,\n",
       " 'NMF': 0.27}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# facebook_2020\n",
    "\n",
    "fb_intruder = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "fb_intruder_bad_reads = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "for i in output:\n",
    "    if i[0] == 'facebook_2020':\n",
    "        if i[3] == i[4]:\n",
    "            fb_intruder[i[1]] += 1\n",
    "\n",
    "fb_intruder = {i : fb_intruder[i] / (100 - fb_intruder_bad_reads[i]) for i in fb_intruder.keys()}\n",
    "fb_intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': 0.35,\n",
       " 'Mallet_LDA': 0.69,\n",
       " 'CTM': 0.55,\n",
       " 'BERTopic': 0.76,\n",
       " 'NMF': 0.47}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twitter\n",
    "\n",
    "twitter_intruder = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 1,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "twitter_intruder_bad_reads = {\n",
    "    'GensimLDA': 0,\n",
    "    'Mallet_LDA': 0,\n",
    "    'CTM': 0,\n",
    "    'BERTopic': 0,\n",
    "    'NMF': 0\n",
    "}\n",
    "\n",
    "for i in output:\n",
    "    if i[0] == 'twitter':\n",
    "        if i[3] == i[4]:\n",
    "            twitter_intruder[i[1]] += 1\n",
    "\n",
    "twitter_intruder = {i : twitter_intruder[i] / (100 - twitter_intruder_bad_reads[i]) for i in twitter_intruder.keys()}\n",
    "twitter_intruder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
