{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1807.4432373046875\n",
      "Epoch 51, Loss: 846.8967895507812\n",
      "Epoch 101, Loss: 364.09808349609375\n",
      "Epoch 151, Loss: 165.1435089111328\n",
      "Epoch 201, Loss: 101.82080841064453\n",
      "Epoch 251, Loss: 87.878662109375\n",
      "Epoch 301, Loss: 86.59911346435547\n",
      "Epoch 351, Loss: 87.09384155273438\n",
      "Epoch 401, Loss: 87.38656616210938\n",
      "Epoch 451, Loss: 87.44734954833984\n",
      "Train RMSE: 9.732276386240782\n",
      "Test RMSE: 10.514234920805848\n",
      "R2 Train: -0.03833259058507599\n",
      "R2 Test: -0.0012626944879936541\n",
      "Run: 2\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'facebook/Mallet_LDA/run_2/topics_100.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    119\u001b[0m topics \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/run_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopics_100.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    121\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "File \u001b[0;32m/nlp/data/arnavjha/miniconda3/envs/BERTopic_1/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'facebook/Mallet_LDA/run_2/topics_100.txt'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# facebook\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# change this\n",
    "dataset = 'facebook' # folder and dataset name\n",
    "\n",
    "cols = ['age', 'UCLA_3item_sum', 'stress', 'depression',\n",
    "        'Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness_to_newexp'] # outcome columns\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "# START\n",
    "\n",
    "train = pickle.load(open(dataset + '/NMF/run_1/train.pkl', 'rb'))\n",
    "test = pickle.load(open(dataset + '/NMF/run_1/test.pkl', 'rb'))\n",
    "\n",
    "temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "df = pd.read_csv('data/' + dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "df['isFemale'] = df['sex'] == 'Female'\n",
    "\n",
    "def func(x):\n",
    "    education = x['education']\n",
    "    bachelor_plus = [\"Master's degree or equivalent\", \"Professional degree (MD, JD, etc.), doctorate degree or higher\"]\n",
    "    bachelor_minus = [\"High school diploma or equivalent\", \"Vocational/Technical Training\"]\n",
    "\n",
    "    if education in bachelor_plus:\n",
    "        return 'bachelor+'\n",
    "    elif education in bachelor_minus:\n",
    "        return \"bachelor-\"\n",
    "    else:\n",
    "        return 'bachelor'\n",
    "\n",
    "df['education_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "label_encoder = LabelEncoder()\n",
    "df['education_bucket'] = label_encoder.fit_transform(df['education_bucket'])\n",
    "\n",
    "def func(x):\n",
    "    income = x['income']\n",
    "    under = [\"$10,000 - $19,999\", \"$20,000 - $34,999\", \"$35,000 - $49,999\"]\n",
    "    over = [\"$100,000 - $149,999\", \"More than $150,000\"]\n",
    "\n",
    "    if income in under:\n",
    "        return '<50k+'\n",
    "    elif income in over:\n",
    "        return \"100k+\"\n",
    "    else:\n",
    "        return '$50,000 - $99,999'\n",
    "\n",
    "df['income_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "label_encoder = LabelEncoder()\n",
    "df['income_bucket'] = label_encoder.fit_transform(df['income_bucket'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['marital_status'] = label_encoder.fit_transform(df['marital_status'])\n",
    "\n",
    "merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "all_user_ids = list(user_ids.keys())\n",
    "\n",
    "labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "\n",
    "all_ys = []\n",
    "for key in user_ids.keys():\n",
    "    all_ys.append(labels_data[labels_data['user_id'] == key][cols].iloc[0])\n",
    "\n",
    "for outcome in cols:\n",
    "    print(outcome)\n",
    "    print()\n",
    "\n",
    "    y = [i[outcome] for i in all_ys]\n",
    "    \n",
    "    # REGRESSION\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for model in ['Mallet_LDA']:\n",
    "        print(model)\n",
    "        \n",
    "        train_error_list = []\n",
    "        test_error_list = []\n",
    "\n",
    "        r2s_train = []\n",
    "        r2s_test = []\n",
    "\n",
    "        for run in range(1, 6):\n",
    "            print(\"Run: \" + str(run))\n",
    "            print()\n",
    "\n",
    "            topics = []\n",
    "            with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    topic_list = [item.strip() for item in row if item.strip()]\n",
    "                    topics.append(topic_list)\n",
    "\n",
    "            X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "            # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "            X_train = X[:round(0.80 * len(X))]\n",
    "            X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "            y_train = y[:round(0.80 * len(X))]\n",
    "            y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            X_test = np.array(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            torch.manual_seed(42) # for reproducibility\n",
    "\n",
    "            X_train_NN = torch.tensor(X_train, dtype = torch.float32)\n",
    "            y_train_NN = torch.tensor(y_train, dtype = torch.float32).reshape(-1, 1)\n",
    "            X_test_NN = torch.tensor(X_test, dtype = torch.float32)\n",
    "            y_test_NN = torch.tensor(y_test, dtype = torch.float32).reshape(-1, 1)\n",
    "\n",
    "            model_NN = nn.Sequential(\n",
    "                nn.Linear(X_train_NN.shape[1], 1),\n",
    "            )\n",
    "\n",
    "            loss_fn = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model_NN.parameters(), lr = lr)\n",
    "\n",
    "            n_epochs = 1000\n",
    "            batch_size = 32\n",
    "            batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "            best_mse = np.inf\n",
    "            best_weights = None\n",
    "            history = []\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                model_NN.train()\n",
    "                with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                    bar.set_description(f\"Epoch {epoch}\")\n",
    "                    for start in bar:\n",
    "\n",
    "                        X_batch = X_train_NN[start : start+batch_size]\n",
    "                        y_batch = y_train_NN[start : start+batch_size]\n",
    "\n",
    "                        y_pred = model_NN(X_batch)\n",
    "                        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                        bar.set_postfix(mse=float(loss))\n",
    "\n",
    "                model_NN.eval()\n",
    "                y_pred = model_NN(X_test_NN)\n",
    "                mse = loss_fn(y_pred, y_test_NN)\n",
    "                mse = float(mse)\n",
    "                history.append(mse)\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_weights = copy.deepcopy(model_NN.state_dict())\n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "            model_NN.load_state_dict(best_weights)\n",
    "\n",
    "            y_pred_train = model_NN(X_train_NN).detach().numpy()\n",
    "            y_pred_test = model_NN(X_test_NN).detach().numpy()\n",
    "\n",
    "            train_error = np.sqrt(mean_squared_error(y_pred_train, y_train))\n",
    "            test_error = np.sqrt(mean_squared_error(y_pred_test, y_test))\n",
    "\n",
    "            train_error_list.append(train_error)\n",
    "            test_error_list.append(test_error)\n",
    "\n",
    "            print(f'Train RMSE: {train_error}')\n",
    "            print(f'Test RMSE: {test_error}')\n",
    "\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "            r2s_train.append(r2_train)\n",
    "            r2s_test.append(r2_test)\n",
    "            \n",
    "            print(f'R2 Train: {r2_train}')\n",
    "            print(f'R2 Test: {r2_test}')\n",
    "\n",
    "            all_scores[model] = {\n",
    "                'Train RMSE': train_error_list,\n",
    "                'Test RMSE': test_error_list,\n",
    "                'R2 Train': r2s_train,\n",
    "                'R2 Test': r2s_test,\n",
    "\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': y_pred_train,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_pred_test\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    for m in all_scores.keys():\n",
    "        print(f'{m} Average Train RMSE: {np.mean(all_scores[m][\"Train RMSE\"])}')\n",
    "        print(f'{m} Average Test RMSE: {np.mean(all_scores[m][\"Test RMSE\"])}')\n",
    "        print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "        print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "        print()\n",
    "\n",
    "    # with open('all_results/' + dataset + '_all_scores_' + outcome + '_NN.pkl', 'wb') as f:\n",
    "    #     pickle.dump(all_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
