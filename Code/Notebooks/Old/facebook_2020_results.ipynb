{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1754.3682861328125\n",
      "Epoch 51, Loss: 112.29078674316406\n",
      "Epoch 101, Loss: 112.30257415771484\n",
      "Epoch 151, Loss: 112.68301391601562\n",
      "Epoch 201, Loss: 113.63865661621094\n",
      "Epoch 251, Loss: 114.87398529052734\n",
      "Epoch 301, Loss: 116.0321044921875\n",
      "Epoch 351, Loss: 116.93688201904297\n",
      "Epoch 401, Loss: 117.54356384277344\n",
      "Epoch 451, Loss: 117.87154388427734\n",
      "Train RMSE: 10.507967544140913\n",
      "Test RMSE: 10.84224266615706\n",
      "R2 Train: 0.20843269492981997\n",
      "R2 Test: 0.18096409056151608\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1754.3682861328125\n",
      "Epoch 51, Loss: 112.29078674316406\n",
      "Epoch 101, Loss: 112.30257415771484\n",
      "Epoch 151, Loss: 112.68301391601562\n",
      "Epoch 201, Loss: 113.63865661621094\n",
      "Epoch 251, Loss: 114.87398529052734\n",
      "Epoch 301, Loss: 116.0321044921875\n",
      "Epoch 351, Loss: 116.93688201904297\n",
      "Epoch 401, Loss: 117.54356384277344\n",
      "Epoch 451, Loss: 117.87154388427734\n",
      "Train RMSE: 10.507967544140913\n",
      "Test RMSE: 10.84224266615706\n",
      "R2 Train: 0.20843269492981997\n",
      "R2 Test: 0.18096409056151608\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1754.3682861328125\n",
      "Epoch 51, Loss: 112.29078674316406\n",
      "Epoch 101, Loss: 112.30257415771484\n",
      "Epoch 151, Loss: 112.68301391601562\n",
      "Epoch 201, Loss: 113.63865661621094\n",
      "Epoch 251, Loss: 114.87398529052734\n",
      "Epoch 301, Loss: 116.0321044921875\n",
      "Epoch 351, Loss: 116.93688201904297\n",
      "Epoch 401, Loss: 117.54356384277344\n",
      "Epoch 451, Loss: 117.87154388427734\n",
      "Train RMSE: 10.507967544140913\n",
      "Test RMSE: 10.84224266615706\n",
      "R2 Train: 0.20843269492981997\n",
      "R2 Test: 0.18096409056151608\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1754.3682861328125\n",
      "Epoch 51, Loss: 112.29078674316406\n",
      "Epoch 101, Loss: 112.30257415771484\n",
      "Epoch 151, Loss: 112.68301391601562\n",
      "Epoch 201, Loss: 113.63865661621094\n",
      "Epoch 251, Loss: 114.87398529052734\n",
      "Epoch 301, Loss: 116.0321044921875\n",
      "Epoch 351, Loss: 116.93688201904297\n",
      "Epoch 401, Loss: 117.54356384277344\n",
      "Epoch 451, Loss: 117.87154388427734\n",
      "Train RMSE: 10.507967544140913\n",
      "Test RMSE: 10.84224266615706\n",
      "R2 Train: 0.20843269492981997\n",
      "R2 Test: 0.18096409056151608\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1754.3682861328125\n",
      "Epoch 51, Loss: 112.29078674316406\n",
      "Epoch 101, Loss: 112.30257415771484\n",
      "Epoch 151, Loss: 112.68301391601562\n",
      "Epoch 201, Loss: 113.63865661621094\n",
      "Epoch 251, Loss: 114.87398529052734\n",
      "Epoch 301, Loss: 116.0321044921875\n",
      "Epoch 351, Loss: 116.93688201904297\n",
      "Epoch 401, Loss: 117.54356384277344\n",
      "Epoch 451, Loss: 117.87154388427734\n",
      "Train RMSE: 10.507967544140913\n",
      "Test RMSE: 10.84224266615706\n",
      "R2 Train: 0.20843269492981997\n",
      "R2 Test: 0.18096409056151608\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1763.8372802734375\n",
      "Epoch 51, Loss: 111.77889251708984\n",
      "Epoch 101, Loss: 110.78470611572266\n",
      "Epoch 151, Loss: 109.74646759033203\n",
      "Epoch 201, Loss: 108.68550872802734\n",
      "Epoch 251, Loss: 107.63678741455078\n",
      "Epoch 301, Loss: 106.62152862548828\n",
      "Epoch 351, Loss: 105.64897918701172\n",
      "Epoch 401, Loss: 104.72346496582031\n",
      "Epoch 451, Loss: 103.84922790527344\n",
      "Train RMSE: 11.509875161450058\n",
      "Test RMSE: 11.802188540877722\n",
      "R2 Train: 0.05028864895150098\n",
      "R2 Test: 0.029512841414388702\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1763.5167236328125\n",
      "Epoch 51, Loss: 112.85546112060547\n",
      "Epoch 101, Loss: 112.39571380615234\n",
      "Epoch 151, Loss: 111.87857818603516\n",
      "Epoch 201, Loss: 111.32121276855469\n",
      "Epoch 251, Loss: 110.75465393066406\n",
      "Epoch 301, Loss: 110.17960357666016\n",
      "Epoch 351, Loss: 109.59423065185547\n",
      "Epoch 401, Loss: 109.00090789794922\n",
      "Epoch 451, Loss: 108.40357208251953\n",
      "Train RMSE: 11.562764507287662\n",
      "Test RMSE: 11.935831394754025\n",
      "R2 Train: 0.04154050579173485\n",
      "R2 Test: 0.007409652878079487\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1763.1719970703125\n",
      "Epoch 51, Loss: 111.49055480957031\n",
      "Epoch 101, Loss: 111.69917297363281\n",
      "Epoch 151, Loss: 112.01273345947266\n",
      "Epoch 201, Loss: 112.27080535888672\n",
      "Epoch 251, Loss: 112.44528198242188\n",
      "Epoch 301, Loss: 112.54255676269531\n",
      "Epoch 351, Loss: 112.57259368896484\n",
      "Epoch 401, Loss: 112.54476928710938\n",
      "Epoch 451, Loss: 112.46800994873047\n",
      "Train RMSE: 11.619808951099865\n",
      "Test RMSE: 11.924093315492216\n",
      "R2 Train: 0.03206013339656821\n",
      "R2 Test: 0.009360983246142873\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1763.738037109375\n",
      "Epoch 51, Loss: 112.66738891601562\n",
      "Epoch 101, Loss: 112.65159606933594\n",
      "Epoch 151, Loss: 112.68168640136719\n",
      "Epoch 201, Loss: 112.55113983154297\n",
      "Epoch 251, Loss: 112.25196838378906\n",
      "Epoch 301, Loss: 111.82343292236328\n",
      "Epoch 351, Loss: 111.30448150634766\n",
      "Epoch 401, Loss: 110.72737884521484\n",
      "Epoch 451, Loss: 110.11653137207031\n",
      "Train RMSE: 11.521369182514\n",
      "Test RMSE: 11.905718547320797\n",
      "R2 Train: 0.04839089548267217\n",
      "R2 Test: 0.012411737171495019\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1763.73046875\n",
      "Epoch 51, Loss: 112.04940795898438\n",
      "Epoch 101, Loss: 112.16419982910156\n",
      "Epoch 151, Loss: 112.39060974121094\n",
      "Epoch 201, Loss: 112.56600952148438\n",
      "Epoch 251, Loss: 112.63520812988281\n",
      "Epoch 301, Loss: 112.592041015625\n",
      "Epoch 351, Loss: 112.45175170898438\n",
      "Epoch 401, Loss: 112.23622131347656\n",
      "Epoch 451, Loss: 111.96542358398438\n",
      "Train RMSE: 11.563929309468511\n",
      "Test RMSE: 11.883684228765917\n",
      "R2 Train: 0.04134739073853433\n",
      "R2 Test: 0.016063880893753324\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1512.7286376953125\n",
      "Epoch 51, Loss: 112.28099822998047\n",
      "Epoch 101, Loss: 112.38359069824219\n",
      "Epoch 151, Loss: 112.52932739257812\n",
      "Epoch 201, Loss: 112.74073791503906\n",
      "Epoch 251, Loss: 112.97815704345703\n",
      "Epoch 301, Loss: 113.19120788574219\n",
      "Epoch 351, Loss: 113.36929321289062\n",
      "Epoch 401, Loss: 113.51384735107422\n",
      "Epoch 451, Loss: 113.62858581542969\n",
      "Train RMSE: 11.809541320294391\n",
      "Test RMSE: 11.990521239584249\n",
      "R2 Train: 0.0001923338055476176\n",
      "R2 Test: -0.0017072617304993987\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1512.6795654296875\n",
      "Epoch 51, Loss: 112.26611328125\n",
      "Epoch 101, Loss: 112.33622741699219\n",
      "Epoch 151, Loss: 112.42098999023438\n",
      "Epoch 201, Loss: 112.54584503173828\n",
      "Epoch 251, Loss: 112.68580627441406\n",
      "Epoch 301, Loss: 112.80325317382812\n",
      "Epoch 351, Loss: 112.89421844482422\n",
      "Epoch 401, Loss: 112.9635009765625\n",
      "Epoch 451, Loss: 113.01608276367188\n",
      "Train RMSE: 11.809614157289374\n",
      "Test RMSE: 11.99087594549871\n",
      "R2 Train: 0.00018000086075964372\n",
      "R2 Test: -0.0017665280023555496\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1512.7078857421875\n",
      "Epoch 51, Loss: 112.30028533935547\n",
      "Epoch 101, Loss: 112.4495849609375\n",
      "Epoch 151, Loss: 112.653076171875\n",
      "Epoch 201, Loss: 112.91720581054688\n",
      "Epoch 251, Loss: 113.2122802734375\n",
      "Epoch 301, Loss: 113.49604797363281\n",
      "Epoch 351, Loss: 113.76042175292969\n",
      "Epoch 401, Loss: 114.00704193115234\n",
      "Epoch 451, Loss: 114.23876953125\n",
      "Train RMSE: 11.809666800442086\n",
      "Test RMSE: 11.991067623371391\n",
      "R2 Train: 0.00017108714122382462\n",
      "R2 Test: -0.0017985553559209944\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1512.704833984375\n",
      "Epoch 51, Loss: 112.25206756591797\n",
      "Epoch 101, Loss: 112.28657531738281\n",
      "Epoch 151, Loss: 112.30642700195312\n",
      "Epoch 201, Loss: 112.35889434814453\n",
      "Epoch 251, Loss: 112.4451904296875\n",
      "Epoch 301, Loss: 112.53023529052734\n",
      "Epoch 351, Loss: 112.60478210449219\n",
      "Epoch 401, Loss: 112.66864776611328\n",
      "Epoch 451, Loss: 112.72338104248047\n",
      "Train RMSE: 11.809455791021902\n",
      "Test RMSE: 11.990784246096021\n",
      "R2 Train: 0.00020681574186376395\n",
      "R2 Test: -0.0017512061790929678\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1512.76318359375\n",
      "Epoch 51, Loss: 112.35652923583984\n",
      "Epoch 101, Loss: 112.66573333740234\n",
      "Epoch 151, Loss: 113.13154602050781\n",
      "Epoch 201, Loss: 113.66716766357422\n",
      "Epoch 251, Loss: 114.16908264160156\n",
      "Epoch 301, Loss: 114.58689880371094\n",
      "Epoch 351, Loss: 114.92417907714844\n",
      "Epoch 401, Loss: 115.19507598876953\n",
      "Epoch 451, Loss: 115.41284942626953\n",
      "Train RMSE: 11.809294182314732\n",
      "Test RMSE: 11.99052974493088\n",
      "R2 Train: 0.00023417926838864567\n",
      "R2 Test: -0.0017086828314398783\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1757.2421875\n",
      "Epoch 51, Loss: 112.81847381591797\n",
      "Epoch 101, Loss: 113.33507537841797\n",
      "Epoch 151, Loss: 113.84754180908203\n",
      "Epoch 201, Loss: 114.23686981201172\n",
      "Epoch 251, Loss: 114.52322387695312\n",
      "Epoch 301, Loss: 114.73049926757812\n",
      "Epoch 351, Loss: 114.87584686279297\n",
      "Epoch 401, Loss: 114.97344207763672\n",
      "Epoch 451, Loss: 115.03429412841797\n",
      "Train RMSE: 11.514027195154227\n",
      "Test RMSE: 11.733272302260911\n",
      "R2 Train: 0.0496033339992753\n",
      "R2 Test: 0.04081363570040797\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1757.44970703125\n",
      "Epoch 51, Loss: 112.61994171142578\n",
      "Epoch 101, Loss: 113.00752258300781\n",
      "Epoch 151, Loss: 113.41094207763672\n",
      "Epoch 201, Loss: 113.71498107910156\n",
      "Epoch 251, Loss: 113.93545532226562\n",
      "Epoch 301, Loss: 114.09608459472656\n",
      "Epoch 351, Loss: 114.21278381347656\n",
      "Epoch 401, Loss: 114.29656219482422\n",
      "Epoch 451, Loss: 114.35430145263672\n",
      "Train RMSE: 11.446892286540503\n",
      "Test RMSE: 11.75046210085822\n",
      "R2 Train: 0.060653990514431766\n",
      "R2 Test: 0.038001070201932\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1757.017333984375\n",
      "Epoch 51, Loss: 112.552001953125\n",
      "Epoch 101, Loss: 112.87299346923828\n",
      "Epoch 151, Loss: 113.13070678710938\n",
      "Epoch 201, Loss: 113.21630096435547\n",
      "Epoch 251, Loss: 113.18108367919922\n",
      "Epoch 301, Loss: 113.06749725341797\n",
      "Epoch 351, Loss: 112.89958953857422\n",
      "Epoch 401, Loss: 112.69419860839844\n",
      "Epoch 451, Loss: 112.46326446533203\n",
      "Train RMSE: 11.336723063946055\n",
      "Test RMSE: 11.61696257459325\n",
      "R2 Train: 0.07864822299558194\n",
      "R2 Test: 0.05973585140806448\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1757.302001953125\n",
      "Epoch 51, Loss: 112.95518493652344\n",
      "Epoch 101, Loss: 113.31450653076172\n",
      "Epoch 151, Loss: 113.56456756591797\n",
      "Epoch 201, Loss: 113.63264465332031\n",
      "Epoch 251, Loss: 113.60511016845703\n",
      "Epoch 301, Loss: 113.52877807617188\n",
      "Epoch 351, Loss: 113.41975402832031\n",
      "Epoch 401, Loss: 113.28697204589844\n",
      "Epoch 451, Loss: 113.13679504394531\n",
      "Train RMSE: 11.433125648985152\n",
      "Test RMSE: 11.66417056307178\n",
      "R2 Train: 0.06291204616721624\n",
      "R2 Test: 0.052078398991181496\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1757.4884033203125\n",
      "Epoch 51, Loss: 112.67110443115234\n",
      "Epoch 101, Loss: 113.14362335205078\n",
      "Epoch 151, Loss: 113.59034729003906\n",
      "Epoch 201, Loss: 113.85547637939453\n",
      "Epoch 251, Loss: 113.96800231933594\n",
      "Epoch 301, Loss: 113.97451782226562\n",
      "Epoch 351, Loss: 113.90702819824219\n",
      "Epoch 401, Loss: 113.78860473632812\n",
      "Epoch 451, Loss: 113.63545989990234\n",
      "Train RMSE: 11.464983430621773\n",
      "Test RMSE: 11.687219855093579\n",
      "R2 Train: 0.057682481311315814\n",
      "R2 Test: 0.048328366488419094\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1453.4432373046875\n",
      "Epoch 51, Loss: 120.48719024658203\n",
      "Epoch 101, Loss: 92.33975219726562\n",
      "Epoch 151, Loss: 84.65955352783203\n",
      "Epoch 201, Loss: 83.64241790771484\n",
      "Epoch 251, Loss: 83.56534576416016\n",
      "Epoch 301, Loss: 83.49337768554688\n",
      "Epoch 351, Loss: 83.37621307373047\n",
      "Epoch 401, Loss: 83.243408203125\n",
      "Epoch 451, Loss: 83.11231994628906\n",
      "Train RMSE: 9.591766743622426\n",
      "Test RMSE: 10.47108530895276\n",
      "R2 Train: 0.34045015880694385\n",
      "R2 Test: 0.2360796269623604\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1445.879638671875\n",
      "Epoch 51, Loss: 123.87924194335938\n",
      "Epoch 101, Loss: 97.21245574951172\n",
      "Epoch 151, Loss: 88.81840515136719\n",
      "Epoch 201, Loss: 87.06521606445312\n",
      "Epoch 251, Loss: 86.55628204345703\n",
      "Epoch 301, Loss: 86.19508361816406\n",
      "Epoch 351, Loss: 85.85326385498047\n",
      "Epoch 401, Loss: 85.52913665771484\n",
      "Epoch 451, Loss: 85.22738647460938\n",
      "Train RMSE: 9.896955519285163\n",
      "Test RMSE: 10.805970506259873\n",
      "R2 Train: 0.29781161877812845\n",
      "R2 Test: 0.18643500811227398\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1450.7784423828125\n",
      "Epoch 51, Loss: 116.03256225585938\n",
      "Epoch 101, Loss: 87.70488739013672\n",
      "Epoch 151, Loss: 80.49018096923828\n",
      "Epoch 201, Loss: 79.94508361816406\n",
      "Epoch 251, Loss: 79.99156188964844\n",
      "Epoch 301, Loss: 79.8546142578125\n",
      "Epoch 351, Loss: 79.61146545410156\n",
      "Epoch 401, Loss: 79.33946228027344\n",
      "Epoch 451, Loss: 79.07136535644531\n",
      "Train RMSE: 9.57267396393077\n",
      "Test RMSE: 10.381957235997822\n",
      "R2 Train: 0.34307326403531\n",
      "R2 Test: 0.24902899704145998\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1453.74462890625\n",
      "Epoch 51, Loss: 119.6358642578125\n",
      "Epoch 101, Loss: 92.41309356689453\n",
      "Epoch 151, Loss: 85.06758117675781\n",
      "Epoch 201, Loss: 84.24947357177734\n",
      "Epoch 251, Loss: 84.31658172607422\n",
      "Epoch 301, Loss: 84.33647155761719\n",
      "Epoch 351, Loss: 84.27848052978516\n",
      "Epoch 401, Loss: 84.18167877197266\n",
      "Epoch 451, Loss: 84.06819152832031\n",
      "Train RMSE: 9.649991575657337\n",
      "Test RMSE: 10.521297765567015\n",
      "R2 Train: 0.33241853418865475\n",
      "R2 Test: 0.22873553833720306\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1451.0606689453125\n",
      "Epoch 51, Loss: 117.00066375732422\n",
      "Epoch 101, Loss: 91.21668243408203\n",
      "Epoch 151, Loss: 87.36653900146484\n",
      "Epoch 201, Loss: 88.02262115478516\n",
      "Epoch 251, Loss: 88.49336242675781\n",
      "Epoch 301, Loss: 88.59172058105469\n",
      "Epoch 351, Loss: 88.51193237304688\n",
      "Epoch 401, Loss: 88.360595703125\n",
      "Epoch 451, Loss: 88.18171691894531\n",
      "Train RMSE: 9.504311351037604\n",
      "Test RMSE: 10.333018013445006\n",
      "R2 Train: 0.3524225576995865\n",
      "R2 Test: 0.2560922731152565\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 10.507967544140913\n",
      "GensimLDA Average Test RMSE: 10.84224266615706\n",
      "GensimLDA Average R2 Train: 0.20843269492982\n",
      "GensimLDA Average R2 Test: 0.18096409056151608\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 11.55554942236402\n",
      "Mallet_LDA Average Test RMSE: 11.890303205442134\n",
      "Mallet_LDA Average R2 Train: 0.04272551487220211\n",
      "Mallet_LDA Average R2 Test: 0.01495181912077188\n",
      "\n",
      "CTM Average Train RMSE: 11.809514450272498\n",
      "CTM Average Test RMSE: 11.990755759896249\n",
      "CTM Average R2 Train: 0.00019688336355669911\n",
      "CTM Average R2 Test: -0.0017464468198617577\n",
      "\n",
      "BERTopic Average Train RMSE: 11.439150325049543\n",
      "BERTopic Average Test RMSE: 11.690417479175547\n",
      "BERTopic Average R2 Train: 0.061900014997564215\n",
      "BERTopic Average R2 Test: 0.047791464558001005\n",
      "\n",
      "NMF Average Train RMSE: 9.643139830706659\n",
      "NMF Average Test RMSE: 10.502665766044496\n",
      "NMF Average R2 Train: 0.33323522670172473\n",
      "NMF Average R2 Test: 0.23127428871371078\n",
      "\n",
      "UCLA_3item_sum\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 20.95458221435547\n",
      "Epoch 51, Loss: 4.013247489929199\n",
      "Epoch 101, Loss: 4.04715633392334\n",
      "Epoch 151, Loss: 4.025546550750732\n",
      "Epoch 201, Loss: 3.9806437492370605\n",
      "Epoch 251, Loss: 3.936497688293457\n",
      "Epoch 301, Loss: 3.897526264190674\n",
      "Epoch 351, Loss: 3.863367795944214\n",
      "Epoch 401, Loss: 3.833127498626709\n",
      "Epoch 451, Loss: 3.806087017059326\n",
      "Train RMSE: 2.2594842474184014\n",
      "Test RMSE: 2.3833868744865154\n",
      "R2 Train: 0.02880677985097646\n",
      "R2 Test: 0.008401269476431827\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 20.95458221435547\n",
      "Epoch 51, Loss: 4.013247489929199\n",
      "Epoch 101, Loss: 4.04715633392334\n",
      "Epoch 151, Loss: 4.025546550750732\n",
      "Epoch 201, Loss: 3.9806437492370605\n",
      "Epoch 251, Loss: 3.936497688293457\n",
      "Epoch 301, Loss: 3.897526264190674\n",
      "Epoch 351, Loss: 3.863367795944214\n",
      "Epoch 401, Loss: 3.833127498626709\n",
      "Epoch 451, Loss: 3.806087017059326\n",
      "Train RMSE: 2.2594842474184014\n",
      "Test RMSE: 2.3833868744865154\n",
      "R2 Train: 0.02880677985097646\n",
      "R2 Test: 0.008401269476431827\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 20.95458221435547\n",
      "Epoch 51, Loss: 4.013247489929199\n",
      "Epoch 101, Loss: 4.04715633392334\n",
      "Epoch 151, Loss: 4.025546550750732\n",
      "Epoch 201, Loss: 3.9806437492370605\n",
      "Epoch 251, Loss: 3.936497688293457\n",
      "Epoch 301, Loss: 3.897526264190674\n",
      "Epoch 351, Loss: 3.863367795944214\n",
      "Epoch 401, Loss: 3.833127498626709\n",
      "Epoch 451, Loss: 3.806087017059326\n",
      "Train RMSE: 2.2594842474184014\n",
      "Test RMSE: 2.3833868744865154\n",
      "R2 Train: 0.02880677985097646\n",
      "R2 Test: 0.008401269476431827\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 20.95458221435547\n",
      "Epoch 51, Loss: 4.013247489929199\n",
      "Epoch 101, Loss: 4.04715633392334\n",
      "Epoch 151, Loss: 4.025546550750732\n",
      "Epoch 201, Loss: 3.9806437492370605\n",
      "Epoch 251, Loss: 3.936497688293457\n",
      "Epoch 301, Loss: 3.897526264190674\n",
      "Epoch 351, Loss: 3.863367795944214\n",
      "Epoch 401, Loss: 3.833127498626709\n",
      "Epoch 451, Loss: 3.806087017059326\n",
      "Train RMSE: 2.2594842474184014\n",
      "Test RMSE: 2.3833868744865154\n",
      "R2 Train: 0.02880677985097646\n",
      "R2 Test: 0.008401269476431827\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 20.95458221435547\n",
      "Epoch 51, Loss: 4.013247489929199\n",
      "Epoch 101, Loss: 4.04715633392334\n",
      "Epoch 151, Loss: 4.025546550750732\n",
      "Epoch 201, Loss: 3.9806437492370605\n",
      "Epoch 251, Loss: 3.936497688293457\n",
      "Epoch 301, Loss: 3.897526264190674\n",
      "Epoch 351, Loss: 3.863367795944214\n",
      "Epoch 401, Loss: 3.833127498626709\n",
      "Epoch 451, Loss: 3.806087017059326\n",
      "Train RMSE: 2.2594842474184014\n",
      "Test RMSE: 2.3833868744865154\n",
      "R2 Train: 0.02880677985097646\n",
      "R2 Test: 0.008401269476431827\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 21.821636199951172\n",
      "Epoch 51, Loss: 3.921922206878662\n",
      "Epoch 101, Loss: 3.897993564605713\n",
      "Epoch 151, Loss: 3.8904898166656494\n",
      "Epoch 201, Loss: 3.882502555847168\n",
      "Epoch 251, Loss: 3.8770506381988525\n",
      "Epoch 301, Loss: 3.8749477863311768\n",
      "Epoch 351, Loss: 3.875244140625\n",
      "Epoch 401, Loss: 3.8768739700317383\n",
      "Epoch 451, Loss: 3.8792054653167725\n",
      "Train RMSE: 2.2686866808514483\n",
      "Test RMSE: 2.39062415585045\n",
      "R2 Train: 0.02087971332483818\n",
      "R2 Test: 0.002370041419836433\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 21.823760986328125\n",
      "Epoch 51, Loss: 3.851274251937866\n",
      "Epoch 101, Loss: 3.7358412742614746\n",
      "Epoch 151, Loss: 3.64845871925354\n",
      "Epoch 201, Loss: 3.5739831924438477\n",
      "Epoch 251, Loss: 3.513352394104004\n",
      "Epoch 301, Loss: 3.4657249450683594\n",
      "Epoch 351, Loss: 3.4297285079956055\n",
      "Epoch 401, Loss: 3.403770923614502\n",
      "Epoch 451, Loss: 3.3859434127807617\n",
      "Train RMSE: 2.2958260706083857\n",
      "Test RMSE: 2.3924786091600696\n",
      "R2 Train: -0.002686049852718142\n",
      "R2 Test: 0.0008216794685376172\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 21.798473358154297\n",
      "Epoch 51, Loss: 3.7316994667053223\n",
      "Epoch 101, Loss: 3.524911642074585\n",
      "Epoch 151, Loss: 3.4024009704589844\n",
      "Epoch 201, Loss: 3.3147807121276855\n",
      "Epoch 251, Loss: 3.2581541538238525\n",
      "Epoch 301, Loss: 3.2286360263824463\n",
      "Epoch 351, Loss: 3.2177860736846924\n",
      "Epoch 401, Loss: 3.215942859649658\n",
      "Epoch 451, Loss: 3.2169888019561768\n",
      "Train RMSE: 2.2705638882566475\n",
      "Test RMSE: 2.390321742146494\n",
      "R2 Train: 0.019258711806508733\n",
      "R2 Test: 0.0026224256249366418\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 21.816123962402344\n",
      "Epoch 51, Loss: 3.8804163932800293\n",
      "Epoch 101, Loss: 3.8513882160186768\n",
      "Epoch 151, Loss: 3.848153829574585\n",
      "Epoch 201, Loss: 3.8373374938964844\n",
      "Epoch 251, Loss: 3.8224122524261475\n",
      "Epoch 301, Loss: 3.8064393997192383\n",
      "Epoch 351, Loss: 3.79068922996521\n",
      "Epoch 401, Loss: 3.7757647037506104\n",
      "Epoch 451, Loss: 3.7619643211364746\n",
      "Train RMSE: 2.2958330964254414\n",
      "Test RMSE: 2.3936824376828003\n",
      "R2 Train: -0.0026921868154612394\n",
      "R2 Test: -0.00018409084393788966\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 21.831846237182617\n",
      "Epoch 51, Loss: 3.893038749694824\n",
      "Epoch 101, Loss: 3.7907907962799072\n",
      "Epoch 151, Loss: 3.700279712677002\n",
      "Epoch 201, Loss: 3.6158957481384277\n",
      "Epoch 251, Loss: 3.541745185852051\n",
      "Epoch 301, Loss: 3.4797134399414062\n",
      "Epoch 351, Loss: 3.4300427436828613\n",
      "Epoch 401, Loss: 3.3923239707946777\n",
      "Epoch 451, Loss: 3.3657121658325195\n",
      "Train RMSE: 2.261335669507215\n",
      "Test RMSE: 2.38169423238015\n",
      "R2 Train: 0.027214535727554612\n",
      "R2 Test: 0.009809203528696409\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 6.559671878814697\n",
      "Epoch 51, Loss: 3.954737663269043\n",
      "Epoch 101, Loss: 3.8841476440429688\n",
      "Epoch 151, Loss: 3.812464475631714\n",
      "Epoch 201, Loss: 3.7578797340393066\n",
      "Epoch 251, Loss: 3.713376760482788\n",
      "Epoch 301, Loss: 3.675295829772949\n",
      "Epoch 351, Loss: 3.6418166160583496\n",
      "Epoch 401, Loss: 3.611922025680542\n",
      "Epoch 451, Loss: 3.584977865219116\n",
      "Train RMSE: 2.290828403063031\n",
      "Test RMSE: 2.394694841676949\n",
      "R2 Train: 0.0016745917462966942\n",
      "R2 Test: -0.0010303221478178415\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 6.55730676651001\n",
      "Epoch 51, Loss: 3.971341848373413\n",
      "Epoch 101, Loss: 3.9428699016571045\n",
      "Epoch 151, Loss: 3.9095683097839355\n",
      "Epoch 201, Loss: 3.8830742835998535\n",
      "Epoch 251, Loss: 3.8597829341888428\n",
      "Epoch 301, Loss: 3.8387868404388428\n",
      "Epoch 351, Loss: 3.8196678161621094\n",
      "Epoch 401, Loss: 3.8021345138549805\n",
      "Epoch 451, Loss: 3.7859673500061035\n",
      "Train RMSE: 2.291438820573589\n",
      "Test RMSE: 2.3948506697594674\n",
      "R2 Train: 0.001142490345943381\n",
      "R2 Test: -0.0011606048946555259\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 6.556884765625\n",
      "Epoch 51, Loss: 4.001791954040527\n",
      "Epoch 101, Loss: 3.9894521236419678\n",
      "Epoch 151, Loss: 3.9551761150360107\n",
      "Epoch 201, Loss: 3.9200243949890137\n",
      "Epoch 251, Loss: 3.8856489658355713\n",
      "Epoch 301, Loss: 3.853283166885376\n",
      "Epoch 351, Loss: 3.8234448432922363\n",
      "Epoch 401, Loss: 3.7962300777435303\n",
      "Epoch 451, Loss: 3.7715342044830322\n",
      "Train RMSE: 2.2911095663299217\n",
      "Test RMSE: 2.3949772452260403\n",
      "R2 Train: 0.0014295191270407015\n",
      "R2 Test: -0.0012664367288650613\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 6.557180404663086\n",
      "Epoch 51, Loss: 3.950324058532715\n",
      "Epoch 101, Loss: 3.8871195316314697\n",
      "Epoch 151, Loss: 3.823397636413574\n",
      "Epoch 201, Loss: 3.7797775268554688\n",
      "Epoch 251, Loss: 3.748717784881592\n",
      "Epoch 301, Loss: 3.725358724594116\n",
      "Epoch 351, Loss: 3.706936836242676\n",
      "Epoch 401, Loss: 3.6918325424194336\n",
      "Epoch 451, Loss: 3.679060697555542\n",
      "Train RMSE: 2.292631564341579\n",
      "Test RMSE: 2.3954261653141953\n",
      "R2 Train: 0.00010236557461118512\n",
      "R2 Test: -0.0016418313131207896\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 6.559908866882324\n",
      "Epoch 51, Loss: 3.956181764602661\n",
      "Epoch 101, Loss: 3.8873114585876465\n",
      "Epoch 151, Loss: 3.8146145343780518\n",
      "Epoch 201, Loss: 3.7608439922332764\n",
      "Epoch 251, Loss: 3.7206356525421143\n",
      "Epoch 301, Loss: 3.68992280960083\n",
      "Epoch 351, Loss: 3.666046142578125\n",
      "Epoch 401, Loss: 3.6472156047821045\n",
      "Epoch 451, Loss: 3.6321933269500732\n",
      "Train RMSE: 2.2913718033216166\n",
      "Test RMSE: 2.3929735698446484\n",
      "R2 Train: 0.0012009162623656255\n",
      "R2 Test: 0.00040821274762015136\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 21.26361083984375\n",
      "Epoch 51, Loss: 4.018489360809326\n",
      "Epoch 101, Loss: 4.05281400680542\n",
      "Epoch 151, Loss: 4.091620922088623\n",
      "Epoch 201, Loss: 4.118879318237305\n",
      "Epoch 251, Loss: 4.1383867263793945\n",
      "Epoch 301, Loss: 4.152754783630371\n",
      "Epoch 351, Loss: 4.163261413574219\n",
      "Epoch 401, Loss: 4.170768737792969\n",
      "Epoch 451, Loss: 4.175943851470947\n",
      "Train RMSE: 2.265147650304554\n",
      "Test RMSE: 2.376630048080238\n",
      "R2 Train: 0.02393208183253792\n",
      "R2 Test: 0.01401560202870833\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 21.272029876708984\n",
      "Epoch 51, Loss: 4.011570930480957\n",
      "Epoch 101, Loss: 4.043635845184326\n",
      "Epoch 151, Loss: 4.082562446594238\n",
      "Epoch 201, Loss: 4.11073637008667\n",
      "Epoch 251, Loss: 4.131524562835693\n",
      "Epoch 301, Loss: 4.147217750549316\n",
      "Epoch 351, Loss: 4.158848285675049\n",
      "Epoch 401, Loss: 4.167121887207031\n",
      "Epoch 451, Loss: 4.172626972198486\n",
      "Train RMSE: 2.264800125442145\n",
      "Test RMSE: 2.375905765280193\n",
      "R2 Train: 0.024231560630610716\n",
      "R2 Test: 0.014616471923984076\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 21.233266830444336\n",
      "Epoch 51, Loss: 4.017540454864502\n",
      "Epoch 101, Loss: 4.054844856262207\n",
      "Epoch 151, Loss: 4.0985212326049805\n",
      "Epoch 201, Loss: 4.129443645477295\n",
      "Epoch 251, Loss: 4.150542259216309\n",
      "Epoch 301, Loss: 4.164485931396484\n",
      "Epoch 351, Loss: 4.1728949546813965\n",
      "Epoch 401, Loss: 4.177033424377441\n",
      "Epoch 451, Loss: 4.177951335906982\n",
      "Train RMSE: 2.2598350750795\n",
      "Test RMSE: 2.3620605400096166\n",
      "R2 Train: 0.028505164207483058\n",
      "R2 Test: 0.02606735208779587\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 21.25947380065918\n",
      "Epoch 51, Loss: 3.9960813522338867\n",
      "Epoch 101, Loss: 4.02012825012207\n",
      "Epoch 151, Loss: 4.048883438110352\n",
      "Epoch 201, Loss: 4.068301677703857\n",
      "Epoch 251, Loss: 4.081915378570557\n",
      "Epoch 301, Loss: 4.091698169708252\n",
      "Epoch 351, Loss: 4.098467826843262\n",
      "Epoch 401, Loss: 4.102783203125\n",
      "Epoch 451, Loss: 4.105116844177246\n",
      "Train RMSE: 2.264982079791037\n",
      "Test RMSE: 2.372987288591429\n",
      "R2 Train: 0.024074767595262392\n",
      "R2 Test: 0.01703580405597227\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 21.276247024536133\n",
      "Epoch 51, Loss: 4.017314434051514\n",
      "Epoch 101, Loss: 4.047427654266357\n",
      "Epoch 151, Loss: 4.083071708679199\n",
      "Epoch 201, Loss: 4.107945442199707\n",
      "Epoch 251, Loss: 4.125758647918701\n",
      "Epoch 301, Loss: 4.138916969299316\n",
      "Epoch 351, Loss: 4.14842414855957\n",
      "Epoch 401, Loss: 4.154914379119873\n",
      "Epoch 451, Loss: 4.158897876739502\n",
      "Train RMSE: 2.2652360395525784\n",
      "Test RMSE: 2.3732284454061556\n",
      "R2 Train: 0.02385590525962411\n",
      "R2 Test: 0.016836004789663406\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 6.8324198722839355\n",
      "Epoch 51, Loss: 3.747087001800537\n",
      "Epoch 101, Loss: 3.72641658782959\n",
      "Epoch 151, Loss: 3.7190334796905518\n",
      "Epoch 201, Loss: 3.712709903717041\n",
      "Epoch 251, Loss: 3.7079381942749023\n",
      "Epoch 301, Loss: 3.7043797969818115\n",
      "Epoch 351, Loss: 3.701663017272949\n",
      "Epoch 401, Loss: 3.699526309967041\n",
      "Epoch 451, Loss: 3.6977970600128174\n",
      "Train RMSE: 2.2120026884604806\n",
      "Test RMSE: 2.3985926202252705\n",
      "R2 Train: 0.06919585811756124\n",
      "R2 Test: -0.004291672921556744\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 6.74984073638916\n",
      "Epoch 51, Loss: 3.915407657623291\n",
      "Epoch 101, Loss: 3.934856414794922\n",
      "Epoch 151, Loss: 3.926154136657715\n",
      "Epoch 201, Loss: 3.914716958999634\n",
      "Epoch 251, Loss: 3.9048948287963867\n",
      "Epoch 301, Loss: 3.8965303897857666\n",
      "Epoch 351, Loss: 3.8892481327056885\n",
      "Epoch 401, Loss: 3.882791757583618\n",
      "Epoch 451, Loss: 3.877007007598877\n",
      "Train RMSE: 2.2122671721966456\n",
      "Test RMSE: 2.4083389949087337\n",
      "R2 Train: 0.06897325687381983\n",
      "R2 Test: -0.012469876597452467\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 6.8248114585876465\n",
      "Epoch 51, Loss: 3.7847418785095215\n",
      "Epoch 101, Loss: 3.808560848236084\n",
      "Epoch 151, Loss: 3.8062283992767334\n",
      "Epoch 201, Loss: 3.800217866897583\n",
      "Epoch 251, Loss: 3.7952804565429688\n",
      "Epoch 301, Loss: 3.7914342880249023\n",
      "Epoch 351, Loss: 3.7883541584014893\n",
      "Epoch 401, Loss: 3.7858006954193115\n",
      "Epoch 451, Loss: 3.783618688583374\n",
      "Train RMSE: 2.210964845970506\n",
      "Test RMSE: 2.3985739773365053\n",
      "R2 Train: 0.07006909527003746\n",
      "R2 Test: -0.0042760614125185725\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 6.759737968444824\n",
      "Epoch 51, Loss: 3.8388638496398926\n",
      "Epoch 101, Loss: 3.859480619430542\n",
      "Epoch 151, Loss: 3.843289375305176\n",
      "Epoch 201, Loss: 3.8277535438537598\n",
      "Epoch 251, Loss: 3.8162434101104736\n",
      "Epoch 301, Loss: 3.8076512813568115\n",
      "Epoch 351, Loss: 3.8010122776031494\n",
      "Epoch 401, Loss: 3.7957026958465576\n",
      "Epoch 451, Loss: 3.7913312911987305\n",
      "Train RMSE: 2.209964873565791\n",
      "Test RMSE: 2.391227126162689\n",
      "R2 Train: 0.07091008101119711\n",
      "R2 Test: 0.0018667275823933682\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 6.6984782218933105\n",
      "Epoch 51, Loss: 3.660494804382324\n",
      "Epoch 101, Loss: 3.6560235023498535\n",
      "Epoch 151, Loss: 3.646878719329834\n",
      "Epoch 201, Loss: 3.63899827003479\n",
      "Epoch 251, Loss: 3.6339662075042725\n",
      "Epoch 301, Loss: 3.630861520767212\n",
      "Epoch 351, Loss: 3.628950834274292\n",
      "Epoch 401, Loss: 3.627774238586426\n",
      "Epoch 451, Loss: 3.6270556449890137\n",
      "Train RMSE: 2.2116049092915815\n",
      "Test RMSE: 2.4027516591222877\n",
      "R2 Train: 0.06953059659586913\n",
      "R2 Test: -0.007777474848313259\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 2.2594842474184014\n",
      "GensimLDA Average Test RMSE: 2.3833868744865154\n",
      "GensimLDA Average R2 Train: 0.02880677985097646\n",
      "GensimLDA Average R2 Test: 0.008401269476431827\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 2.2784490811298275\n",
      "Mallet_LDA Average Test RMSE: 2.389760235443993\n",
      "Mallet_LDA Average R2 Train: 0.012394944838144429\n",
      "Mallet_LDA Average R2 Test: 0.003087851839613842\n",
      "\n",
      "CTM Average Train RMSE: 2.291476031525948\n",
      "CTM Average Test RMSE: 2.39458449836426\n",
      "CTM Average R2 Train: 0.0011099766112515174\n",
      "CTM Average R2 Test: -0.0009381964673678134\n",
      "\n",
      "BERTopic Average Train RMSE: 2.264000194033963\n",
      "BERTopic Average Test RMSE: 2.3721624174735267\n",
      "BERTopic Average R2 Train: 0.02491989590510364\n",
      "BERTopic Average R2 Test: 0.01771424697722479\n",
      "\n",
      "NMF Average Train RMSE: 2.211360897897001\n",
      "NMF Average Test RMSE: 2.3998968755510974\n",
      "NMF Average R2 Train: 0.06973577757369695\n",
      "NMF Average R2 Test: -0.005389671639489535\n",
      "\n",
      "stress\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 200.19775390625\n",
      "Epoch 51, Loss: 24.068275451660156\n",
      "Epoch 101, Loss: 24.116657257080078\n",
      "Epoch 151, Loss: 24.307064056396484\n",
      "Epoch 201, Loss: 24.480215072631836\n",
      "Epoch 251, Loss: 24.580310821533203\n",
      "Epoch 301, Loss: 24.61534309387207\n",
      "Epoch 351, Loss: 24.60594367980957\n",
      "Epoch 401, Loss: 24.569438934326172\n",
      "Epoch 451, Loss: 24.517574310302734\n",
      "Train RMSE: 6.935160226459724\n",
      "Test RMSE: 7.458611057330953\n",
      "R2 Train: 0.04077307928663698\n",
      "R2 Test: 0.011256528910372099\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 200.19775390625\n",
      "Epoch 51, Loss: 24.068275451660156\n",
      "Epoch 101, Loss: 24.116657257080078\n",
      "Epoch 151, Loss: 24.307064056396484\n",
      "Epoch 201, Loss: 24.480215072631836\n",
      "Epoch 251, Loss: 24.580310821533203\n",
      "Epoch 301, Loss: 24.61534309387207\n",
      "Epoch 351, Loss: 24.60594367980957\n",
      "Epoch 401, Loss: 24.569438934326172\n",
      "Epoch 451, Loss: 24.517574310302734\n",
      "Train RMSE: 6.935160226459724\n",
      "Test RMSE: 7.458611057330953\n",
      "R2 Train: 0.04077307928663698\n",
      "R2 Test: 0.011256528910372099\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 200.19775390625\n",
      "Epoch 51, Loss: 24.068275451660156\n",
      "Epoch 101, Loss: 24.116657257080078\n",
      "Epoch 151, Loss: 24.307064056396484\n",
      "Epoch 201, Loss: 24.480215072631836\n",
      "Epoch 251, Loss: 24.580310821533203\n",
      "Epoch 301, Loss: 24.61534309387207\n",
      "Epoch 351, Loss: 24.60594367980957\n",
      "Epoch 401, Loss: 24.569438934326172\n",
      "Epoch 451, Loss: 24.517574310302734\n",
      "Train RMSE: 6.935160226459724\n",
      "Test RMSE: 7.458611057330953\n",
      "R2 Train: 0.04077307928663698\n",
      "R2 Test: 0.011256528910372099\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 200.19775390625\n",
      "Epoch 51, Loss: 24.068275451660156\n",
      "Epoch 101, Loss: 24.116657257080078\n",
      "Epoch 151, Loss: 24.307064056396484\n",
      "Epoch 201, Loss: 24.480215072631836\n",
      "Epoch 251, Loss: 24.580310821533203\n",
      "Epoch 301, Loss: 24.61534309387207\n",
      "Epoch 351, Loss: 24.60594367980957\n",
      "Epoch 401, Loss: 24.569438934326172\n",
      "Epoch 451, Loss: 24.517574310302734\n",
      "Train RMSE: 6.935160226459724\n",
      "Test RMSE: 7.458611057330953\n",
      "R2 Train: 0.04077307928663698\n",
      "R2 Test: 0.011256528910372099\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 200.19775390625\n",
      "Epoch 51, Loss: 24.068275451660156\n",
      "Epoch 101, Loss: 24.116657257080078\n",
      "Epoch 151, Loss: 24.307064056396484\n",
      "Epoch 201, Loss: 24.480215072631836\n",
      "Epoch 251, Loss: 24.580310821533203\n",
      "Epoch 301, Loss: 24.61534309387207\n",
      "Epoch 351, Loss: 24.60594367980957\n",
      "Epoch 401, Loss: 24.569438934326172\n",
      "Epoch 451, Loss: 24.517574310302734\n",
      "Train RMSE: 6.935160226459724\n",
      "Test RMSE: 7.458611057330953\n",
      "R2 Train: 0.04077307928663698\n",
      "R2 Test: 0.011256528910372099\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 203.10398864746094\n",
      "Epoch 51, Loss: 23.605079650878906\n",
      "Epoch 101, Loss: 23.59761619567871\n",
      "Epoch 151, Loss: 23.607261657714844\n",
      "Epoch 201, Loss: 23.62459373474121\n",
      "Epoch 251, Loss: 23.646339416503906\n",
      "Epoch 301, Loss: 23.669971466064453\n",
      "Epoch 351, Loss: 23.69379997253418\n",
      "Epoch 401, Loss: 23.716909408569336\n",
      "Epoch 451, Loss: 23.738964080810547\n",
      "Train RMSE: 7.036049215525821\n",
      "Test RMSE: 7.499513099125726\n",
      "R2 Train: 0.01266144244666667\n",
      "R2 Test: 0.0003825163770690976\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 202.99607849121094\n",
      "Epoch 51, Loss: 23.37171173095703\n",
      "Epoch 101, Loss: 23.07791519165039\n",
      "Epoch 151, Loss: 22.84429168701172\n",
      "Epoch 201, Loss: 22.65287208557129\n",
      "Epoch 251, Loss: 22.491716384887695\n",
      "Epoch 301, Loss: 22.35323715209961\n",
      "Epoch 351, Loss: 22.23276138305664\n",
      "Epoch 401, Loss: 22.127504348754883\n",
      "Epoch 451, Loss: 22.035724639892578\n",
      "Train RMSE: 7.013138289966751\n",
      "Test RMSE: 7.48469833134613\n",
      "R2 Train: 0.01908095722934311\n",
      "R2 Test: 0.0043279655061275735\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 202.92176818847656\n",
      "Epoch 51, Loss: 23.333782196044922\n",
      "Epoch 101, Loss: 22.915061950683594\n",
      "Epoch 151, Loss: 22.55265235900879\n",
      "Epoch 201, Loss: 22.23049545288086\n",
      "Epoch 251, Loss: 21.943479537963867\n",
      "Epoch 301, Loss: 21.688289642333984\n",
      "Epoch 351, Loss: 21.46118927001953\n",
      "Epoch 401, Loss: 21.25824737548828\n",
      "Epoch 451, Loss: 21.076162338256836\n",
      "Train RMSE: 7.080465436682952\n",
      "Test RMSE: 7.498247789824456\n",
      "R2 Train: 0.00015662228265478362\n",
      "R2 Test: 0.0007197965666181361\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 203.03656005859375\n",
      "Epoch 51, Loss: 23.584239959716797\n",
      "Epoch 101, Loss: 23.61598777770996\n",
      "Epoch 151, Loss: 23.63454246520996\n",
      "Epoch 201, Loss: 23.633169174194336\n",
      "Epoch 251, Loss: 23.619102478027344\n",
      "Epoch 301, Loss: 23.59838104248047\n",
      "Epoch 351, Loss: 23.574565887451172\n",
      "Epoch 401, Loss: 23.54969024658203\n",
      "Epoch 451, Loss: 23.524860382080078\n",
      "Train RMSE: 7.021000394997976\n",
      "Test RMSE: 7.490374905912221\n",
      "R2 Train: 0.016880398490796922\n",
      "R2 Test: 0.0028171097293231417\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 203.0880584716797\n",
      "Epoch 51, Loss: 23.477943420410156\n",
      "Epoch 101, Loss: 23.315046310424805\n",
      "Epoch 151, Loss: 23.205902099609375\n",
      "Epoch 201, Loss: 23.1280517578125\n",
      "Epoch 251, Loss: 23.06830596923828\n",
      "Epoch 301, Loss: 23.02004623413086\n",
      "Epoch 351, Loss: 22.979997634887695\n",
      "Epoch 401, Loss: 22.94637680053711\n",
      "Epoch 451, Loss: 22.918102264404297\n",
      "Train RMSE: 6.9955121490331775\n",
      "Test RMSE: 7.451370444195278\n",
      "R2 Train: 0.02400545445275415\n",
      "R2 Test: 0.013175286699774302\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 130.4610137939453\n",
      "Epoch 51, Loss: 23.50737190246582\n",
      "Epoch 101, Loss: 23.194026947021484\n",
      "Epoch 151, Loss: 22.923431396484375\n",
      "Epoch 201, Loss: 22.730012893676758\n",
      "Epoch 251, Loss: 22.595577239990234\n",
      "Epoch 301, Loss: 22.49864387512207\n",
      "Epoch 351, Loss: 22.425273895263672\n",
      "Epoch 401, Loss: 22.367454528808594\n",
      "Epoch 451, Loss: 22.320547103881836\n",
      "Train RMSE: 7.082843637008282\n",
      "Test RMSE: 7.500972700922085\n",
      "R2 Train: -0.0005151491431434341\n",
      "R2 Test: -6.625008386107467e-06\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 130.44778442382812\n",
      "Epoch 51, Loss: 23.56296157836914\n",
      "Epoch 101, Loss: 23.317766189575195\n",
      "Epoch 151, Loss: 23.099679946899414\n",
      "Epoch 201, Loss: 22.940866470336914\n",
      "Epoch 251, Loss: 22.829410552978516\n",
      "Epoch 301, Loss: 22.750579833984375\n",
      "Epoch 351, Loss: 22.69423484802246\n",
      "Epoch 401, Loss: 22.653785705566406\n",
      "Epoch 451, Loss: 22.624839782714844\n",
      "Train RMSE: 7.082797970628384\n",
      "Test RMSE: 7.5009606561690765\n",
      "R2 Train: -0.0005022476137734877\n",
      "R2 Test: -3.4134720654499517e-06\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 130.44610595703125\n",
      "Epoch 51, Loss: 23.57491683959961\n",
      "Epoch 101, Loss: 23.35932159423828\n",
      "Epoch 151, Loss: 23.156009674072266\n",
      "Epoch 201, Loss: 22.992189407348633\n",
      "Epoch 251, Loss: 22.862213134765625\n",
      "Epoch 301, Loss: 22.756771087646484\n",
      "Epoch 351, Loss: 22.669212341308594\n",
      "Epoch 401, Loss: 22.595121383666992\n",
      "Epoch 451, Loss: 22.53147315979004\n",
      "Train RMSE: 7.082765301448495\n",
      "Test RMSE: 7.500983299543912\n",
      "R2 Train: -0.0004930180656030902\n",
      "R2 Test: -9.450961753643128e-06\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 130.44590759277344\n",
      "Epoch 51, Loss: 23.434558868408203\n",
      "Epoch 101, Loss: 22.99325942993164\n",
      "Epoch 151, Loss: 22.59972381591797\n",
      "Epoch 201, Loss: 22.30832290649414\n",
      "Epoch 251, Loss: 22.098060607910156\n",
      "Epoch 301, Loss: 21.9434814453125\n",
      "Epoch 351, Loss: 21.82692527770996\n",
      "Epoch 401, Loss: 21.73694610595703\n",
      "Epoch 451, Loss: 21.66605567932129\n",
      "Train RMSE: 7.082708802232568\n",
      "Test RMSE: 7.501199240842461\n",
      "R2 Train: -0.00047705626447092087\n",
      "R2 Test: -6.702913227063512e-05\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 130.46388244628906\n",
      "Epoch 51, Loss: 23.634361267089844\n",
      "Epoch 101, Loss: 23.481082916259766\n",
      "Epoch 151, Loss: 23.28821563720703\n",
      "Epoch 201, Loss: 23.09484100341797\n",
      "Epoch 251, Loss: 22.914674758911133\n",
      "Epoch 301, Loss: 22.74939727783203\n",
      "Epoch 351, Loss: 22.598323822021484\n",
      "Epoch 401, Loss: 22.460369110107422\n",
      "Epoch 451, Loss: 22.334400177001953\n",
      "Train RMSE: 7.082820704635536\n",
      "Test RMSE: 7.5009279186349875\n",
      "R2 Train: -0.000508670347233009\n",
      "R2 Test: 5.315429710384123e-06\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 201.1344451904297\n",
      "Epoch 51, Loss: 23.974153518676758\n",
      "Epoch 101, Loss: 23.79633331298828\n",
      "Epoch 151, Loss: 23.694204330444336\n",
      "Epoch 201, Loss: 23.644609451293945\n",
      "Epoch 251, Loss: 23.62548065185547\n",
      "Epoch 301, Loss: 23.622957229614258\n",
      "Epoch 351, Loss: 23.629772186279297\n",
      "Epoch 401, Loss: 23.64217185974121\n",
      "Epoch 451, Loss: 23.658138275146484\n",
      "Train RMSE: 7.020923607120848\n",
      "Test RMSE: 7.469602276214298\n",
      "R2 Train: 0.016901902906149235\n",
      "R2 Test: 0.008340301438438003\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 201.18873596191406\n",
      "Epoch 51, Loss: 23.95918083190918\n",
      "Epoch 101, Loss: 23.784679412841797\n",
      "Epoch 151, Loss: 23.65618896484375\n",
      "Epoch 201, Loss: 23.577178955078125\n",
      "Epoch 251, Loss: 23.53226089477539\n",
      "Epoch 301, Loss: 23.50778579711914\n",
      "Epoch 351, Loss: 23.495473861694336\n",
      "Epoch 401, Loss: 23.490642547607422\n",
      "Epoch 451, Loss: 23.49061393737793\n",
      "Train RMSE: 7.023140113381184\n",
      "Test RMSE: 7.4575723933195235\n",
      "R2 Train: 0.016281076595895727\n",
      "R2 Test: 0.011531888691304726\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 201.0655975341797\n",
      "Epoch 51, Loss: 24.025203704833984\n",
      "Epoch 101, Loss: 23.8736572265625\n",
      "Epoch 151, Loss: 23.795663833618164\n",
      "Epoch 201, Loss: 23.781917572021484\n",
      "Epoch 251, Loss: 23.80825424194336\n",
      "Epoch 301, Loss: 23.857078552246094\n",
      "Epoch 351, Loss: 23.91815948486328\n",
      "Epoch 401, Loss: 23.985647201538086\n",
      "Epoch 451, Loss: 24.05609893798828\n",
      "Train RMSE: 7.006163295146059\n",
      "Test RMSE: 7.450193544007348\n",
      "R2 Train: 0.021031154876546165\n",
      "R2 Test: 0.013486988414657008\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 201.14122009277344\n",
      "Epoch 51, Loss: 23.960914611816406\n",
      "Epoch 101, Loss: 23.786436080932617\n",
      "Epoch 151, Loss: 23.667146682739258\n",
      "Epoch 201, Loss: 23.596940994262695\n",
      "Epoch 251, Loss: 23.55990219116211\n",
      "Epoch 301, Loss: 23.5430908203125\n",
      "Epoch 351, Loss: 23.538698196411133\n",
      "Epoch 401, Loss: 23.542299270629883\n",
      "Epoch 451, Loss: 23.551321029663086\n",
      "Train RMSE: 7.015416991228169\n",
      "Test RMSE: 7.464176583306118\n",
      "R2 Train: 0.01844341537559968\n",
      "R2 Test: 0.009780401379564263\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 201.19996643066406\n",
      "Epoch 51, Loss: 23.931217193603516\n",
      "Epoch 101, Loss: 23.76420783996582\n",
      "Epoch 151, Loss: 23.666988372802734\n",
      "Epoch 201, Loss: 23.62169075012207\n",
      "Epoch 251, Loss: 23.605693817138672\n",
      "Epoch 301, Loss: 23.605295181274414\n",
      "Epoch 351, Loss: 23.613780975341797\n",
      "Epoch 401, Loss: 23.627933502197266\n",
      "Epoch 451, Loss: 23.646106719970703\n",
      "Train RMSE: 7.015334721182515\n",
      "Test RMSE: 7.459262005691932\n",
      "R2 Train: 0.018466436738872827\n",
      "R2 Test: 0.011083936715146181\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 118.94658660888672\n",
      "Epoch 51, Loss: 22.979320526123047\n",
      "Epoch 101, Loss: 20.211734771728516\n",
      "Epoch 151, Loss: 19.373106002807617\n",
      "Epoch 201, Loss: 19.08974838256836\n",
      "Epoch 251, Loss: 18.968141555786133\n",
      "Epoch 301, Loss: 18.901960372924805\n",
      "Epoch 351, Loss: 18.859220504760742\n",
      "Epoch 401, Loss: 18.82847785949707\n",
      "Epoch 451, Loss: 18.80486297607422\n",
      "Train RMSE: 6.800433240877696\n",
      "Test RMSE: 7.5585201148301575\n",
      "R2 Train: 0.07768021838490657\n",
      "R2 Test: -0.015409573564290291\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 116.98037719726562\n",
      "Epoch 51, Loss: 23.885141372680664\n",
      "Epoch 101, Loss: 21.65577507019043\n",
      "Epoch 151, Loss: 21.032087326049805\n",
      "Epoch 201, Loss: 20.848604202270508\n",
      "Epoch 251, Loss: 20.78055191040039\n",
      "Epoch 301, Loss: 20.74837875366211\n",
      "Epoch 351, Loss: 20.73052215576172\n",
      "Epoch 401, Loss: 20.720022201538086\n",
      "Epoch 451, Loss: 20.714052200317383\n",
      "Train RMSE: 6.80508261223305\n",
      "Test RMSE: 7.568404775540151\n",
      "R2 Train: 0.07641863021308215\n",
      "R2 Test: -0.018067115489826646\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 118.16749572753906\n",
      "Epoch 51, Loss: 23.484561920166016\n",
      "Epoch 101, Loss: 21.15078353881836\n",
      "Epoch 151, Loss: 20.463956832885742\n",
      "Epoch 201, Loss: 20.20862579345703\n",
      "Epoch 251, Loss: 20.077054977416992\n",
      "Epoch 301, Loss: 19.990766525268555\n",
      "Epoch 351, Loss: 19.92646598815918\n",
      "Epoch 401, Loss: 19.875627517700195\n",
      "Epoch 451, Loss: 19.834339141845703\n",
      "Train RMSE: 6.80744686409454\n",
      "Test RMSE: 7.561414193681203\n",
      "R2 Train: 0.07577676929484534\n",
      "R2 Test: -0.016187301987708347\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 118.87093353271484\n",
      "Epoch 51, Loss: 24.097654342651367\n",
      "Epoch 101, Loss: 21.676496505737305\n",
      "Epoch 151, Loss: 21.01204490661621\n",
      "Epoch 201, Loss: 20.82547378540039\n",
      "Epoch 251, Loss: 20.759946823120117\n",
      "Epoch 301, Loss: 20.730100631713867\n",
      "Epoch 351, Loss: 20.713590621948242\n",
      "Epoch 401, Loss: 20.703502655029297\n",
      "Epoch 451, Loss: 20.697246551513672\n",
      "Train RMSE: 6.799201758699042\n",
      "Test RMSE: 7.547771114757117\n",
      "R2 Train: 0.07801423167249755\n",
      "R2 Test: -0.012523591509338683\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 118.18392944335938\n",
      "Epoch 51, Loss: 23.582477569580078\n",
      "Epoch 101, Loss: 20.983110427856445\n",
      "Epoch 151, Loss: 20.155717849731445\n",
      "Epoch 201, Loss: 19.858089447021484\n",
      "Epoch 251, Loss: 19.710866928100586\n",
      "Epoch 301, Loss: 19.616018295288086\n",
      "Epoch 351, Loss: 19.545284271240234\n",
      "Epoch 401, Loss: 19.48882293701172\n",
      "Epoch 451, Loss: 19.442293167114258\n",
      "Train RMSE: 6.820632889788662\n",
      "Test RMSE: 7.576250088585882\n",
      "R2 Train: 0.07219286053792007\n",
      "R2 Test: -0.020178840509825546\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 6.935160226459724\n",
      "GensimLDA Average Test RMSE: 7.458611057330953\n",
      "GensimLDA Average R2 Train: 0.04077307928663698\n",
      "GensimLDA Average R2 Test: 0.011256528910372099\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 7.0292330972413355\n",
      "Mallet_LDA Average Test RMSE: 7.4848409140807615\n",
      "Mallet_LDA Average R2 Train: 0.014556974980443126\n",
      "Mallet_LDA Average R2 Test: 0.00428453497578245\n",
      "\n",
      "CTM Average Train RMSE: 7.082787283190653\n",
      "CTM Average Test RMSE: 7.5010087632225035\n",
      "CTM Average R2 Train: -0.0004992282868447884\n",
      "CTM Average R2 Test: -1.6240628953090307e-05\n",
      "\n",
      "BERTopic Average Train RMSE: 7.016195745611755\n",
      "BERTopic Average Test RMSE: 7.460161360507844\n",
      "BERTopic Average R2 Train: 0.018224797298612726\n",
      "BERTopic Average R2 Test: 0.010844703327822036\n",
      "\n",
      "NMF Average Train RMSE: 6.806559473138597\n",
      "NMF Average Test RMSE: 7.562472057478901\n",
      "NMF Average R2 Train: 0.07601654202065034\n",
      "NMF Average R2 Test: -0.0164732846121979\n",
      "\n",
      "depression\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 56.50953674316406\n",
      "Epoch 51, Loss: 39.626224517822266\n",
      "Epoch 101, Loss: 38.44346237182617\n",
      "Epoch 151, Loss: 37.76380157470703\n",
      "Epoch 201, Loss: 37.28225326538086\n",
      "Epoch 251, Loss: 36.90066909790039\n",
      "Epoch 301, Loss: 36.583499908447266\n",
      "Epoch 351, Loss: 36.313297271728516\n",
      "Epoch 401, Loss: 36.07972717285156\n",
      "Epoch 451, Loss: 35.87586975097656\n",
      "Train RMSE: 5.820092279383699\n",
      "Test RMSE: 6.165841421728875\n",
      "R2 Train: 0.06346255712481286\n",
      "R2 Test: 0.023948198824667033\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 56.50953674316406\n",
      "Epoch 51, Loss: 39.626224517822266\n",
      "Epoch 101, Loss: 38.44346237182617\n",
      "Epoch 151, Loss: 37.76380157470703\n",
      "Epoch 201, Loss: 37.28225326538086\n",
      "Epoch 251, Loss: 36.90066909790039\n",
      "Epoch 301, Loss: 36.583499908447266\n",
      "Epoch 351, Loss: 36.313297271728516\n",
      "Epoch 401, Loss: 36.07972717285156\n",
      "Epoch 451, Loss: 35.87586975097656\n",
      "Train RMSE: 5.820092279383699\n",
      "Test RMSE: 6.165841421728875\n",
      "R2 Train: 0.06346255712481286\n",
      "R2 Test: 0.023948198824667033\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 56.50953674316406\n",
      "Epoch 51, Loss: 39.626224517822266\n",
      "Epoch 101, Loss: 38.44346237182617\n",
      "Epoch 151, Loss: 37.76380157470703\n",
      "Epoch 201, Loss: 37.28225326538086\n",
      "Epoch 251, Loss: 36.90066909790039\n",
      "Epoch 301, Loss: 36.583499908447266\n",
      "Epoch 351, Loss: 36.313297271728516\n",
      "Epoch 401, Loss: 36.07972717285156\n",
      "Epoch 451, Loss: 35.87586975097656\n",
      "Train RMSE: 5.820092279383699\n",
      "Test RMSE: 6.165841421728875\n",
      "R2 Train: 0.06346255712481286\n",
      "R2 Test: 0.023948198824667033\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 56.50953674316406\n",
      "Epoch 51, Loss: 39.626224517822266\n",
      "Epoch 101, Loss: 38.44346237182617\n",
      "Epoch 151, Loss: 37.76380157470703\n",
      "Epoch 201, Loss: 37.28225326538086\n",
      "Epoch 251, Loss: 36.90066909790039\n",
      "Epoch 301, Loss: 36.583499908447266\n",
      "Epoch 351, Loss: 36.313297271728516\n",
      "Epoch 401, Loss: 36.07972717285156\n",
      "Epoch 451, Loss: 35.87586975097656\n",
      "Train RMSE: 5.820092279383699\n",
      "Test RMSE: 6.165841421728875\n",
      "R2 Train: 0.06346255712481286\n",
      "R2 Test: 0.023948198824667033\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 56.50953674316406\n",
      "Epoch 51, Loss: 39.626224517822266\n",
      "Epoch 101, Loss: 38.44346237182617\n",
      "Epoch 151, Loss: 37.76380157470703\n",
      "Epoch 201, Loss: 37.28225326538086\n",
      "Epoch 251, Loss: 36.90066909790039\n",
      "Epoch 301, Loss: 36.583499908447266\n",
      "Epoch 351, Loss: 36.313297271728516\n",
      "Epoch 401, Loss: 36.07972717285156\n",
      "Epoch 451, Loss: 35.87586975097656\n",
      "Train RMSE: 5.820092279383699\n",
      "Test RMSE: 6.165841421728875\n",
      "R2 Train: 0.06346255712481286\n",
      "R2 Test: 0.023948198824667033\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 57.14885711669922\n",
      "Epoch 51, Loss: 40.87209701538086\n",
      "Epoch 101, Loss: 40.644432067871094\n",
      "Epoch 151, Loss: 40.454776763916016\n",
      "Epoch 201, Loss: 40.29205322265625\n",
      "Epoch 251, Loss: 40.15043258666992\n",
      "Epoch 301, Loss: 40.02714538574219\n",
      "Epoch 351, Loss: 39.91912841796875\n",
      "Epoch 401, Loss: 39.823890686035156\n",
      "Epoch 451, Loss: 39.7393798828125\n",
      "Train RMSE: 5.953864133420108\n",
      "Test RMSE: 6.243819146370438\n",
      "R2 Train: 0.01991612538461629\n",
      "R2 Test: -0.0008956356186922143\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 57.16524887084961\n",
      "Epoch 51, Loss: 40.55257034301758\n",
      "Epoch 101, Loss: 39.97456359863281\n",
      "Epoch 151, Loss: 39.47467041015625\n",
      "Epoch 201, Loss: 39.039859771728516\n",
      "Epoch 251, Loss: 38.65813446044922\n",
      "Epoch 301, Loss: 38.322994232177734\n",
      "Epoch 351, Loss: 38.030513763427734\n",
      "Epoch 401, Loss: 37.776039123535156\n",
      "Epoch 451, Loss: 37.55461120605469\n",
      "Train RMSE: 5.891714587621656\n",
      "Test RMSE: 6.205344802252786\n",
      "R2 Train: 0.040270588254372686\n",
      "R2 Test: 0.01140137581053402\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 57.10225296020508\n",
      "Epoch 51, Loss: 39.84070587158203\n",
      "Epoch 101, Loss: 38.998050689697266\n",
      "Epoch 151, Loss: 38.436378479003906\n",
      "Epoch 201, Loss: 38.02058029174805\n",
      "Epoch 251, Loss: 37.687129974365234\n",
      "Epoch 301, Loss: 37.410011291503906\n",
      "Epoch 351, Loss: 37.177162170410156\n",
      "Epoch 401, Loss: 36.98085021972656\n",
      "Epoch 451, Loss: 36.81464767456055\n",
      "Train RMSE: 5.977160623784904\n",
      "Test RMSE: 6.246551681993991\n",
      "R2 Train: 0.012231306260554464\n",
      "R2 Test: -0.0017718882399095914\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 57.09206771850586\n",
      "Epoch 51, Loss: 40.46419906616211\n",
      "Epoch 101, Loss: 40.16942596435547\n",
      "Epoch 151, Loss: 40.01687240600586\n",
      "Epoch 201, Loss: 39.91773223876953\n",
      "Epoch 251, Loss: 39.83872604370117\n",
      "Epoch 301, Loss: 39.767478942871094\n",
      "Epoch 351, Loss: 39.699462890625\n",
      "Epoch 401, Loss: 39.633056640625\n",
      "Epoch 451, Loss: 39.56767272949219\n",
      "Train RMSE: 5.943613357354085\n",
      "Test RMSE: 6.233906613649612\n",
      "R2 Train: 0.023288043674872028\n",
      "R2 Test: 0.002279835999800972\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 57.149085998535156\n",
      "Epoch 51, Loss: 40.394168853759766\n",
      "Epoch 101, Loss: 39.836063385009766\n",
      "Epoch 151, Loss: 39.426639556884766\n",
      "Epoch 201, Loss: 39.1155891418457\n",
      "Epoch 251, Loss: 38.865699768066406\n",
      "Epoch 301, Loss: 38.654136657714844\n",
      "Epoch 351, Loss: 38.46934509277344\n",
      "Epoch 401, Loss: 38.305931091308594\n",
      "Epoch 451, Loss: 38.1612663269043\n",
      "Train RMSE: 5.92345204683068\n",
      "Test RMSE: 6.222079777117057\n",
      "R2 Train: 0.02990300788252287\n",
      "R2 Test: 0.006061952345055688\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 43.278419494628906\n",
      "Epoch 51, Loss: 41.12213897705078\n",
      "Epoch 101, Loss: 41.00228500366211\n",
      "Epoch 151, Loss: 40.88930130004883\n",
      "Epoch 201, Loss: 40.79405975341797\n",
      "Epoch 251, Loss: 40.715858459472656\n",
      "Epoch 301, Loss: 40.65125274658203\n",
      "Epoch 351, Loss: 40.59701156616211\n",
      "Epoch 401, Loss: 40.55074691772461\n",
      "Epoch 451, Loss: 40.510799407958984\n",
      "Train RMSE: 6.018874935344262\n",
      "Test RMSE: 6.242823318729876\n",
      "R2 Train: -0.0016039825360731896\n",
      "R2 Test: -0.0005763950922035299\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 43.27627944946289\n",
      "Epoch 51, Loss: 40.95455551147461\n",
      "Epoch 101, Loss: 40.65144729614258\n",
      "Epoch 151, Loss: 40.369224548339844\n",
      "Epoch 201, Loss: 40.12046813964844\n",
      "Epoch 251, Loss: 39.904422760009766\n",
      "Epoch 301, Loss: 39.71737289428711\n",
      "Epoch 351, Loss: 39.555137634277344\n",
      "Epoch 401, Loss: 39.41383743286133\n",
      "Epoch 451, Loss: 39.29009246826172\n",
      "Train RMSE: 6.0188573073915554\n",
      "Test RMSE: 6.242796949222737\n",
      "R2 Train: -0.0015981155918456391\n",
      "R2 Test: -0.0005679422979079085\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 43.277400970458984\n",
      "Epoch 51, Loss: 41.38612365722656\n",
      "Epoch 101, Loss: 41.458106994628906\n",
      "Epoch 151, Loss: 41.40668487548828\n",
      "Epoch 201, Loss: 41.29407501220703\n",
      "Epoch 251, Loss: 41.15559387207031\n",
      "Epoch 301, Loss: 41.009239196777344\n",
      "Epoch 351, Loss: 40.86405563354492\n",
      "Epoch 401, Loss: 40.72441864013672\n",
      "Epoch 451, Loss: 40.59233093261719\n",
      "Train RMSE: 6.018844265762626\n",
      "Test RMSE: 6.242848055261319\n",
      "R2 Train: -0.0015937750812999862\n",
      "R2 Test: -0.0005843244655756585\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 43.28040313720703\n",
      "Epoch 51, Loss: 40.92573547363281\n",
      "Epoch 101, Loss: 40.57261657714844\n",
      "Epoch 151, Loss: 40.26405334472656\n",
      "Epoch 201, Loss: 40.01679229736328\n",
      "Epoch 251, Loss: 39.82112121582031\n",
      "Epoch 301, Loss: 39.66478729248047\n",
      "Epoch 351, Loss: 39.53804397583008\n",
      "Epoch 401, Loss: 39.43385314941406\n",
      "Epoch 451, Loss: 39.347206115722656\n",
      "Train RMSE: 6.0187405408035435\n",
      "Test RMSE: 6.242916007348118\n",
      "R2 Train: -0.0015592537102326887\n",
      "R2 Test: -0.0006061068835689909\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 43.27882385253906\n",
      "Epoch 51, Loss: 40.754676818847656\n",
      "Epoch 101, Loss: 40.2769775390625\n",
      "Epoch 151, Loss: 39.91798782348633\n",
      "Epoch 201, Loss: 39.64786148071289\n",
      "Epoch 251, Loss: 39.430599212646484\n",
      "Epoch 301, Loss: 39.24549865722656\n",
      "Epoch 351, Loss: 39.08207702636719\n",
      "Epoch 401, Loss: 38.93486404418945\n",
      "Epoch 451, Loss: 38.80071258544922\n",
      "Train RMSE: 6.018849942340514\n",
      "Test RMSE: 6.24275952027506\n",
      "R2 Train: -0.0015956643568841855\n",
      "R2 Test: -0.0005559444407943204\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 56.723995208740234\n",
      "Epoch 51, Loss: 40.91501998901367\n",
      "Epoch 101, Loss: 40.520816802978516\n",
      "Epoch 151, Loss: 40.22554397583008\n",
      "Epoch 201, Loss: 40.00094985961914\n",
      "Epoch 251, Loss: 39.823204040527344\n",
      "Epoch 301, Loss: 39.67682647705078\n",
      "Epoch 351, Loss: 39.552101135253906\n",
      "Epoch 401, Loss: 39.44281768798828\n",
      "Epoch 451, Loss: 39.34489440917969\n",
      "Train RMSE: 5.909647480987646\n",
      "Test RMSE: 6.160014236137689\n",
      "R2 Train: 0.03441934834848415\n",
      "R2 Test: 0.025792212311558704\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 56.754695892333984\n",
      "Epoch 51, Loss: 40.99803161621094\n",
      "Epoch 101, Loss: 40.590965270996094\n",
      "Epoch 151, Loss: 40.27033996582031\n",
      "Epoch 201, Loss: 40.01933288574219\n",
      "Epoch 251, Loss: 39.81639862060547\n",
      "Epoch 301, Loss: 39.64669418334961\n",
      "Epoch 351, Loss: 39.50068664550781\n",
      "Epoch 401, Loss: 39.37216567993164\n",
      "Epoch 451, Loss: 39.25700378417969\n",
      "Train RMSE: 5.9207049553221225\n",
      "Test RMSE: 6.158112343385651\n",
      "R2 Train: 0.030802593878628937\n",
      "R2 Test: 0.026393689070910553\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 56.701576232910156\n",
      "Epoch 51, Loss: 40.93204116821289\n",
      "Epoch 101, Loss: 40.492088317871094\n",
      "Epoch 151, Loss: 40.14448547363281\n",
      "Epoch 201, Loss: 39.874977111816406\n",
      "Epoch 251, Loss: 39.661598205566406\n",
      "Epoch 301, Loss: 39.48814010620117\n",
      "Epoch 351, Loss: 39.343502044677734\n",
      "Epoch 401, Loss: 39.220054626464844\n",
      "Epoch 451, Loss: 39.11239242553711\n",
      "Train RMSE: 5.8894254950673215\n",
      "Test RMSE: 6.133661082488625\n",
      "R2 Train: 0.041016205727241295\n",
      "R2 Test: 0.034109897872625305\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 56.717220306396484\n",
      "Epoch 51, Loss: 40.871376037597656\n",
      "Epoch 101, Loss: 40.35292053222656\n",
      "Epoch 151, Loss: 39.90438461303711\n",
      "Epoch 201, Loss: 39.52641296386719\n",
      "Epoch 251, Loss: 39.20525360107422\n",
      "Epoch 301, Loss: 38.92879867553711\n",
      "Epoch 351, Loss: 38.687984466552734\n",
      "Epoch 401, Loss: 38.47605895996094\n",
      "Epoch 451, Loss: 38.287925720214844\n",
      "Train RMSE: 5.897143093183816\n",
      "Test RMSE: 6.130609619986532\n",
      "R2 Train: 0.03850122332046868\n",
      "R2 Test: 0.03507070879326879\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 56.736019134521484\n",
      "Epoch 51, Loss: 40.884395599365234\n",
      "Epoch 101, Loss: 40.44929885864258\n",
      "Epoch 151, Loss: 40.12035369873047\n",
      "Epoch 201, Loss: 39.86847686767578\n",
      "Epoch 251, Loss: 39.66697311401367\n",
      "Epoch 301, Loss: 39.49898910522461\n",
      "Epoch 351, Loss: 39.35431671142578\n",
      "Epoch 401, Loss: 39.226539611816406\n",
      "Epoch 451, Loss: 39.11149978637695\n",
      "Train RMSE: 5.907392694715941\n",
      "Test RMSE: 6.159707664031827\n",
      "R2 Train: 0.03515602939782103\n",
      "R2 Test: 0.02588917880857633\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 48.18666076660156\n",
      "Epoch 51, Loss: 34.016510009765625\n",
      "Epoch 101, Loss: 33.89857482910156\n",
      "Epoch 151, Loss: 33.891170501708984\n",
      "Epoch 201, Loss: 33.885616302490234\n",
      "Epoch 251, Loss: 33.880615234375\n",
      "Epoch 301, Loss: 33.87568283081055\n",
      "Epoch 351, Loss: 33.870208740234375\n",
      "Epoch 401, Loss: 33.86393356323242\n",
      "Epoch 451, Loss: 33.85681915283203\n",
      "Train RMSE: 5.746968322453221\n",
      "Test RMSE: 6.191098313087559\n",
      "R2 Train: 0.08684813462515817\n",
      "R2 Test: 0.015935496846881003\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 48.15148162841797\n",
      "Epoch 51, Loss: 34.2506217956543\n",
      "Epoch 101, Loss: 34.24829864501953\n",
      "Epoch 151, Loss: 34.30580520629883\n",
      "Epoch 201, Loss: 34.32724380493164\n",
      "Epoch 251, Loss: 34.335140228271484\n",
      "Epoch 301, Loss: 34.33778762817383\n",
      "Epoch 351, Loss: 34.33786392211914\n",
      "Epoch 401, Loss: 34.33637237548828\n",
      "Epoch 451, Loss: 34.33378601074219\n",
      "Train RMSE: 5.713006956078254\n",
      "Test RMSE: 6.167567937833112\n",
      "R2 Train: 0.09760867892572778\n",
      "R2 Test: 0.023401507798186283\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 48.272457122802734\n",
      "Epoch 51, Loss: 33.669029235839844\n",
      "Epoch 101, Loss: 33.647743225097656\n",
      "Epoch 151, Loss: 33.69880676269531\n",
      "Epoch 201, Loss: 33.72751235961914\n",
      "Epoch 251, Loss: 33.747840881347656\n",
      "Epoch 301, Loss: 33.76422882080078\n",
      "Epoch 351, Loss: 33.77765655517578\n",
      "Epoch 401, Loss: 33.78845977783203\n",
      "Epoch 451, Loss: 33.796897888183594\n",
      "Train RMSE: 5.734597242575102\n",
      "Test RMSE: 6.172157868680929\n",
      "R2 Train: 0.09077525420198995\n",
      "R2 Test: 0.02194738923938122\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 48.07448959350586\n",
      "Epoch 51, Loss: 34.21381759643555\n",
      "Epoch 101, Loss: 34.42669677734375\n",
      "Epoch 151, Loss: 34.522762298583984\n",
      "Epoch 201, Loss: 34.54819107055664\n",
      "Epoch 251, Loss: 34.554222106933594\n",
      "Epoch 301, Loss: 34.5540885925293\n",
      "Epoch 351, Loss: 34.55118942260742\n",
      "Epoch 401, Loss: 34.54658508300781\n",
      "Epoch 451, Loss: 34.54074478149414\n",
      "Train RMSE: 5.72632488325561\n",
      "Test RMSE: 6.167548297746168\n",
      "R2 Train: 0.09339653988758023\n",
      "R2 Test: 0.023407727575236015\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 48.046302795410156\n",
      "Epoch 51, Loss: 34.2483024597168\n",
      "Epoch 101, Loss: 34.29848861694336\n",
      "Epoch 151, Loss: 34.325050354003906\n",
      "Epoch 201, Loss: 34.30500411987305\n",
      "Epoch 251, Loss: 34.275325775146484\n",
      "Epoch 301, Loss: 34.245758056640625\n",
      "Epoch 351, Loss: 34.21808624267578\n",
      "Epoch 401, Loss: 34.19229507446289\n",
      "Epoch 451, Loss: 34.16806411743164\n",
      "Train RMSE: 5.7504744563763746\n",
      "Test RMSE: 6.180004950415982\n",
      "R2 Train: 0.085733595903147\n",
      "R2 Test: 0.01945887947513647\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 5.820092279383699\n",
      "GensimLDA Average Test RMSE: 6.165841421728875\n",
      "GensimLDA Average R2 Train: 0.06346255712481286\n",
      "GensimLDA Average R2 Test: 0.023948198824667033\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 5.937960949802287\n",
      "Mallet_LDA Average Test RMSE: 6.2303404042767765\n",
      "Mallet_LDA Average R2 Train: 0.025121814291387667\n",
      "Mallet_LDA Average R2 Test: 0.0034151280593577747\n",
      "\n",
      "CTM Average Train RMSE: 6.018833398328501\n",
      "CTM Average Test RMSE: 6.2428287701674225\n",
      "CTM Average R2 Train: -0.0015901582552671379\n",
      "CTM Average R2 Test: -0.0005781426360100816\n",
      "\n",
      "BERTopic Average Train RMSE: 5.904862743855369\n",
      "BERTopic Average Test RMSE: 6.148420989206065\n",
      "BERTopic Average R2 Train: 0.035979080134528815\n",
      "BERTopic Average R2 Test: 0.029451137371387937\n",
      "\n",
      "NMF Average Train RMSE: 5.734274372147713\n",
      "NMF Average Test RMSE: 6.17567547355275\n",
      "NMF Average R2 Train: 0.09087244070872062\n",
      "NMF Average R2 Test: 0.020830200186964198\n",
      "\n",
      "Extraversion\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 2.953946828842163\n",
      "Epoch 51, Loss: 2.2252275943756104\n",
      "Epoch 101, Loss: 2.1549696922302246\n",
      "Epoch 151, Loss: 2.0935258865356445\n",
      "Epoch 201, Loss: 2.047433376312256\n",
      "Epoch 251, Loss: 2.013158082962036\n",
      "Epoch 301, Loss: 1.9874911308288574\n",
      "Epoch 351, Loss: 1.9679251909255981\n",
      "Epoch 401, Loss: 1.9526492357254028\n",
      "Epoch 451, Loss: 1.9404000043869019\n",
      "Train RMSE: 1.5208327366105592\n",
      "Test RMSE: 1.5295686604809116\n",
      "R2 Train: 0.030292912947966433\n",
      "R2 Test: 0.014071088145954813\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 2.953946828842163\n",
      "Epoch 51, Loss: 2.2252275943756104\n",
      "Epoch 101, Loss: 2.1549696922302246\n",
      "Epoch 151, Loss: 2.0935258865356445\n",
      "Epoch 201, Loss: 2.047433376312256\n",
      "Epoch 251, Loss: 2.013158082962036\n",
      "Epoch 301, Loss: 1.9874911308288574\n",
      "Epoch 351, Loss: 1.9679251909255981\n",
      "Epoch 401, Loss: 1.9526492357254028\n",
      "Epoch 451, Loss: 1.9404000043869019\n",
      "Train RMSE: 1.5208327366105592\n",
      "Test RMSE: 1.5295686604809116\n",
      "R2 Train: 0.030292912947966433\n",
      "R2 Test: 0.014071088145954813\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 2.953946828842163\n",
      "Epoch 51, Loss: 2.2252275943756104\n",
      "Epoch 101, Loss: 2.1549696922302246\n",
      "Epoch 151, Loss: 2.0935258865356445\n",
      "Epoch 201, Loss: 2.047433376312256\n",
      "Epoch 251, Loss: 2.013158082962036\n",
      "Epoch 301, Loss: 1.9874911308288574\n",
      "Epoch 351, Loss: 1.9679251909255981\n",
      "Epoch 401, Loss: 1.9526492357254028\n",
      "Epoch 451, Loss: 1.9404000043869019\n",
      "Train RMSE: 1.5208327366105592\n",
      "Test RMSE: 1.5295686604809116\n",
      "R2 Train: 0.030292912947966433\n",
      "R2 Test: 0.014071088145954813\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 2.953946828842163\n",
      "Epoch 51, Loss: 2.2252275943756104\n",
      "Epoch 101, Loss: 2.1549696922302246\n",
      "Epoch 151, Loss: 2.0935258865356445\n",
      "Epoch 201, Loss: 2.047433376312256\n",
      "Epoch 251, Loss: 2.013158082962036\n",
      "Epoch 301, Loss: 1.9874911308288574\n",
      "Epoch 351, Loss: 1.9679251909255981\n",
      "Epoch 401, Loss: 1.9526492357254028\n",
      "Epoch 451, Loss: 1.9404000043869019\n",
      "Train RMSE: 1.5208327366105592\n",
      "Test RMSE: 1.5295686604809116\n",
      "R2 Train: 0.030292912947966433\n",
      "R2 Test: 0.014071088145954813\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 2.953946828842163\n",
      "Epoch 51, Loss: 2.2252275943756104\n",
      "Epoch 101, Loss: 2.1549696922302246\n",
      "Epoch 151, Loss: 2.0935258865356445\n",
      "Epoch 201, Loss: 2.047433376312256\n",
      "Epoch 251, Loss: 2.013158082962036\n",
      "Epoch 301, Loss: 1.9874911308288574\n",
      "Epoch 351, Loss: 1.9679251909255981\n",
      "Epoch 401, Loss: 1.9526492357254028\n",
      "Epoch 451, Loss: 1.9404000043869019\n",
      "Train RMSE: 1.5208327366105592\n",
      "Test RMSE: 1.5295686604809116\n",
      "R2 Train: 0.030292912947966433\n",
      "R2 Test: 0.014071088145954813\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 3.077106475830078\n",
      "Epoch 51, Loss: 2.2345004081726074\n",
      "Epoch 101, Loss: 2.222935676574707\n",
      "Epoch 151, Loss: 2.21093487739563\n",
      "Epoch 201, Loss: 2.2037599086761475\n",
      "Epoch 251, Loss: 2.1992111206054688\n",
      "Epoch 301, Loss: 2.1966702938079834\n",
      "Epoch 351, Loss: 2.1956610679626465\n",
      "Epoch 401, Loss: 2.1957342624664307\n",
      "Epoch 451, Loss: 2.1965010166168213\n",
      "Train RMSE: 1.5376335327226889\n",
      "Test RMSE: 1.5391186232631564\n",
      "R2 Train: 0.008749662933355662\n",
      "R2 Test: 0.0017212302164335558\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 3.0992753505706787\n",
      "Epoch 51, Loss: 2.1836912631988525\n",
      "Epoch 101, Loss: 2.1079447269439697\n",
      "Epoch 151, Loss: 2.047333002090454\n",
      "Epoch 201, Loss: 2.003300428390503\n",
      "Epoch 251, Loss: 1.9707590341567993\n",
      "Epoch 301, Loss: 1.946799635887146\n",
      "Epoch 351, Loss: 1.9293071031570435\n",
      "Epoch 401, Loss: 1.91665518283844\n",
      "Epoch 451, Loss: 1.9076101779937744\n",
      "Train RMSE: 1.5402007685449437\n",
      "Test RMSE: 1.5396789683390557\n",
      "R2 Train: 0.005436912919608905\n",
      "R2 Test: 0.0009942135835657728\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 3.0657052993774414\n",
      "Epoch 51, Loss: 2.0646274089813232\n",
      "Epoch 101, Loss: 1.9605557918548584\n",
      "Epoch 151, Loss: 1.9034518003463745\n",
      "Epoch 201, Loss: 1.87369966506958\n",
      "Epoch 251, Loss: 1.856331467628479\n",
      "Epoch 301, Loss: 1.844634771347046\n",
      "Epoch 351, Loss: 1.8358350992202759\n",
      "Epoch 401, Loss: 1.8288023471832275\n",
      "Epoch 451, Loss: 1.8230088949203491\n",
      "Train RMSE: 1.543864215173162\n",
      "Test RMSE: 1.5397008411640078\n",
      "R2 Train: 0.0007000474114414734\n",
      "R2 Test: 0.0009658294407005608\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 3.0697991847991943\n",
      "Epoch 51, Loss: 2.215236186981201\n",
      "Epoch 101, Loss: 2.2063727378845215\n",
      "Epoch 151, Loss: 2.2014901638031006\n",
      "Epoch 201, Loss: 2.2016258239746094\n",
      "Epoch 251, Loss: 2.2031631469726562\n",
      "Epoch 301, Loss: 2.205235481262207\n",
      "Epoch 351, Loss: 2.207557201385498\n",
      "Epoch 401, Loss: 2.2099766731262207\n",
      "Epoch 451, Loss: 2.212392568588257\n",
      "Train RMSE: 1.5434354003618096\n",
      "Test RMSE: 1.5404093133421501\n",
      "R2 Train: 0.0012550898893607387\n",
      "R2 Test: 4.623423705119034e-05\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 3.0813000202178955\n",
      "Epoch 51, Loss: 2.199557304382324\n",
      "Epoch 101, Loss: 2.146702766418457\n",
      "Epoch 151, Loss: 2.105402946472168\n",
      "Epoch 201, Loss: 2.0773403644561768\n",
      "Epoch 251, Loss: 2.0573763847351074\n",
      "Epoch 301, Loss: 2.0432000160217285\n",
      "Epoch 351, Loss: 2.0334322452545166\n",
      "Epoch 401, Loss: 2.0268325805664062\n",
      "Epoch 451, Loss: 2.0223867893218994\n",
      "Train RMSE: 1.543901017638769\n",
      "Test RMSE: 1.540153062330916\n",
      "R2 Train: 0.0006524044385501915\n",
      "R2 Test: 0.0003788962748956237\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 2.597681760787964\n",
      "Epoch 51, Loss: 2.1196987628936768\n",
      "Epoch 101, Loss: 2.060814142227173\n",
      "Epoch 151, Loss: 2.0293631553649902\n",
      "Epoch 201, Loss: 2.0074517726898193\n",
      "Epoch 251, Loss: 1.991053819656372\n",
      "Epoch 301, Loss: 1.9784786701202393\n",
      "Epoch 351, Loss: 1.9687554836273193\n",
      "Epoch 401, Loss: 1.9612233638763428\n",
      "Epoch 451, Loss: 1.9553959369659424\n",
      "Train RMSE: 1.544148530689608\n",
      "Test RMSE: 1.5410301430133138\n",
      "R2 Train: 0.0003319546212521285\n",
      "R2 Test: -0.0007599489801042303\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 2.59779691696167\n",
      "Epoch 51, Loss: 2.1201236248016357\n",
      "Epoch 101, Loss: 2.0577971935272217\n",
      "Epoch 151, Loss: 2.025900363922119\n",
      "Epoch 201, Loss: 2.0038228034973145\n",
      "Epoch 251, Loss: 1.9867357015609741\n",
      "Epoch 301, Loss: 1.9727669954299927\n",
      "Epoch 351, Loss: 1.9609495401382446\n",
      "Epoch 401, Loss: 1.9507153034210205\n",
      "Epoch 451, Loss: 1.9417037963867188\n",
      "Train RMSE: 1.5441616992195668\n",
      "Test RMSE: 1.5411356707683015\n",
      "R2 Train: 0.0003149041698542465\n",
      "R2 Test: -0.0008970151719949193\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 2.597803831100464\n",
      "Epoch 51, Loss: 2.112502098083496\n",
      "Epoch 101, Loss: 2.0416998863220215\n",
      "Epoch 151, Loss: 2.006420612335205\n",
      "Epoch 201, Loss: 1.98387610912323\n",
      "Epoch 251, Loss: 1.9676175117492676\n",
      "Epoch 301, Loss: 1.9552370309829712\n",
      "Epoch 351, Loss: 1.9455877542495728\n",
      "Epoch 401, Loss: 1.937992811203003\n",
      "Epoch 451, Loss: 1.9319877624511719\n",
      "Train RMSE: 1.5441810915700478\n",
      "Test RMSE: 1.5410215590525986\n",
      "R2 Train: 0.0002897949270983524\n",
      "R2 Test: -0.0007487999961595104\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 2.598630428314209\n",
      "Epoch 51, Loss: 2.0620970726013184\n",
      "Epoch 101, Loss: 1.9520328044891357\n",
      "Epoch 151, Loss: 1.891979694366455\n",
      "Epoch 201, Loss: 1.8530547618865967\n",
      "Epoch 251, Loss: 1.825703501701355\n",
      "Epoch 301, Loss: 1.8054986000061035\n",
      "Epoch 351, Loss: 1.7900347709655762\n",
      "Epoch 401, Loss: 1.7778681516647339\n",
      "Epoch 451, Loss: 1.7680714130401611\n",
      "Train RMSE: 1.544128241568367\n",
      "Test RMSE: 1.541042833933862\n",
      "R2 Train: 0.00035822444247890495\n",
      "R2 Test: -0.0007764322622190178\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 2.5958101749420166\n",
      "Epoch 51, Loss: 2.1595003604888916\n",
      "Epoch 101, Loss: 2.090137243270874\n",
      "Epoch 151, Loss: 2.0504796504974365\n",
      "Epoch 201, Loss: 2.0257580280303955\n",
      "Epoch 251, Loss: 2.009089231491089\n",
      "Epoch 301, Loss: 1.9972283840179443\n",
      "Epoch 351, Loss: 1.9884510040283203\n",
      "Epoch 401, Loss: 1.9817442893981934\n",
      "Epoch 451, Loss: 1.9764723777770996\n",
      "Train RMSE: 1.536709277314037\n",
      "Test RMSE: 1.5378475979952222\n",
      "R2 Train: 0.009940965173104055\n",
      "R2 Test: 0.0033693340207092692\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 3.0076889991760254\n",
      "Epoch 51, Loss: 2.2553842067718506\n",
      "Epoch 101, Loss: 2.230151414871216\n",
      "Epoch 151, Loss: 2.205754280090332\n",
      "Epoch 201, Loss: 2.185816526412964\n",
      "Epoch 251, Loss: 2.1681296825408936\n",
      "Epoch 301, Loss: 2.1520888805389404\n",
      "Epoch 351, Loss: 2.137408971786499\n",
      "Epoch 401, Loss: 2.123887538909912\n",
      "Epoch 451, Loss: 2.1113638877868652\n",
      "Train RMSE: 1.5287043386186134\n",
      "Test RMSE: 1.5348754140349914\n",
      "R2 Train: 0.020228818344594224\n",
      "R2 Test: 0.007217969222868215\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 3.0104799270629883\n",
      "Epoch 51, Loss: 2.2667131423950195\n",
      "Epoch 101, Loss: 2.251518726348877\n",
      "Epoch 151, Loss: 2.2341039180755615\n",
      "Epoch 201, Loss: 2.2182703018188477\n",
      "Epoch 251, Loss: 2.2029218673706055\n",
      "Epoch 301, Loss: 2.188169479370117\n",
      "Epoch 351, Loss: 2.174137830734253\n",
      "Epoch 401, Loss: 2.160860776901245\n",
      "Epoch 451, Loss: 2.1483190059661865\n",
      "Train RMSE: 1.528641426584938\n",
      "Test RMSE: 1.5317636419469598\n",
      "R2 Train: 0.02030945935236661\n",
      "R2 Test: 0.011239376803206302\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 3.0050153732299805\n",
      "Epoch 51, Loss: 2.254884958267212\n",
      "Epoch 101, Loss: 2.2293124198913574\n",
      "Epoch 151, Loss: 2.2099995613098145\n",
      "Epoch 201, Loss: 2.1972243785858154\n",
      "Epoch 251, Loss: 2.187469959259033\n",
      "Epoch 301, Loss: 2.1794791221618652\n",
      "Epoch 351, Loss: 2.172621488571167\n",
      "Epoch 401, Loss: 2.166520357131958\n",
      "Epoch 451, Loss: 2.160938024520874\n",
      "Train RMSE: 1.5292539447636444\n",
      "Test RMSE: 1.5386091386022247\n",
      "R2 Train: 0.019524188877238724\n",
      "R2 Test: 0.002382028587996299\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 3.0082852840423584\n",
      "Epoch 51, Loss: 2.256983995437622\n",
      "Epoch 101, Loss: 2.2357065677642822\n",
      "Epoch 151, Loss: 2.2159156799316406\n",
      "Epoch 201, Loss: 2.200277805328369\n",
      "Epoch 251, Loss: 2.18652081489563\n",
      "Epoch 301, Loss: 2.1740753650665283\n",
      "Epoch 351, Loss: 2.162702798843384\n",
      "Epoch 401, Loss: 2.1522459983825684\n",
      "Epoch 451, Loss: 2.142582893371582\n",
      "Train RMSE: 1.5347826723730662\n",
      "Test RMSE: 1.539200961397352\n",
      "R2 Train: 0.012421924909117643\n",
      "R2 Test: 0.0016144176437093982\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 3.007809638977051\n",
      "Epoch 51, Loss: 2.262373208999634\n",
      "Epoch 101, Loss: 2.2512335777282715\n",
      "Epoch 151, Loss: 2.2426390647888184\n",
      "Epoch 201, Loss: 2.2378299236297607\n",
      "Epoch 251, Loss: 2.2341244220733643\n",
      "Epoch 301, Loss: 2.2308402061462402\n",
      "Epoch 351, Loss: 2.2277140617370605\n",
      "Epoch 401, Loss: 2.2246060371398926\n",
      "Epoch 451, Loss: 2.2214372158050537\n",
      "Train RMSE: 1.5297048024066162\n",
      "Test RMSE: 1.5341188419686491\n",
      "R2 Train: 0.01894597205658588\n",
      "R2 Test: 0.008196453895199074\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 3.7321252822875977\n",
      "Epoch 51, Loss: 1.788848638534546\n",
      "Epoch 101, Loss: 1.7732638120651245\n",
      "Epoch 151, Loss: 1.7604206800460815\n",
      "Epoch 201, Loss: 1.7507293224334717\n",
      "Epoch 251, Loss: 1.743152379989624\n",
      "Epoch 301, Loss: 1.7370655536651611\n",
      "Epoch 351, Loss: 1.7320927381515503\n",
      "Epoch 401, Loss: 1.7279822826385498\n",
      "Epoch 451, Loss: 1.7245535850524902\n",
      "Train RMSE: 1.5135476456374761\n",
      "Test RMSE: 1.534127436020588\n",
      "R2 Train: 0.03956084126981907\n",
      "R2 Test: 0.008185341802928514\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 3.822727680206299\n",
      "Epoch 51, Loss: 1.7683625221252441\n",
      "Epoch 101, Loss: 1.7371069192886353\n",
      "Epoch 151, Loss: 1.7159817218780518\n",
      "Epoch 201, Loss: 1.7007100582122803\n",
      "Epoch 251, Loss: 1.6890267133712769\n",
      "Epoch 301, Loss: 1.6797643899917603\n",
      "Epoch 351, Loss: 1.672243595123291\n",
      "Epoch 401, Loss: 1.66602623462677\n",
      "Epoch 451, Loss: 1.6608136892318726\n",
      "Train RMSE: 1.516815909382191\n",
      "Test RMSE: 1.5511899352975123\n",
      "R2 Train: 0.03540853388735499\n",
      "R2 Test: -0.013999183446186336\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 3.7943761348724365\n",
      "Epoch 51, Loss: 1.8015233278274536\n",
      "Epoch 101, Loss: 1.7779110670089722\n",
      "Epoch 151, Loss: 1.7620371580123901\n",
      "Epoch 201, Loss: 1.7513012886047363\n",
      "Epoch 251, Loss: 1.743489146232605\n",
      "Epoch 301, Loss: 1.737528920173645\n",
      "Epoch 351, Loss: 1.7328261137008667\n",
      "Epoch 401, Loss: 1.729016661643982\n",
      "Epoch 451, Loss: 1.7258639335632324\n",
      "Train RMSE: 1.511487129154278\n",
      "Test RMSE: 1.5379282935501017\n",
      "R2 Train: 0.04217411034716845\n",
      "R2 Test: 0.0032647387737633204\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 3.75533127784729\n",
      "Epoch 51, Loss: 1.6943740844726562\n",
      "Epoch 101, Loss: 1.6833057403564453\n",
      "Epoch 151, Loss: 1.675781488418579\n",
      "Epoch 201, Loss: 1.6704686880111694\n",
      "Epoch 251, Loss: 1.6664751768112183\n",
      "Epoch 301, Loss: 1.663354754447937\n",
      "Epoch 351, Loss: 1.6608588695526123\n",
      "Epoch 401, Loss: 1.6588304042816162\n",
      "Epoch 451, Loss: 1.6571636199951172\n",
      "Train RMSE: 1.5128082475486626\n",
      "Test RMSE: 1.5357133139961896\n",
      "R2 Train: 0.04049899927231748\n",
      "R2 Test: 0.006133739096335411\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 3.828721761703491\n",
      "Epoch 51, Loss: 1.7070643901824951\n",
      "Epoch 101, Loss: 1.695204496383667\n",
      "Epoch 151, Loss: 1.6874600648880005\n",
      "Epoch 201, Loss: 1.6820926666259766\n",
      "Epoch 251, Loss: 1.6779754161834717\n",
      "Epoch 301, Loss: 1.6746582984924316\n",
      "Epoch 351, Loss: 1.6719214916229248\n",
      "Epoch 401, Loss: 1.669631838798523\n",
      "Epoch 451, Loss: 1.6676985025405884\n",
      "Train RMSE: 1.513669420162744\n",
      "Test RMSE: 1.5360630519382035\n",
      "R2 Train: 0.03940628818665948\n",
      "R2 Test: 0.0056810083459660765\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.5208327366105592\n",
      "GensimLDA Average Test RMSE: 1.5295686604809116\n",
      "GensimLDA Average R2 Train: 0.030292912947966433\n",
      "GensimLDA Average R2 Test: 0.014071088145954813\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.5418069868882747\n",
      "Mallet_LDA Average Test RMSE: 1.5398121616878573\n",
      "Mallet_LDA Average R2 Train: 0.003358823518463394\n",
      "Mallet_LDA Average R2 Test: 0.0008212807505293407\n",
      "\n",
      "CTM Average Train RMSE: 1.5426657680723252\n",
      "CTM Average Test RMSE: 1.5404155609526597\n",
      "CTM Average R2 Train: 0.0022471686667575375\n",
      "CTM Average R2 Test: 3.7427522046318276e-05\n",
      "\n",
      "BERTopic Average Train RMSE: 1.5302174369493757\n",
      "BERTopic Average Test RMSE: 1.5357135995900353\n",
      "BERTopic Average R2 Train: 0.018286072707980616\n",
      "BERTopic Average R2 Test: 0.006130049230595858\n",
      "\n",
      "NMF Average Train RMSE: 1.5136656703770703\n",
      "NMF Average Test RMSE: 1.539004406160519\n",
      "NMF Average R2 Train: 0.039409754592663895\n",
      "NMF Average R2 Test: 0.0018531289145613972\n",
      "\n",
      "Agreeableness\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 4.100070476531982\n",
      "Epoch 51, Loss: 1.0445330142974854\n",
      "Epoch 101, Loss: 0.9850132465362549\n",
      "Epoch 151, Loss: 0.9443232417106628\n",
      "Epoch 201, Loss: 0.9261233806610107\n",
      "Epoch 251, Loss: 0.9143760204315186\n",
      "Epoch 301, Loss: 0.9059290289878845\n",
      "Epoch 351, Loss: 0.8995993137359619\n",
      "Epoch 401, Loss: 0.8947222828865051\n",
      "Epoch 451, Loss: 0.890872061252594\n",
      "Train RMSE: 1.1370454595112318\n",
      "Test RMSE: 1.164355569005678\n",
      "R2 Train: 0.054338504711148095\n",
      "R2 Test: 0.010844933047302052\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.100070476531982\n",
      "Epoch 51, Loss: 1.0445330142974854\n",
      "Epoch 101, Loss: 0.9850132465362549\n",
      "Epoch 151, Loss: 0.9443232417106628\n",
      "Epoch 201, Loss: 0.9261233806610107\n",
      "Epoch 251, Loss: 0.9143760204315186\n",
      "Epoch 301, Loss: 0.9059290289878845\n",
      "Epoch 351, Loss: 0.8995993137359619\n",
      "Epoch 401, Loss: 0.8947222828865051\n",
      "Epoch 451, Loss: 0.890872061252594\n",
      "Train RMSE: 1.1370454595112318\n",
      "Test RMSE: 1.164355569005678\n",
      "R2 Train: 0.054338504711148095\n",
      "R2 Test: 0.010844933047302052\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 4.100070476531982\n",
      "Epoch 51, Loss: 1.0445330142974854\n",
      "Epoch 101, Loss: 0.9850132465362549\n",
      "Epoch 151, Loss: 0.9443232417106628\n",
      "Epoch 201, Loss: 0.9261233806610107\n",
      "Epoch 251, Loss: 0.9143760204315186\n",
      "Epoch 301, Loss: 0.9059290289878845\n",
      "Epoch 351, Loss: 0.8995993137359619\n",
      "Epoch 401, Loss: 0.8947222828865051\n",
      "Epoch 451, Loss: 0.890872061252594\n",
      "Train RMSE: 1.1370454595112318\n",
      "Test RMSE: 1.164355569005678\n",
      "R2 Train: 0.054338504711148095\n",
      "R2 Test: 0.010844933047302052\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 4.100070476531982\n",
      "Epoch 51, Loss: 1.0445330142974854\n",
      "Epoch 101, Loss: 0.9850132465362549\n",
      "Epoch 151, Loss: 0.9443232417106628\n",
      "Epoch 201, Loss: 0.9261233806610107\n",
      "Epoch 251, Loss: 0.9143760204315186\n",
      "Epoch 301, Loss: 0.9059290289878845\n",
      "Epoch 351, Loss: 0.8995993137359619\n",
      "Epoch 401, Loss: 0.8947222828865051\n",
      "Epoch 451, Loss: 0.890872061252594\n",
      "Train RMSE: 1.1370454595112318\n",
      "Test RMSE: 1.164355569005678\n",
      "R2 Train: 0.054338504711148095\n",
      "R2 Test: 0.010844933047302052\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 4.100070476531982\n",
      "Epoch 51, Loss: 1.0445330142974854\n",
      "Epoch 101, Loss: 0.9850132465362549\n",
      "Epoch 151, Loss: 0.9443232417106628\n",
      "Epoch 201, Loss: 0.9261233806610107\n",
      "Epoch 251, Loss: 0.9143760204315186\n",
      "Epoch 301, Loss: 0.9059290289878845\n",
      "Epoch 351, Loss: 0.8995993137359619\n",
      "Epoch 401, Loss: 0.8947222828865051\n",
      "Epoch 451, Loss: 0.890872061252594\n",
      "Train RMSE: 1.1370454595112318\n",
      "Test RMSE: 1.164355569005678\n",
      "R2 Train: 0.054338504711148095\n",
      "R2 Test: 0.010844933047302052\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 4.47441291809082\n",
      "Epoch 51, Loss: 1.1362452507019043\n",
      "Epoch 101, Loss: 1.1111657619476318\n",
      "Epoch 151, Loss: 1.0829943418502808\n",
      "Epoch 201, Loss: 1.0775861740112305\n",
      "Epoch 251, Loss: 1.0760982036590576\n",
      "Epoch 301, Loss: 1.0751919746398926\n",
      "Epoch 351, Loss: 1.074318289756775\n",
      "Epoch 401, Loss: 1.0734236240386963\n",
      "Epoch 451, Loss: 1.072536587715149\n",
      "Train RMSE: 1.1454344434070196\n",
      "Test RMSE: 1.172837154617915\n",
      "R2 Train: 0.04033307712066614\n",
      "R2 Test: -0.0036182774283592956\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.462089538574219\n",
      "Epoch 51, Loss: 1.1292179822921753\n",
      "Epoch 101, Loss: 1.0990084409713745\n",
      "Epoch 151, Loss: 1.0666793584823608\n",
      "Epoch 201, Loss: 1.0582475662231445\n",
      "Epoch 251, Loss: 1.0546003580093384\n",
      "Epoch 301, Loss: 1.0524067878723145\n",
      "Epoch 351, Loss: 1.0509397983551025\n",
      "Epoch 401, Loss: 1.0499440431594849\n",
      "Epoch 451, Loss: 1.0493019819259644\n",
      "Train RMSE: 1.1658904959499565\n",
      "Test RMSE: 1.1741435433636107\n",
      "R2 Train: 0.005750059538636165\n",
      "R2 Test: -0.005855324274922635\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 4.447497844696045\n",
      "Epoch 51, Loss: 1.104178547859192\n",
      "Epoch 101, Loss: 1.067777395248413\n",
      "Epoch 151, Loss: 1.0277862548828125\n",
      "Epoch 201, Loss: 1.0131789445877075\n",
      "Epoch 251, Loss: 1.0055726766586304\n",
      "Epoch 301, Loss: 1.000552773475647\n",
      "Epoch 351, Loss: 0.9968596696853638\n",
      "Epoch 401, Loss: 0.9940017461776733\n",
      "Epoch 451, Loss: 0.9917223453521729\n",
      "Train RMSE: 1.1469134478049812\n",
      "Test RMSE: 1.1688003496750912\n",
      "R2 Train: 0.037853200680540655\n",
      "R2 Test: 0.003278569081061944\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 4.47119665145874\n",
      "Epoch 51, Loss: 1.1220721006393433\n",
      "Epoch 101, Loss: 1.0919550657272339\n",
      "Epoch 151, Loss: 1.0602036714553833\n",
      "Epoch 201, Loss: 1.0517934560775757\n",
      "Epoch 251, Loss: 1.0482209920883179\n",
      "Epoch 301, Loss: 1.0457921028137207\n",
      "Epoch 351, Loss: 1.0437382459640503\n",
      "Epoch 401, Loss: 1.0419104099273682\n",
      "Epoch 451, Loss: 1.0402926206588745\n",
      "Train RMSE: 1.1695111597940433\n",
      "Test RMSE: 1.1747748757375043\n",
      "R2 Train: -0.0004347999639233713\n",
      "R2 Test: -0.0069373040031532884\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 4.472164154052734\n",
      "Epoch 51, Loss: 1.1134388446807861\n",
      "Epoch 101, Loss: 1.066100001335144\n",
      "Epoch 151, Loss: 1.0204545259475708\n",
      "Epoch 201, Loss: 0.9997384548187256\n",
      "Epoch 251, Loss: 0.9863795638084412\n",
      "Epoch 301, Loss: 0.9765304327011108\n",
      "Epoch 351, Loss: 0.9689944386482239\n",
      "Epoch 401, Loss: 0.9631443023681641\n",
      "Epoch 451, Loss: 0.9585535526275635\n",
      "Train RMSE: 1.167028691461658\n",
      "Test RMSE: 1.1743877853233018\n",
      "R2 Train: 0.0038078476272012107\n",
      "R2 Test: -0.006273838057081926\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1.0661629438400269\n",
      "Epoch 51, Loss: 1.0870518684387207\n",
      "Epoch 101, Loss: 1.1057088375091553\n",
      "Epoch 151, Loss: 1.1111050844192505\n",
      "Epoch 201, Loss: 1.1061640977859497\n",
      "Epoch 251, Loss: 1.1016602516174316\n",
      "Epoch 301, Loss: 1.0984609127044678\n",
      "Epoch 351, Loss: 1.0962860584259033\n",
      "Epoch 401, Loss: 1.0948395729064941\n",
      "Epoch 451, Loss: 1.0939046144485474\n",
      "Train RMSE: 1.1694346956313428\n",
      "Test RMSE: 1.1747217849456164\n",
      "R2 Train: -0.00030398478028881293\n",
      "R2 Test: -0.0068462944114691116\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1.066979169845581\n",
      "Epoch 51, Loss: 1.0763213634490967\n",
      "Epoch 101, Loss: 1.0867869853973389\n",
      "Epoch 151, Loss: 1.0894168615341187\n",
      "Epoch 201, Loss: 1.0839061737060547\n",
      "Epoch 251, Loss: 1.0790420770645142\n",
      "Epoch 301, Loss: 1.0752246379852295\n",
      "Epoch 351, Loss: 1.0721435546875\n",
      "Epoch 401, Loss: 1.0695736408233643\n",
      "Epoch 451, Loss: 1.0673730373382568\n",
      "Train RMSE: 1.169450317777767\n",
      "Test RMSE: 1.1748527510568527\n",
      "R2 Train: -0.0003307105132519528\n",
      "R2 Test: -0.007070807306296167\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1.0660737752914429\n",
      "Epoch 51, Loss: 1.064115047454834\n",
      "Epoch 101, Loss: 1.0609850883483887\n",
      "Epoch 151, Loss: 1.057706356048584\n",
      "Epoch 201, Loss: 1.0496405363082886\n",
      "Epoch 251, Loss: 1.043515682220459\n",
      "Epoch 301, Loss: 1.0390851497650146\n",
      "Epoch 351, Loss: 1.0357835292816162\n",
      "Epoch 401, Loss: 1.0332642793655396\n",
      "Epoch 451, Loss: 1.0313156843185425\n",
      "Train RMSE: 1.1694652244206598\n",
      "Test RMSE: 1.1747270532354201\n",
      "R2 Train: -0.0003562125247291714\n",
      "R2 Test: -0.006855325264610634\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1.066057801246643\n",
      "Epoch 51, Loss: 1.084366798400879\n",
      "Epoch 101, Loss: 1.0933088064193726\n",
      "Epoch 151, Loss: 1.0893263816833496\n",
      "Epoch 201, Loss: 1.07810640335083\n",
      "Epoch 251, Loss: 1.0685248374938965\n",
      "Epoch 301, Loss: 1.060760736465454\n",
      "Epoch 351, Loss: 1.0543609857559204\n",
      "Epoch 401, Loss: 1.048994779586792\n",
      "Epoch 451, Loss: 1.0444326400756836\n",
      "Train RMSE: 1.1694662457306841\n",
      "Test RMSE: 1.1745835553939619\n",
      "R2 Train: -0.0003579597751004471\n",
      "R2 Test: -0.006609357078344091\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1.0662020444869995\n",
      "Epoch 51, Loss: 1.0730869770050049\n",
      "Epoch 101, Loss: 1.0779742002487183\n",
      "Epoch 151, Loss: 1.075133204460144\n",
      "Epoch 201, Loss: 1.0630714893341064\n",
      "Epoch 251, Loss: 1.05198335647583\n",
      "Epoch 301, Loss: 1.0428880453109741\n",
      "Epoch 351, Loss: 1.0355236530303955\n",
      "Epoch 401, Loss: 1.0295491218566895\n",
      "Epoch 451, Loss: 1.0246809720993042\n",
      "Train RMSE: 1.1693896923414144\n",
      "Test RMSE: 1.1745945766912795\n",
      "R2 Train: -0.00022699697717487588\n",
      "R2 Test: -0.006628247506368812\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 4.240368366241455\n",
      "Epoch 51, Loss: 1.1361796855926514\n",
      "Epoch 101, Loss: 1.0941020250320435\n",
      "Epoch 151, Loss: 1.058210849761963\n",
      "Epoch 201, Loss: 1.043337106704712\n",
      "Epoch 251, Loss: 1.0329862833023071\n",
      "Epoch 301, Loss: 1.0243496894836426\n",
      "Epoch 351, Loss: 1.016785979270935\n",
      "Epoch 401, Loss: 1.0100512504577637\n",
      "Epoch 451, Loss: 1.0039995908737183\n",
      "Train RMSE: 1.14370779229531\n",
      "Test RMSE: 1.1553735693054956\n",
      "R2 Train: 0.04322413922130541\n",
      "R2 Test: 0.026047028130849026\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.243741035461426\n",
      "Epoch 51, Loss: 1.1291179656982422\n",
      "Epoch 101, Loss: 1.085585355758667\n",
      "Epoch 151, Loss: 1.0501163005828857\n",
      "Epoch 201, Loss: 1.036476492881775\n",
      "Epoch 251, Loss: 1.0277564525604248\n",
      "Epoch 301, Loss: 1.0209376811981201\n",
      "Epoch 351, Loss: 1.0152537822723389\n",
      "Epoch 401, Loss: 1.0103896856307983\n",
      "Epoch 451, Loss: 1.0061591863632202\n",
      "Train RMSE: 1.1440592800843472\n",
      "Test RMSE: 1.1559590111279574\n",
      "R2 Train: 0.04263597026803678\n",
      "R2 Test: 0.02505975045754083\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 4.23215913772583\n",
      "Epoch 51, Loss: 1.1363282203674316\n",
      "Epoch 101, Loss: 1.0889239311218262\n",
      "Epoch 151, Loss: 1.0493768453598022\n",
      "Epoch 201, Loss: 1.0328797101974487\n",
      "Epoch 251, Loss: 1.02254319190979\n",
      "Epoch 301, Loss: 1.0150210857391357\n",
      "Epoch 351, Loss: 1.0092780590057373\n",
      "Epoch 401, Loss: 1.0048038959503174\n",
      "Epoch 451, Loss: 1.001273274421692\n",
      "Train RMSE: 1.1462282070906427\n",
      "Test RMSE: 1.1577536212793615\n",
      "R2 Train: 0.03900255542273612\n",
      "R2 Test: 0.022030238518180534\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 4.2434186935424805\n",
      "Epoch 51, Loss: 1.1372935771942139\n",
      "Epoch 101, Loss: 1.0927108526229858\n",
      "Epoch 151, Loss: 1.0554559230804443\n",
      "Epoch 201, Loss: 1.0400971174240112\n",
      "Epoch 251, Loss: 1.0299006700515747\n",
      "Epoch 301, Loss: 1.0218397378921509\n",
      "Epoch 351, Loss: 1.015120267868042\n",
      "Epoch 401, Loss: 1.0093967914581299\n",
      "Epoch 451, Loss: 1.0044575929641724\n",
      "Train RMSE: 1.145079548892303\n",
      "Test RMSE: 1.1571197926809007\n",
      "R2 Train: 0.04092765984364033\n",
      "R2 Test: 0.023100752166127392\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 4.248603343963623\n",
      "Epoch 51, Loss: 1.1327311992645264\n",
      "Epoch 101, Loss: 1.0851495265960693\n",
      "Epoch 151, Loss: 1.0440900325775146\n",
      "Epoch 201, Loss: 1.0253887176513672\n",
      "Epoch 251, Loss: 1.0125771760940552\n",
      "Epoch 301, Loss: 1.0025655031204224\n",
      "Epoch 351, Loss: 0.9944319128990173\n",
      "Epoch 401, Loss: 0.9877138137817383\n",
      "Epoch 451, Loss: 0.9821004867553711\n",
      "Train RMSE: 1.1459602638689046\n",
      "Test RMSE: 1.1561892678665657\n",
      "R2 Train: 0.039451789971203466\n",
      "R2 Test: 0.02467131294739\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1.6510026454925537\n",
      "Epoch 51, Loss: 0.8636046051979065\n",
      "Epoch 101, Loss: 0.8681778311729431\n",
      "Epoch 151, Loss: 0.8703818917274475\n",
      "Epoch 201, Loss: 0.8687025308609009\n",
      "Epoch 251, Loss: 0.8668441772460938\n",
      "Epoch 301, Loss: 0.86518794298172\n",
      "Epoch 351, Loss: 0.8637346625328064\n",
      "Epoch 401, Loss: 0.862454354763031\n",
      "Epoch 451, Loss: 0.8613203763961792\n",
      "Train RMSE: 1.1320789903075748\n",
      "Test RMSE: 1.1806769709588611\n",
      "R2 Train: 0.062581520120784\n",
      "R2 Test: -0.01708047092934928\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1.7466384172439575\n",
      "Epoch 51, Loss: 0.8580007553100586\n",
      "Epoch 101, Loss: 0.8635911345481873\n",
      "Epoch 151, Loss: 0.8662921190261841\n",
      "Epoch 201, Loss: 0.8645481467247009\n",
      "Epoch 251, Loss: 0.862687349319458\n",
      "Epoch 301, Loss: 0.8610928058624268\n",
      "Epoch 351, Loss: 0.8597151041030884\n",
      "Epoch 401, Loss: 0.8584964275360107\n",
      "Epoch 451, Loss: 0.8574011325836182\n",
      "Train RMSE: 1.139628914448322\n",
      "Test RMSE: 1.1799684731679443\n",
      "R2 Train: 0.050036391256613966\n",
      "R2 Test: -0.015860182778022924\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1.6479252576828003\n",
      "Epoch 51, Loss: 0.8668768405914307\n",
      "Epoch 101, Loss: 0.8735792636871338\n",
      "Epoch 151, Loss: 0.8786097764968872\n",
      "Epoch 201, Loss: 0.878524124622345\n",
      "Epoch 251, Loss: 0.8779605031013489\n",
      "Epoch 301, Loss: 0.8774656653404236\n",
      "Epoch 351, Loss: 0.877059280872345\n",
      "Epoch 401, Loss: 0.876721978187561\n",
      "Epoch 451, Loss: 0.8764380812644958\n",
      "Train RMSE: 1.1352907659597853\n",
      "Test RMSE: 1.1840305218481721\n",
      "R2 Train: 0.05725495059050312\n",
      "R2 Test: -0.022866431358204764\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1.6496580839157104\n",
      "Epoch 51, Loss: 0.8491427302360535\n",
      "Epoch 101, Loss: 0.8507092595100403\n",
      "Epoch 151, Loss: 0.8547422885894775\n",
      "Epoch 201, Loss: 0.8549889326095581\n",
      "Epoch 251, Loss: 0.8548396229743958\n",
      "Epoch 301, Loss: 0.8546606302261353\n",
      "Epoch 351, Loss: 0.8544517159461975\n",
      "Epoch 401, Loss: 0.854201078414917\n",
      "Epoch 451, Loss: 0.8539072871208191\n",
      "Train RMSE: 1.1355863297387465\n",
      "Test RMSE: 1.186826305811619\n",
      "R2 Train: 0.05676401457875946\n",
      "R2 Test: -0.027702607034840954\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1.6971149444580078\n",
      "Epoch 51, Loss: 0.85622638463974\n",
      "Epoch 101, Loss: 0.8649278879165649\n",
      "Epoch 151, Loss: 0.8699257969856262\n",
      "Epoch 201, Loss: 0.8699491024017334\n",
      "Epoch 251, Loss: 0.8693414926528931\n",
      "Epoch 301, Loss: 0.8686429262161255\n",
      "Epoch 351, Loss: 0.8679288029670715\n",
      "Epoch 401, Loss: 0.8672223687171936\n",
      "Epoch 451, Loss: 0.8665367960929871\n",
      "Train RMSE: 1.13316810118576\n",
      "Test RMSE: 1.185029133566359\n",
      "R2 Train: 0.060776975077952655\n",
      "R2 Test: -0.024592530693904546\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.1370454595112318\n",
      "GensimLDA Average Test RMSE: 1.164355569005678\n",
      "GensimLDA Average R2 Train: 0.054338504711148095\n",
      "GensimLDA Average R2 Test: 0.010844933047302052\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.1589556476835317\n",
      "Mallet_LDA Average Test RMSE: 1.1729887417434846\n",
      "Mallet_LDA Average R2 Train: 0.01746187700062416\n",
      "Mallet_LDA Average R2 Test: -0.0038812349364910404\n",
      "\n",
      "CTM Average Train RMSE: 1.1694412351803736\n",
      "CTM Average Test RMSE: 1.174695944264626\n",
      "CTM Average R2 Train: -0.000315172914109052\n",
      "CTM Average R2 Test: -0.006802006313417763\n",
      "\n",
      "BERTopic Average Train RMSE: 1.1450070184463015\n",
      "BERTopic Average Test RMSE: 1.156479052452056\n",
      "BERTopic Average R2 Train: 0.041048422945384425\n",
      "BERTopic Average R2 Test: 0.024181816444017557\n",
      "\n",
      "NMF Average Train RMSE: 1.1351506203280377\n",
      "NMF Average Test RMSE: 1.1833062810705912\n",
      "NMF Average R2 Train: 0.05748277032492264\n",
      "NMF Average R2 Test: -0.021620444558864492\n",
      "\n",
      "Conscientiousness\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 7.361541271209717\n",
      "Epoch 51, Loss: 1.2467083930969238\n",
      "Epoch 101, Loss: 1.2161540985107422\n",
      "Epoch 151, Loss: 1.2046456336975098\n",
      "Epoch 201, Loss: 1.1944504976272583\n",
      "Epoch 251, Loss: 1.1868761777877808\n",
      "Epoch 301, Loss: 1.1807968616485596\n",
      "Epoch 351, Loss: 1.1756049394607544\n",
      "Epoch 401, Loss: 1.1710368394851685\n",
      "Epoch 451, Loss: 1.1669628620147705\n",
      "Train RMSE: 1.21316160144472\n",
      "Test RMSE: 1.237012470608857\n",
      "R2 Train: 0.02106378502011319\n",
      "R2 Test: 0.025822463605449597\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 7.361541271209717\n",
      "Epoch 51, Loss: 1.2467083930969238\n",
      "Epoch 101, Loss: 1.2161540985107422\n",
      "Epoch 151, Loss: 1.2046456336975098\n",
      "Epoch 201, Loss: 1.1944504976272583\n",
      "Epoch 251, Loss: 1.1868761777877808\n",
      "Epoch 301, Loss: 1.1807968616485596\n",
      "Epoch 351, Loss: 1.1756049394607544\n",
      "Epoch 401, Loss: 1.1710368394851685\n",
      "Epoch 451, Loss: 1.1669628620147705\n",
      "Train RMSE: 1.21316160144472\n",
      "Test RMSE: 1.237012470608857\n",
      "R2 Train: 0.02106378502011319\n",
      "R2 Test: 0.025822463605449597\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 7.361541271209717\n",
      "Epoch 51, Loss: 1.2467083930969238\n",
      "Epoch 101, Loss: 1.2161540985107422\n",
      "Epoch 151, Loss: 1.2046456336975098\n",
      "Epoch 201, Loss: 1.1944504976272583\n",
      "Epoch 251, Loss: 1.1868761777877808\n",
      "Epoch 301, Loss: 1.1807968616485596\n",
      "Epoch 351, Loss: 1.1756049394607544\n",
      "Epoch 401, Loss: 1.1710368394851685\n",
      "Epoch 451, Loss: 1.1669628620147705\n",
      "Train RMSE: 1.21316160144472\n",
      "Test RMSE: 1.237012470608857\n",
      "R2 Train: 0.02106378502011319\n",
      "R2 Test: 0.025822463605449597\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 7.361541271209717\n",
      "Epoch 51, Loss: 1.2467083930969238\n",
      "Epoch 101, Loss: 1.2161540985107422\n",
      "Epoch 151, Loss: 1.2046456336975098\n",
      "Epoch 201, Loss: 1.1944504976272583\n",
      "Epoch 251, Loss: 1.1868761777877808\n",
      "Epoch 301, Loss: 1.1807968616485596\n",
      "Epoch 351, Loss: 1.1756049394607544\n",
      "Epoch 401, Loss: 1.1710368394851685\n",
      "Epoch 451, Loss: 1.1669628620147705\n",
      "Train RMSE: 1.21316160144472\n",
      "Test RMSE: 1.237012470608857\n",
      "R2 Train: 0.02106378502011319\n",
      "R2 Test: 0.025822463605449597\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 7.361541271209717\n",
      "Epoch 51, Loss: 1.2467083930969238\n",
      "Epoch 101, Loss: 1.2161540985107422\n",
      "Epoch 151, Loss: 1.2046456336975098\n",
      "Epoch 201, Loss: 1.1944504976272583\n",
      "Epoch 251, Loss: 1.1868761777877808\n",
      "Epoch 301, Loss: 1.1807968616485596\n",
      "Epoch 351, Loss: 1.1756049394607544\n",
      "Epoch 401, Loss: 1.1710368394851685\n",
      "Epoch 451, Loss: 1.1669628620147705\n",
      "Train RMSE: 1.21316160144472\n",
      "Test RMSE: 1.237012470608857\n",
      "R2 Train: 0.02106378502011319\n",
      "R2 Test: 0.025822463605449597\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 7.8459978103637695\n",
      "Epoch 51, Loss: 1.2571710348129272\n",
      "Epoch 101, Loss: 1.2256087064743042\n",
      "Epoch 151, Loss: 1.2066783905029297\n",
      "Epoch 201, Loss: 1.1904020309448242\n",
      "Epoch 251, Loss: 1.178141713142395\n",
      "Epoch 301, Loss: 1.169029951095581\n",
      "Epoch 351, Loss: 1.1622629165649414\n",
      "Epoch 401, Loss: 1.1572790145874023\n",
      "Epoch 451, Loss: 1.1536716222763062\n",
      "Train RMSE: 1.2160119225991717\n",
      "Test RMSE: 1.2529673882987942\n",
      "R2 Train: 0.016458363151758526\n",
      "R2 Test: 0.0005306275853880216\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 7.836154460906982\n",
      "Epoch 51, Loss: 1.2488383054733276\n",
      "Epoch 101, Loss: 1.215924620628357\n",
      "Epoch 151, Loss: 1.1994316577911377\n",
      "Epoch 201, Loss: 1.1849253177642822\n",
      "Epoch 251, Loss: 1.1736105680465698\n",
      "Epoch 301, Loss: 1.1647146940231323\n",
      "Epoch 351, Loss: 1.15760338306427\n",
      "Epoch 401, Loss: 1.1518675088882446\n",
      "Epoch 451, Loss: 1.1472086906433105\n",
      "Train RMSE: 1.2206982062660108\n",
      "Test RMSE: 1.2546324518420984\n",
      "R2 Train: 0.008862982817671727\n",
      "R2 Test: -0.0021275194524661423\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 7.805197715759277\n",
      "Epoch 51, Loss: 1.2416185140609741\n",
      "Epoch 101, Loss: 1.2076903581619263\n",
      "Epoch 151, Loss: 1.1883695125579834\n",
      "Epoch 201, Loss: 1.1695014238357544\n",
      "Epoch 251, Loss: 1.1552056074142456\n",
      "Epoch 301, Loss: 1.1446589231491089\n",
      "Epoch 351, Loss: 1.1368027925491333\n",
      "Epoch 401, Loss: 1.1308830976486206\n",
      "Epoch 451, Loss: 1.126372218132019\n",
      "Train RMSE: 1.213144448952409\n",
      "Test RMSE: 1.2505424474474534\n",
      "R2 Train: 0.021091466537856207\n",
      "R2 Test: 0.004395546678858886\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 7.8314948081970215\n",
      "Epoch 51, Loss: 1.2558388710021973\n",
      "Epoch 101, Loss: 1.237023115158081\n",
      "Epoch 151, Loss: 1.2298434972763062\n",
      "Epoch 201, Loss: 1.2220215797424316\n",
      "Epoch 251, Loss: 1.2162312269210815\n",
      "Epoch 301, Loss: 1.2118464708328247\n",
      "Epoch 351, Loss: 1.2082531452178955\n",
      "Epoch 401, Loss: 1.2051234245300293\n",
      "Epoch 451, Loss: 1.2023143768310547\n",
      "Train RMSE: 1.2177975131023993\n",
      "Test RMSE: 1.2548798392869054\n",
      "R2 Train: 0.013567779635984656\n",
      "R2 Test: -0.0025227558545937168\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 7.843660354614258\n",
      "Epoch 51, Loss: 1.299521565437317\n",
      "Epoch 101, Loss: 1.2696772813796997\n",
      "Epoch 151, Loss: 1.2474744319915771\n",
      "Epoch 201, Loss: 1.2265336513519287\n",
      "Epoch 251, Loss: 1.209776520729065\n",
      "Epoch 301, Loss: 1.1969199180603027\n",
      "Epoch 351, Loss: 1.1870988607406616\n",
      "Epoch 401, Loss: 1.1795309782028198\n",
      "Epoch 451, Loss: 1.1736056804656982\n",
      "Train RMSE: 1.2201889883843011\n",
      "Test RMSE: 1.2549421083322316\n",
      "R2 Train: 0.00968972184334227\n",
      "R2 Test: -0.0026222517295138026\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1.37702476978302\n",
      "Epoch 51, Loss: 1.2649433612823486\n",
      "Epoch 101, Loss: 1.244620442390442\n",
      "Epoch 151, Loss: 1.2374147176742554\n",
      "Epoch 201, Loss: 1.2337515354156494\n",
      "Epoch 251, Loss: 1.2318110466003418\n",
      "Epoch 301, Loss: 1.230635166168213\n",
      "Epoch 351, Loss: 1.2298024892807007\n",
      "Epoch 401, Loss: 1.2291299104690552\n",
      "Epoch 451, Loss: 1.2285354137420654\n",
      "Train RMSE: 1.226242837342562\n",
      "Test RMSE: 1.253299111565316\n",
      "R2 Train: -0.000161311359041294\n",
      "R2 Test: 1.3382572076059418e-06\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1.377168893814087\n",
      "Epoch 51, Loss: 1.2674903869628906\n",
      "Epoch 101, Loss: 1.259559154510498\n",
      "Epoch 151, Loss: 1.2579975128173828\n",
      "Epoch 201, Loss: 1.2554136514663696\n",
      "Epoch 251, Loss: 1.2534784078598022\n",
      "Epoch 301, Loss: 1.2521612644195557\n",
      "Epoch 351, Loss: 1.2512285709381104\n",
      "Epoch 401, Loss: 1.250507116317749\n",
      "Epoch 451, Loss: 1.249891757965088\n",
      "Train RMSE: 1.2262958943721596\n",
      "Test RMSE: 1.253343270005679\n",
      "R2 Train: -0.00024786311649549475\n",
      "R2 Test: -6.913041031664235e-05\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1.3767108917236328\n",
      "Epoch 51, Loss: 1.2538121938705444\n",
      "Epoch 101, Loss: 1.2314900159835815\n",
      "Epoch 151, Loss: 1.2286165952682495\n",
      "Epoch 201, Loss: 1.2312754392623901\n",
      "Epoch 251, Loss: 1.2361735105514526\n",
      "Epoch 301, Loss: 1.2417513132095337\n",
      "Epoch 351, Loss: 1.247308611869812\n",
      "Epoch 401, Loss: 1.2525461912155151\n",
      "Epoch 451, Loss: 1.2573513984680176\n",
      "Train RMSE: 1.2263093991076197\n",
      "Test RMSE: 1.2533440852931566\n",
      "R2 Train: -0.0002698939448262383\n",
      "R2 Test: -7.043148101804064e-05\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1.376983880996704\n",
      "Epoch 51, Loss: 1.2523136138916016\n",
      "Epoch 101, Loss: 1.2223981618881226\n",
      "Epoch 151, Loss: 1.2090129852294922\n",
      "Epoch 201, Loss: 1.2006258964538574\n",
      "Epoch 251, Loss: 1.195061206817627\n",
      "Epoch 301, Loss: 1.1911174058914185\n",
      "Epoch 351, Loss: 1.1881922483444214\n",
      "Epoch 401, Loss: 1.1859586238861084\n",
      "Epoch 451, Loss: 1.1842176914215088\n",
      "Train RMSE: 1.2262844894809788\n",
      "Test RMSE: 1.253357500766645\n",
      "R2 Train: -0.00022925803914430887\n",
      "R2 Test: -9.184059016953405e-05\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1.3775213956832886\n",
      "Epoch 51, Loss: 1.2779736518859863\n",
      "Epoch 101, Loss: 1.2741154432296753\n",
      "Epoch 151, Loss: 1.2714741230010986\n",
      "Epoch 201, Loss: 1.2655383348464966\n",
      "Epoch 251, Loss: 1.2591067552566528\n",
      "Epoch 301, Loss: 1.2530577182769775\n",
      "Epoch 351, Loss: 1.2476449012756348\n",
      "Epoch 401, Loss: 1.2429143190383911\n",
      "Epoch 451, Loss: 1.2388312816619873\n",
      "Train RMSE: 1.2262797896317061\n",
      "Test RMSE: 1.2533112635769386\n",
      "R2 Train: -0.0002215911106577817\n",
      "R2 Test: -1.8053848253929417e-05\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 7.552156448364258\n",
      "Epoch 51, Loss: 1.328644871711731\n",
      "Epoch 101, Loss: 1.3157864809036255\n",
      "Epoch 151, Loss: 1.2996054887771606\n",
      "Epoch 201, Loss: 1.2811135053634644\n",
      "Epoch 251, Loss: 1.2655863761901855\n",
      "Epoch 301, Loss: 1.2527825832366943\n",
      "Epoch 351, Loss: 1.2420856952667236\n",
      "Epoch 401, Loss: 1.2330305576324463\n",
      "Epoch 451, Loss: 1.225282073020935\n",
      "Train RMSE: 1.2201467168261644\n",
      "Test RMSE: 1.2433863616959957\n",
      "R2 Train: 0.009758336187142613\n",
      "R2 Test: 0.015757409549655432\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 7.55794620513916\n",
      "Epoch 51, Loss: 1.319070816040039\n",
      "Epoch 101, Loss: 1.3043005466461182\n",
      "Epoch 151, Loss: 1.288341760635376\n",
      "Epoch 201, Loss: 1.2708067893981934\n",
      "Epoch 251, Loss: 1.2566916942596436\n",
      "Epoch 301, Loss: 1.2455698251724243\n",
      "Epoch 351, Loss: 1.2366783618927002\n",
      "Epoch 401, Loss: 1.2294609546661377\n",
      "Epoch 451, Loss: 1.2235333919525146\n",
      "Train RMSE: 1.2232610307046774\n",
      "Test RMSE: 1.246319592443914\n",
      "R2 Train: 0.004696880820333926\n",
      "R2 Test: 0.011108145180397266\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 7.541079044342041\n",
      "Epoch 51, Loss: 1.3233568668365479\n",
      "Epoch 101, Loss: 1.3003227710723877\n",
      "Epoch 151, Loss: 1.2773933410644531\n",
      "Epoch 201, Loss: 1.2556570768356323\n",
      "Epoch 251, Loss: 1.239166259765625\n",
      "Epoch 301, Loss: 1.226733684539795\n",
      "Epoch 351, Loss: 1.2171344757080078\n",
      "Epoch 401, Loss: 1.209539532661438\n",
      "Epoch 451, Loss: 1.2033997774124146\n",
      "Train RMSE: 1.2205066526816717\n",
      "Test RMSE: 1.2432863713753786\n",
      "R2 Train: 0.009174019486722473\n",
      "R2 Test: 0.015915704313167023\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 7.557055950164795\n",
      "Epoch 51, Loss: 1.3259040117263794\n",
      "Epoch 101, Loss: 1.3068681955337524\n",
      "Epoch 151, Loss: 1.2883448600769043\n",
      "Epoch 201, Loss: 1.2695401906967163\n",
      "Epoch 251, Loss: 1.2544033527374268\n",
      "Epoch 301, Loss: 1.2423343658447266\n",
      "Epoch 351, Loss: 1.2325928211212158\n",
      "Epoch 401, Loss: 1.2246333360671997\n",
      "Epoch 451, Loss: 1.2180583477020264\n",
      "Train RMSE: 1.223474642715261\n",
      "Test RMSE: 1.246264404195365\n",
      "R2 Train: 0.004349240768876994\n",
      "R2 Test: 0.011195721435285577\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 7.561800956726074\n",
      "Epoch 51, Loss: 1.3281939029693604\n",
      "Epoch 101, Loss: 1.3120049238204956\n",
      "Epoch 151, Loss: 1.2915655374526978\n",
      "Epoch 201, Loss: 1.2697371244430542\n",
      "Epoch 251, Loss: 1.2525430917739868\n",
      "Epoch 301, Loss: 1.2394534349441528\n",
      "Epoch 351, Loss: 1.229419469833374\n",
      "Epoch 401, Loss: 1.221635103225708\n",
      "Epoch 451, Loss: 1.215528964996338\n",
      "Train RMSE: 1.2228110107287924\n",
      "Test RMSE: 1.2455079241670433\n",
      "R2 Train: 0.00542906125499254\n",
      "R2 Test: 0.012395761596316945\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 2.233569860458374\n",
      "Epoch 51, Loss: 1.1508570909500122\n",
      "Epoch 101, Loss: 1.1578235626220703\n",
      "Epoch 151, Loss: 1.1598066091537476\n",
      "Epoch 201, Loss: 1.1596006155014038\n",
      "Epoch 251, Loss: 1.1595661640167236\n",
      "Epoch 301, Loss: 1.1597810983657837\n",
      "Epoch 351, Loss: 1.1601474285125732\n",
      "Epoch 401, Loss: 1.160601258277893\n",
      "Epoch 451, Loss: 1.1611043214797974\n",
      "Train RMSE: 1.2057797895004283\n",
      "Test RMSE: 1.2745365965117816\n",
      "R2 Train: 0.032940748107636475\n",
      "R2 Test: -0.0341762869987321\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 2.3050286769866943\n",
      "Epoch 51, Loss: 1.1395785808563232\n",
      "Epoch 101, Loss: 1.1498477458953857\n",
      "Epoch 151, Loss: 1.158316731452942\n",
      "Epoch 201, Loss: 1.1631114482879639\n",
      "Epoch 251, Loss: 1.1670585870742798\n",
      "Epoch 301, Loss: 1.170477032661438\n",
      "Epoch 351, Loss: 1.173410177230835\n",
      "Epoch 401, Loss: 1.1759032011032104\n",
      "Epoch 451, Loss: 1.1780074834823608\n",
      "Train RMSE: 1.208728831758523\n",
      "Test RMSE: 1.2693182308746267\n",
      "R2 Train: 0.028204582934241662\n",
      "R2 Test: -0.02572511804174238\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 2.2119483947753906\n",
      "Epoch 51, Loss: 1.1606777906417847\n",
      "Epoch 101, Loss: 1.1717097759246826\n",
      "Epoch 151, Loss: 1.1757575273513794\n",
      "Epoch 201, Loss: 1.1763936281204224\n",
      "Epoch 251, Loss: 1.1767691373825073\n",
      "Epoch 301, Loss: 1.1772043704986572\n",
      "Epoch 351, Loss: 1.177679181098938\n",
      "Epoch 401, Loss: 1.1781721115112305\n",
      "Epoch 451, Loss: 1.1786713600158691\n",
      "Train RMSE: 1.2063428306254007\n",
      "Test RMSE: 1.2726359645192216\n",
      "R2 Train: 0.03203739699805863\n",
      "R2 Test: -0.0310941896052066\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 2.225050687789917\n",
      "Epoch 51, Loss: 1.1452467441558838\n",
      "Epoch 101, Loss: 1.152436375617981\n",
      "Epoch 151, Loss: 1.156380295753479\n",
      "Epoch 201, Loss: 1.1576857566833496\n",
      "Epoch 251, Loss: 1.1587040424346924\n",
      "Epoch 301, Loss: 1.159654140472412\n",
      "Epoch 351, Loss: 1.1605260372161865\n",
      "Epoch 401, Loss: 1.1613123416900635\n",
      "Epoch 451, Loss: 1.1620159149169922\n",
      "Train RMSE: 1.2078082068744393\n",
      "Test RMSE: 1.2745526114276216\n",
      "R2 Train: 0.029684349632000506\n",
      "R2 Test: -0.03420227660201203\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 2.25315523147583\n",
      "Epoch 51, Loss: 1.1265546083450317\n",
      "Epoch 101, Loss: 1.1304606199264526\n",
      "Epoch 151, Loss: 1.1329842805862427\n",
      "Epoch 201, Loss: 1.1332223415374756\n",
      "Epoch 251, Loss: 1.133461356163025\n",
      "Epoch 301, Loss: 1.1338274478912354\n",
      "Epoch 351, Loss: 1.1342545747756958\n",
      "Epoch 401, Loss: 1.1346980333328247\n",
      "Epoch 451, Loss: 1.1351330280303955\n",
      "Train RMSE: 1.2054746228725777\n",
      "Test RMSE: 1.2710860010688259\n",
      "R2 Train: 0.03343018551351207\n",
      "R2 Test: -0.02858414723456071\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.21316160144472\n",
      "GensimLDA Average Test RMSE: 1.237012470608857\n",
      "GensimLDA Average R2 Train: 0.02106378502011319\n",
      "GensimLDA Average R2 Test: 0.025822463605449597\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.2175682158608585\n",
      "Mallet_LDA Average Test RMSE: 1.2535928470414968\n",
      "Mallet_LDA Average R2 Train: 0.013934062797322677\n",
      "Mallet_LDA Average R2 Test: -0.00046927055446535084\n",
      "\n",
      "CTM Average Train RMSE: 1.226282481987005\n",
      "CTM Average Test RMSE: 1.2533310462415472\n",
      "CTM Average R2 Train: -0.00022598351403302352\n",
      "CTM Average R2 Test: -4.9623614510108105e-05\n",
      "\n",
      "BERTopic Average Train RMSE: 1.2220400107313132\n",
      "BERTopic Average Test RMSE: 1.2449529307755394\n",
      "BERTopic Average R2 Train: 0.0066815077036137096\n",
      "BERTopic Average R2 Test: 0.013274548414964449\n",
      "\n",
      "NMF Average Train RMSE: 1.2068268563262738\n",
      "NMF Average Test RMSE: 1.2724258808804154\n",
      "NMF Average R2 Train: 0.03125945263708987\n",
      "NMF Average R2 Test: -0.030756403696450763\n",
      "\n",
      "Emotional_Stability\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 4.128591060638428\n",
      "Epoch 51, Loss: 1.2312276363372803\n",
      "Epoch 101, Loss: 1.2081806659698486\n",
      "Epoch 151, Loss: 1.1986967325210571\n",
      "Epoch 201, Loss: 1.189749002456665\n",
      "Epoch 251, Loss: 1.1821316480636597\n",
      "Epoch 301, Loss: 1.1756304502487183\n",
      "Epoch 351, Loss: 1.1699016094207764\n",
      "Epoch 401, Loss: 1.1647436618804932\n",
      "Epoch 451, Loss: 1.1600594520568848\n",
      "Train RMSE: 1.3896866245093005\n",
      "Test RMSE: 1.4476376581819348\n",
      "R2 Train: 0.03078861795130783\n",
      "R2 Test: 0.01135976488016055\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.128591060638428\n",
      "Epoch 51, Loss: 1.2312276363372803\n",
      "Epoch 101, Loss: 1.2081806659698486\n",
      "Epoch 151, Loss: 1.1986967325210571\n",
      "Epoch 201, Loss: 1.189749002456665\n",
      "Epoch 251, Loss: 1.1821316480636597\n",
      "Epoch 301, Loss: 1.1756304502487183\n",
      "Epoch 351, Loss: 1.1699016094207764\n",
      "Epoch 401, Loss: 1.1647436618804932\n",
      "Epoch 451, Loss: 1.1600594520568848\n",
      "Train RMSE: 1.3896866245093005\n",
      "Test RMSE: 1.4476376581819348\n",
      "R2 Train: 0.03078861795130783\n",
      "R2 Test: 0.01135976488016055\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 4.128591060638428\n",
      "Epoch 51, Loss: 1.2312276363372803\n",
      "Epoch 101, Loss: 1.2081806659698486\n",
      "Epoch 151, Loss: 1.1986967325210571\n",
      "Epoch 201, Loss: 1.189749002456665\n",
      "Epoch 251, Loss: 1.1821316480636597\n",
      "Epoch 301, Loss: 1.1756304502487183\n",
      "Epoch 351, Loss: 1.1699016094207764\n",
      "Epoch 401, Loss: 1.1647436618804932\n",
      "Epoch 451, Loss: 1.1600594520568848\n",
      "Train RMSE: 1.3896866245093005\n",
      "Test RMSE: 1.4476376581819348\n",
      "R2 Train: 0.03078861795130783\n",
      "R2 Test: 0.01135976488016055\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 4.128591060638428\n",
      "Epoch 51, Loss: 1.2312276363372803\n",
      "Epoch 101, Loss: 1.2081806659698486\n",
      "Epoch 151, Loss: 1.1986967325210571\n",
      "Epoch 201, Loss: 1.189749002456665\n",
      "Epoch 251, Loss: 1.1821316480636597\n",
      "Epoch 301, Loss: 1.1756304502487183\n",
      "Epoch 351, Loss: 1.1699016094207764\n",
      "Epoch 401, Loss: 1.1647436618804932\n",
      "Epoch 451, Loss: 1.1600594520568848\n",
      "Train RMSE: 1.3896866245093005\n",
      "Test RMSE: 1.4476376581819348\n",
      "R2 Train: 0.03078861795130783\n",
      "R2 Test: 0.01135976488016055\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 4.128591060638428\n",
      "Epoch 51, Loss: 1.2312276363372803\n",
      "Epoch 101, Loss: 1.2081806659698486\n",
      "Epoch 151, Loss: 1.1986967325210571\n",
      "Epoch 201, Loss: 1.189749002456665\n",
      "Epoch 251, Loss: 1.1821316480636597\n",
      "Epoch 301, Loss: 1.1756304502487183\n",
      "Epoch 351, Loss: 1.1699016094207764\n",
      "Epoch 401, Loss: 1.1647436618804932\n",
      "Epoch 451, Loss: 1.1600594520568848\n",
      "Train RMSE: 1.3896866245093005\n",
      "Test RMSE: 1.4476376581819348\n",
      "R2 Train: 0.03078861795130783\n",
      "R2 Test: 0.01135976488016055\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 4.491147041320801\n",
      "Epoch 51, Loss: 1.3171234130859375\n",
      "Epoch 101, Loss: 1.3165918588638306\n",
      "Epoch 151, Loss: 1.3222525119781494\n",
      "Epoch 201, Loss: 1.3261102437973022\n",
      "Epoch 251, Loss: 1.3302702903747559\n",
      "Epoch 301, Loss: 1.334021806716919\n",
      "Epoch 351, Loss: 1.3371531963348389\n",
      "Epoch 401, Loss: 1.3397564888000488\n",
      "Epoch 451, Loss: 1.3419232368469238\n",
      "Train RMSE: 1.411382881555276\n",
      "Test RMSE: 1.4557629384868702\n",
      "R2 Train: 0.00028906640227222\n",
      "R2 Test: 0.00023056734724480332\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.487431049346924\n",
      "Epoch 51, Loss: 1.320703387260437\n",
      "Epoch 101, Loss: 1.2993175983428955\n",
      "Epoch 151, Loss: 1.2877649068832397\n",
      "Epoch 201, Loss: 1.2773404121398926\n",
      "Epoch 251, Loss: 1.2685681581497192\n",
      "Epoch 301, Loss: 1.261451244354248\n",
      "Epoch 351, Loss: 1.255732774734497\n",
      "Epoch 401, Loss: 1.251130223274231\n",
      "Epoch 451, Loss: 1.2474052906036377\n",
      "Train RMSE: 1.4100647338243362\n",
      "Test RMSE: 1.4594707961389712\n",
      "R2 Train: 0.0021555356049826058\n",
      "R2 Test: -0.0048687844064694286\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 4.479926586151123\n",
      "Epoch 51, Loss: 1.1750729084014893\n",
      "Epoch 101, Loss: 1.1239628791809082\n",
      "Epoch 151, Loss: 1.1163063049316406\n",
      "Epoch 201, Loss: 1.1103373765945435\n",
      "Epoch 251, Loss: 1.1040431261062622\n",
      "Epoch 301, Loss: 1.0980292558670044\n",
      "Epoch 351, Loss: 1.0925626754760742\n",
      "Epoch 401, Loss: 1.0877315998077393\n",
      "Epoch 451, Loss: 1.083533525466919\n",
      "Train RMSE: 1.4093347739123152\n",
      "Test RMSE: 1.4569911279538053\n",
      "R2 Train: 0.003188393040590598\n",
      "R2 Test: -0.0014571033997268223\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 4.498673439025879\n",
      "Epoch 51, Loss: 1.2942591905593872\n",
      "Epoch 101, Loss: 1.2817307710647583\n",
      "Epoch 151, Loss: 1.280692458152771\n",
      "Epoch 201, Loss: 1.279317855834961\n",
      "Epoch 251, Loss: 1.2783602476119995\n",
      "Epoch 301, Loss: 1.2779031991958618\n",
      "Epoch 351, Loss: 1.2778023481369019\n",
      "Epoch 401, Loss: 1.2779145240783691\n",
      "Epoch 451, Loss: 1.2781436443328857\n",
      "Train RMSE: 1.4113757611621796\n",
      "Test RMSE: 1.457379723218617\n",
      "R2 Train: 0.0002991534126326467\n",
      "R2 Test: -0.0019913734822427642\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 4.493107795715332\n",
      "Epoch 51, Loss: 1.2736926078796387\n",
      "Epoch 101, Loss: 1.2372254133224487\n",
      "Epoch 151, Loss: 1.2144532203674316\n",
      "Epoch 201, Loss: 1.1968110799789429\n",
      "Epoch 251, Loss: 1.1841909885406494\n",
      "Epoch 301, Loss: 1.1749699115753174\n",
      "Epoch 351, Loss: 1.1678324937820435\n",
      "Epoch 401, Loss: 1.1620583534240723\n",
      "Epoch 451, Loss: 1.1572778224945068\n",
      "Train RMSE: 1.411468073677243\n",
      "Test RMSE: 1.4574339167645287\n",
      "R2 Train: 0.00016837616677845002\n",
      "R2 Test: -0.0020658941775060224\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1.3744044303894043\n",
      "Epoch 51, Loss: 1.3764535188674927\n",
      "Epoch 101, Loss: 1.3626291751861572\n",
      "Epoch 151, Loss: 1.3456192016601562\n",
      "Epoch 201, Loss: 1.3313393592834473\n",
      "Epoch 251, Loss: 1.3204421997070312\n",
      "Epoch 301, Loss: 1.312034010887146\n",
      "Epoch 351, Loss: 1.3053728342056274\n",
      "Epoch 401, Loss: 1.2999546527862549\n",
      "Epoch 451, Loss: 1.2954356670379639\n",
      "Train RMSE: 1.4113672949162932\n",
      "Test RMSE: 1.456085639345718\n",
      "R2 Train: 0.0003111469412556689\n",
      "R2 Test: -0.0002127221534680146\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1.3745640516281128\n",
      "Epoch 51, Loss: 1.3528679609298706\n",
      "Epoch 101, Loss: 1.3216311931610107\n",
      "Epoch 151, Loss: 1.2955387830734253\n",
      "Epoch 201, Loss: 1.2754590511322021\n",
      "Epoch 251, Loss: 1.2603449821472168\n",
      "Epoch 301, Loss: 1.2485787868499756\n",
      "Epoch 351, Loss: 1.2390940189361572\n",
      "Epoch 401, Loss: 1.2312397956848145\n",
      "Epoch 451, Loss: 1.2245995998382568\n",
      "Train RMSE: 1.410530982398804\n",
      "Test RMSE: 1.4557638064636804\n",
      "R2 Train: 0.0014955368601671237\n",
      "R2 Test: 0.00022937515184151547\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1.373748779296875\n",
      "Epoch 51, Loss: 1.361168384552002\n",
      "Epoch 101, Loss: 1.3317204713821411\n",
      "Epoch 151, Loss: 1.3009077310562134\n",
      "Epoch 201, Loss: 1.2753649950027466\n",
      "Epoch 251, Loss: 1.2556544542312622\n",
      "Epoch 301, Loss: 1.2403604984283447\n",
      "Epoch 351, Loss: 1.2282612323760986\n",
      "Epoch 401, Loss: 1.2184922695159912\n",
      "Epoch 451, Loss: 1.2104566097259521\n",
      "Train RMSE: 1.4105649759119439\n",
      "Test RMSE: 1.4558272808903678\n",
      "R2 Train: 0.0014474087665379276\n",
      "R2 Test: 0.00014218896072659692\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1.3740397691726685\n",
      "Epoch 51, Loss: 1.3820348978042603\n",
      "Epoch 101, Loss: 1.3736796379089355\n",
      "Epoch 151, Loss: 1.3505457639694214\n",
      "Epoch 201, Loss: 1.3232790231704712\n",
      "Epoch 251, Loss: 1.2971738576889038\n",
      "Epoch 301, Loss: 1.2736010551452637\n",
      "Epoch 351, Loss: 1.2527735233306885\n",
      "Epoch 401, Loss: 1.234529733657837\n",
      "Epoch 451, Loss: 1.2185938358306885\n",
      "Train RMSE: 1.4100665567211152\n",
      "Test RMSE: 1.455863318919425\n",
      "R2 Train: 0.0021529556260749816\n",
      "R2 Test: 9.268672745266215e-05\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1.3744990825653076\n",
      "Epoch 51, Loss: 1.3222342729568481\n",
      "Epoch 101, Loss: 1.2679641246795654\n",
      "Epoch 151, Loss: 1.2337119579315186\n",
      "Epoch 201, Loss: 1.2098115682601929\n",
      "Epoch 251, Loss: 1.1917802095413208\n",
      "Epoch 301, Loss: 1.1772797107696533\n",
      "Epoch 351, Loss: 1.1651040315628052\n",
      "Epoch 401, Loss: 1.1545895338058472\n",
      "Epoch 451, Loss: 1.145341396331787\n",
      "Train RMSE: 1.4102697019459907\n",
      "Test RMSE: 1.4556785637432825\n",
      "R2 Train: 0.0018654196043248472\n",
      "R2 Test: 0.0003464555083146914\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 4.274909019470215\n",
      "Epoch 51, Loss: 1.3284693956375122\n",
      "Epoch 101, Loss: 1.3148581981658936\n",
      "Epoch 151, Loss: 1.2932109832763672\n",
      "Epoch 201, Loss: 1.271003246307373\n",
      "Epoch 251, Loss: 1.253180980682373\n",
      "Epoch 301, Loss: 1.23934006690979\n",
      "Epoch 351, Loss: 1.2283320426940918\n",
      "Epoch 401, Loss: 1.2192386388778687\n",
      "Epoch 451, Loss: 1.211439609527588\n",
      "Train RMSE: 1.4086571954924658\n",
      "Test RMSE: 1.4578204250314\n",
      "R2 Train: 0.00414665460152186\n",
      "R2 Test: -0.0025974559917918416\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.271266937255859\n",
      "Epoch 51, Loss: 1.3197097778320312\n",
      "Epoch 101, Loss: 1.313170075416565\n",
      "Epoch 151, Loss: 1.2957051992416382\n",
      "Epoch 201, Loss: 1.2752079963684082\n",
      "Epoch 251, Loss: 1.2584137916564941\n",
      "Epoch 301, Loss: 1.2458245754241943\n",
      "Epoch 351, Loss: 1.2365803718566895\n",
      "Epoch 401, Loss: 1.229774832725525\n",
      "Epoch 451, Loss: 1.2246992588043213\n",
      "Train RMSE: 1.412362921970283\n",
      "Test RMSE: 1.4569710946504226\n",
      "R2 Train: -0.00109978037268732\n",
      "R2 Test: -0.0014295639649770653\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 4.2609028816223145\n",
      "Epoch 51, Loss: 1.3130418062210083\n",
      "Epoch 101, Loss: 1.2875360250473022\n",
      "Epoch 151, Loss: 1.2552286386489868\n",
      "Epoch 201, Loss: 1.2255371809005737\n",
      "Epoch 251, Loss: 1.2031936645507812\n",
      "Epoch 301, Loss: 1.186995267868042\n",
      "Epoch 351, Loss: 1.1751289367675781\n",
      "Epoch 401, Loss: 1.166219711303711\n",
      "Epoch 451, Loss: 1.1593410968780518\n",
      "Train RMSE: 1.4129473335139844\n",
      "Test RMSE: 1.4566152727731148\n",
      "R2 Train: -0.001928427601601168\n",
      "R2 Test: -0.0009404848924019493\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 4.274799823760986\n",
      "Epoch 51, Loss: 1.3224631547927856\n",
      "Epoch 101, Loss: 1.3028807640075684\n",
      "Epoch 151, Loss: 1.2732113599777222\n",
      "Epoch 201, Loss: 1.242841124534607\n",
      "Epoch 251, Loss: 1.2181378602981567\n",
      "Epoch 301, Loss: 1.1990004777908325\n",
      "Epoch 351, Loss: 1.1841862201690674\n",
      "Epoch 401, Loss: 1.1725821495056152\n",
      "Epoch 451, Loss: 1.1633657217025757\n",
      "Train RMSE: 1.4080566739489053\n",
      "Test RMSE: 1.4575023900734354\n",
      "R2 Train: 0.004995553705335776\n",
      "R2 Test: -0.002160054713446158\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 4.27748441696167\n",
      "Epoch 51, Loss: 1.3215253353118896\n",
      "Epoch 101, Loss: 1.3034892082214355\n",
      "Epoch 151, Loss: 1.2757619619369507\n",
      "Epoch 201, Loss: 1.246789813041687\n",
      "Epoch 251, Loss: 1.2224745750427246\n",
      "Epoch 301, Loss: 1.202910304069519\n",
      "Epoch 351, Loss: 1.1871018409729004\n",
      "Epoch 401, Loss: 1.1741089820861816\n",
      "Epoch 451, Loss: 1.1632192134857178\n",
      "Train RMSE: 1.4128253132375699\n",
      "Test RMSE: 1.4570269197829344\n",
      "R2 Train: -0.001755384627007306\n",
      "R2 Test: -0.0015063067491520865\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1.315143346786499\n",
      "Epoch 51, Loss: 0.9564229249954224\n",
      "Epoch 101, Loss: 0.9683494567871094\n",
      "Epoch 151, Loss: 0.9732258319854736\n",
      "Epoch 201, Loss: 0.975687563419342\n",
      "Epoch 251, Loss: 0.9771904349327087\n",
      "Epoch 301, Loss: 0.9782416224479675\n",
      "Epoch 351, Loss: 0.9790700078010559\n",
      "Epoch 401, Loss: 0.9797881841659546\n",
      "Epoch 451, Loss: 0.9804515838623047\n",
      "Train RMSE: 1.3738415728504842\n",
      "Test RMSE: 1.4723813532442565\n",
      "R2 Train: 0.052764297335309296\n",
      "R2 Test: -0.022725664620455888\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1.373713731765747\n",
      "Epoch 51, Loss: 0.9944679737091064\n",
      "Epoch 101, Loss: 1.0061825513839722\n",
      "Epoch 151, Loss: 1.0096008777618408\n",
      "Epoch 201, Loss: 1.0111985206604004\n",
      "Epoch 251, Loss: 1.0123482942581177\n",
      "Epoch 301, Loss: 1.0132960081100464\n",
      "Epoch 351, Loss: 1.0141066312789917\n",
      "Epoch 401, Loss: 1.0148122310638428\n",
      "Epoch 451, Loss: 1.0154340267181396\n",
      "Train RMSE: 1.3744129377076582\n",
      "Test RMSE: 1.4743675453621885\n",
      "R2 Train: 0.051976244701507524\n",
      "R2 Test: -0.025486769610725046\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1.3472033739089966\n",
      "Epoch 51, Loss: 0.9706235527992249\n",
      "Epoch 101, Loss: 0.9800704717636108\n",
      "Epoch 151, Loss: 0.983795702457428\n",
      "Epoch 201, Loss: 0.9859370589256287\n",
      "Epoch 251, Loss: 0.9875837564468384\n",
      "Epoch 301, Loss: 0.9890083074569702\n",
      "Epoch 351, Loss: 0.9903027415275574\n",
      "Epoch 401, Loss: 0.9915046095848083\n",
      "Epoch 451, Loss: 0.9926289916038513\n",
      "Train RMSE: 1.3701756550088493\n",
      "Test RMSE: 1.4656556958859235\n",
      "R2 Train: 0.05781270377336245\n",
      "R2 Test: -0.01340363361371022\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1.3058148622512817\n",
      "Epoch 51, Loss: 0.9959161281585693\n",
      "Epoch 101, Loss: 1.0108118057250977\n",
      "Epoch 151, Loss: 1.016339659690857\n",
      "Epoch 201, Loss: 1.01912522315979\n",
      "Epoch 251, Loss: 1.0209059715270996\n",
      "Epoch 301, Loss: 1.022229552268982\n",
      "Epoch 351, Loss: 1.0233197212219238\n",
      "Epoch 401, Loss: 1.024280309677124\n",
      "Epoch 451, Loss: 1.0251595973968506\n",
      "Train RMSE: 1.3710291970246717\n",
      "Test RMSE: 1.4675637074847532\n",
      "R2 Train: 0.05663847924873888\n",
      "R2 Test: -0.016043877809639007\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1.3527947664260864\n",
      "Epoch 51, Loss: 0.9395865201950073\n",
      "Epoch 101, Loss: 0.946255624294281\n",
      "Epoch 151, Loss: 0.9481598138809204\n",
      "Epoch 201, Loss: 0.9492210149765015\n",
      "Epoch 251, Loss: 0.9501708149909973\n",
      "Epoch 301, Loss: 0.9510928392410278\n",
      "Epoch 351, Loss: 0.9519990682601929\n",
      "Epoch 401, Loss: 0.9528900384902954\n",
      "Epoch 451, Loss: 0.9537601470947266\n",
      "Train RMSE: 1.3686358525545737\n",
      "Test RMSE: 1.468160899219992\n",
      "R2 Train: 0.05992917272643028\n",
      "R2 Test: -0.016870958037397576\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.3896866245093005\n",
      "GensimLDA Average Test RMSE: 1.4476376581819348\n",
      "GensimLDA Average R2 Train: 0.03078861795130783\n",
      "GensimLDA Average R2 Test: 0.01135976488016055\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.41072524482627\n",
      "Mallet_LDA Average Test RMSE: 1.4574077005125585\n",
      "Mallet_LDA Average R2 Train: 0.0012201049254513042\n",
      "Mallet_LDA Average R2 Test: -0.002030517623740047\n",
      "\n",
      "CTM Average Train RMSE: 1.4105599023788293\n",
      "CTM Average Test RMSE: 1.4558437218724947\n",
      "CTM Average R2 Train: 0.0014544935596721097\n",
      "CTM Average R2 Test: 0.00011959683897349027\n",
      "\n",
      "BERTopic Average Train RMSE: 1.4109698876326418\n",
      "BERTopic Average Test RMSE: 1.4571872204622616\n",
      "BERTopic Average R2 Train: 0.0008717231411123682\n",
      "BERTopic Average R2 Test: -0.0017267732623538201\n",
      "\n",
      "NMF Average Train RMSE: 1.3716190430292474\n",
      "NMF Average Test RMSE: 1.4696258402394229\n",
      "NMF Average R2 Train: 0.055824179557069684\n",
      "NMF Average R2 Test: -0.01890618073838555\n",
      "\n",
      "Openness_to_newexp\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 5.421377182006836\n",
      "Epoch 51, Loss: 1.5687319040298462\n",
      "Epoch 101, Loss: 1.5000923871994019\n",
      "Epoch 151, Loss: 1.4603068828582764\n",
      "Epoch 201, Loss: 1.4367549419403076\n",
      "Epoch 251, Loss: 1.4194544553756714\n",
      "Epoch 301, Loss: 1.4054088592529297\n",
      "Epoch 351, Loss: 1.3937759399414062\n",
      "Epoch 401, Loss: 1.3841525316238403\n",
      "Epoch 451, Loss: 1.3762152194976807\n",
      "Train RMSE: 1.1871352594782407\n",
      "Test RMSE: 1.2461765548029347\n",
      "R2 Train: 0.010342526532590268\n",
      "R2 Test: 0.0008073823252617851\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 5.421377182006836\n",
      "Epoch 51, Loss: 1.5687319040298462\n",
      "Epoch 101, Loss: 1.5000923871994019\n",
      "Epoch 151, Loss: 1.4603068828582764\n",
      "Epoch 201, Loss: 1.4367549419403076\n",
      "Epoch 251, Loss: 1.4194544553756714\n",
      "Epoch 301, Loss: 1.4054088592529297\n",
      "Epoch 351, Loss: 1.3937759399414062\n",
      "Epoch 401, Loss: 1.3841525316238403\n",
      "Epoch 451, Loss: 1.3762152194976807\n",
      "Train RMSE: 1.1871352594782407\n",
      "Test RMSE: 1.2461765548029347\n",
      "R2 Train: 0.010342526532590268\n",
      "R2 Test: 0.0008073823252617851\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 5.421377182006836\n",
      "Epoch 51, Loss: 1.5687319040298462\n",
      "Epoch 101, Loss: 1.5000923871994019\n",
      "Epoch 151, Loss: 1.4603068828582764\n",
      "Epoch 201, Loss: 1.4367549419403076\n",
      "Epoch 251, Loss: 1.4194544553756714\n",
      "Epoch 301, Loss: 1.4054088592529297\n",
      "Epoch 351, Loss: 1.3937759399414062\n",
      "Epoch 401, Loss: 1.3841525316238403\n",
      "Epoch 451, Loss: 1.3762152194976807\n",
      "Train RMSE: 1.1871352594782407\n",
      "Test RMSE: 1.2461765548029347\n",
      "R2 Train: 0.010342526532590268\n",
      "R2 Test: 0.0008073823252617851\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 5.421377182006836\n",
      "Epoch 51, Loss: 1.5687319040298462\n",
      "Epoch 101, Loss: 1.5000923871994019\n",
      "Epoch 151, Loss: 1.4603068828582764\n",
      "Epoch 201, Loss: 1.4367549419403076\n",
      "Epoch 251, Loss: 1.4194544553756714\n",
      "Epoch 301, Loss: 1.4054088592529297\n",
      "Epoch 351, Loss: 1.3937759399414062\n",
      "Epoch 401, Loss: 1.3841525316238403\n",
      "Epoch 451, Loss: 1.3762152194976807\n",
      "Train RMSE: 1.1871352594782407\n",
      "Test RMSE: 1.2461765548029347\n",
      "R2 Train: 0.010342526532590268\n",
      "R2 Test: 0.0008073823252617851\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 5.421377182006836\n",
      "Epoch 51, Loss: 1.5687319040298462\n",
      "Epoch 101, Loss: 1.5000923871994019\n",
      "Epoch 151, Loss: 1.4603068828582764\n",
      "Epoch 201, Loss: 1.4367549419403076\n",
      "Epoch 251, Loss: 1.4194544553756714\n",
      "Epoch 301, Loss: 1.4054088592529297\n",
      "Epoch 351, Loss: 1.3937759399414062\n",
      "Epoch 401, Loss: 1.3841525316238403\n",
      "Epoch 451, Loss: 1.3762152194976807\n",
      "Train RMSE: 1.1871352594782407\n",
      "Test RMSE: 1.2461765548029347\n",
      "R2 Train: 0.010342526532590268\n",
      "R2 Test: 0.0008073823252617851\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 5.793076515197754\n",
      "Epoch 51, Loss: 1.5621697902679443\n",
      "Epoch 101, Loss: 1.514673113822937\n",
      "Epoch 151, Loss: 1.4796562194824219\n",
      "Epoch 201, Loss: 1.4539589881896973\n",
      "Epoch 251, Loss: 1.435788631439209\n",
      "Epoch 301, Loss: 1.4232306480407715\n",
      "Epoch 351, Loss: 1.4147658348083496\n",
      "Epoch 401, Loss: 1.4092015027999878\n",
      "Epoch 451, Loss: 1.4056334495544434\n",
      "Train RMSE: 1.1925838196558751\n",
      "Test RMSE: 1.2496991320479516\n",
      "R2 Train: 0.0012372750676150623\n",
      "R2 Test: -0.004849453046585461\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 5.765986919403076\n",
      "Epoch 51, Loss: 1.5157763957977295\n",
      "Epoch 101, Loss: 1.454426646232605\n",
      "Epoch 151, Loss: 1.4122793674468994\n",
      "Epoch 201, Loss: 1.3822596073150635\n",
      "Epoch 251, Loss: 1.3615849018096924\n",
      "Epoch 301, Loss: 1.3473440408706665\n",
      "Epoch 351, Loss: 1.3374810218811035\n",
      "Epoch 401, Loss: 1.3306000232696533\n",
      "Epoch 451, Loss: 1.325746774673462\n",
      "Train RMSE: 1.1904781464179033\n",
      "Test RMSE: 1.25350662939734\n",
      "R2 Train: 0.004761071510776205\n",
      "R2 Test: -0.01098179301401836\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 5.750856876373291\n",
      "Epoch 51, Loss: 1.457610011100769\n",
      "Epoch 101, Loss: 1.3810839653015137\n",
      "Epoch 151, Loss: 1.3406144380569458\n",
      "Epoch 201, Loss: 1.3149908781051636\n",
      "Epoch 251, Loss: 1.2969815731048584\n",
      "Epoch 301, Loss: 1.2832145690917969\n",
      "Epoch 351, Loss: 1.2722433805465698\n",
      "Epoch 401, Loss: 1.2632832527160645\n",
      "Epoch 451, Loss: 1.2558363676071167\n",
      "Train RMSE: 1.1930398125161936\n",
      "Test RMSE: 1.25081567144476\n",
      "R2 Train: 0.00047336106442097403\n",
      "R2 Test: -0.006645813748738183\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 5.778749942779541\n",
      "Epoch 51, Loss: 1.5564898252487183\n",
      "Epoch 101, Loss: 1.538514494895935\n",
      "Epoch 151, Loss: 1.525409460067749\n",
      "Epoch 201, Loss: 1.5142922401428223\n",
      "Epoch 251, Loss: 1.505637526512146\n",
      "Epoch 301, Loss: 1.4989317655563354\n",
      "Epoch 351, Loss: 1.4937177896499634\n",
      "Epoch 401, Loss: 1.4896353483200073\n",
      "Epoch 451, Loss: 1.4864139556884766\n",
      "Train RMSE: 1.1925986300446012\n",
      "Test RMSE: 1.2522928217674498\n",
      "R2 Train: 0.0012124681638032708\n",
      "R2 Test: -0.009024813691361322\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 5.784235954284668\n",
      "Epoch 51, Loss: 1.5370714664459229\n",
      "Epoch 101, Loss: 1.4770066738128662\n",
      "Epoch 151, Loss: 1.4354043006896973\n",
      "Epoch 201, Loss: 1.4087235927581787\n",
      "Epoch 251, Loss: 1.3936079740524292\n",
      "Epoch 301, Loss: 1.3856850862503052\n",
      "Epoch 351, Loss: 1.3813669681549072\n",
      "Epoch 401, Loss: 1.378602385520935\n",
      "Epoch 451, Loss: 1.3765873908996582\n",
      "Train RMSE: 1.1931346266339562\n",
      "Test RMSE: 1.2510903898668677\n",
      "R2 Train: 0.00031448455210170145\n",
      "R2 Test: -0.0070880444063488035\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 1.6497184038162231\n",
      "Epoch 51, Loss: 1.6314843893051147\n",
      "Epoch 101, Loss: 1.5996556282043457\n",
      "Epoch 151, Loss: 1.5778844356536865\n",
      "Epoch 201, Loss: 1.562057614326477\n",
      "Epoch 251, Loss: 1.5498665571212769\n",
      "Epoch 301, Loss: 1.5399526357650757\n",
      "Epoch 351, Loss: 1.5315678119659424\n",
      "Epoch 401, Loss: 1.5243134498596191\n",
      "Epoch 451, Loss: 1.5179505348205566\n",
      "Train RMSE: 1.1932381229698545\n",
      "Test RMSE: 1.2485141271588787\n",
      "R2 Train: 0.00014104515476864865\n",
      "R2 Test: -0.002944695445833645\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 1.650465726852417\n",
      "Epoch 51, Loss: 1.6200530529022217\n",
      "Epoch 101, Loss: 1.5698492527008057\n",
      "Epoch 151, Loss: 1.5313971042633057\n",
      "Epoch 201, Loss: 1.5030174255371094\n",
      "Epoch 251, Loss: 1.4814362525939941\n",
      "Epoch 301, Loss: 1.4641474485397339\n",
      "Epoch 351, Loss: 1.4497294425964355\n",
      "Epoch 401, Loss: 1.4374043941497803\n",
      "Epoch 451, Loss: 1.4267061948776245\n",
      "Train RMSE: 1.1928589389998823\n",
      "Test RMSE: 1.2481271046574371\n",
      "R2 Train: 0.000776409112867471\n",
      "R2 Test: -0.00232299322575491\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 1.6494845151901245\n",
      "Epoch 51, Loss: 1.6285550594329834\n",
      "Epoch 101, Loss: 1.5994168519973755\n",
      "Epoch 151, Loss: 1.581822156906128\n",
      "Epoch 201, Loss: 1.5704550743103027\n",
      "Epoch 251, Loss: 1.5628873109817505\n",
      "Epoch 301, Loss: 1.5576386451721191\n",
      "Epoch 351, Loss: 1.553861379623413\n",
      "Epoch 401, Loss: 1.551087498664856\n",
      "Epoch 451, Loss: 1.5490342378616333\n",
      "Train RMSE: 1.1929774305779908\n",
      "Test RMSE: 1.248302186923106\n",
      "R2 Train: 0.000577885286026425\n",
      "R2 Test: -0.002604216649837676\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 1.6493078470230103\n",
      "Epoch 51, Loss: 1.6316583156585693\n",
      "Epoch 101, Loss: 1.589896321296692\n",
      "Epoch 151, Loss: 1.5607763528823853\n",
      "Epoch 201, Loss: 1.5426698923110962\n",
      "Epoch 251, Loss: 1.5313518047332764\n",
      "Epoch 301, Loss: 1.5238325595855713\n",
      "Epoch 351, Loss: 1.5184476375579834\n",
      "Epoch 401, Loss: 1.5143210887908936\n",
      "Epoch 451, Loss: 1.510977864265442\n",
      "Train RMSE: 1.192915030162276\n",
      "Test RMSE: 1.24830188986349\n",
      "R2 Train: 0.0006824349997179846\n",
      "R2 Test: -0.0026037394686051485\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 1.6498788595199585\n",
      "Epoch 51, Loss: 1.6278138160705566\n",
      "Epoch 101, Loss: 1.608563780784607\n",
      "Epoch 151, Loss: 1.5910451412200928\n",
      "Epoch 201, Loss: 1.5733075141906738\n",
      "Epoch 251, Loss: 1.5573220252990723\n",
      "Epoch 301, Loss: 1.5434401035308838\n",
      "Epoch 351, Loss: 1.5314475297927856\n",
      "Epoch 401, Loss: 1.5210480690002441\n",
      "Epoch 451, Loss: 1.5119750499725342\n",
      "Train RMSE: 1.193088353211603\n",
      "Test RMSE: 1.2485983992100325\n",
      "R2 Train: 0.0003920247927647891\n",
      "R2 Test: -0.0030800932876482445\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 5.564521789550781\n",
      "Epoch 51, Loss: 1.6018409729003906\n",
      "Epoch 101, Loss: 1.5745136737823486\n",
      "Epoch 151, Loss: 1.5596121549606323\n",
      "Epoch 201, Loss: 1.5489693880081177\n",
      "Epoch 251, Loss: 1.5402201414108276\n",
      "Epoch 301, Loss: 1.5325469970703125\n",
      "Epoch 351, Loss: 1.5256571769714355\n",
      "Epoch 401, Loss: 1.519379734992981\n",
      "Epoch 451, Loss: 1.5135916471481323\n",
      "Train RMSE: 1.179788617071828\n",
      "Test RMSE: 1.2434404506703154\n",
      "R2 Train: 0.02255370819051894\n",
      "R2 Test: 0.005190218402877922\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 5.571167945861816\n",
      "Epoch 51, Loss: 1.6034477949142456\n",
      "Epoch 101, Loss: 1.567487120628357\n",
      "Epoch 151, Loss: 1.545644998550415\n",
      "Epoch 201, Loss: 1.5319156646728516\n",
      "Epoch 251, Loss: 1.522482991218567\n",
      "Epoch 301, Loss: 1.5153021812438965\n",
      "Epoch 351, Loss: 1.5094456672668457\n",
      "Epoch 401, Loss: 1.5044416189193726\n",
      "Epoch 451, Loss: 1.5000261068344116\n",
      "Train RMSE: 1.179184623077455\n",
      "Test RMSE: 1.2479162995036994\n",
      "R2 Train: 0.023554261274076005\n",
      "R2 Test: -0.0019844427554693844\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 5.558043479919434\n",
      "Epoch 51, Loss: 1.6075987815856934\n",
      "Epoch 101, Loss: 1.5794436931610107\n",
      "Epoch 151, Loss: 1.5669224262237549\n",
      "Epoch 201, Loss: 1.5588704347610474\n",
      "Epoch 251, Loss: 1.552148699760437\n",
      "Epoch 301, Loss: 1.5459336042404175\n",
      "Epoch 351, Loss: 1.540034532546997\n",
      "Epoch 401, Loss: 1.534390926361084\n",
      "Epoch 451, Loss: 1.5289762020111084\n",
      "Train RMSE: 1.180122568892854\n",
      "Test RMSE: 1.2485393600132957\n",
      "R2 Train: 0.022000276562653576\n",
      "R2 Test: -0.0029852354968762373\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 5.569112300872803\n",
      "Epoch 51, Loss: 1.6050095558166504\n",
      "Epoch 101, Loss: 1.572628140449524\n",
      "Epoch 151, Loss: 1.5560804605484009\n",
      "Epoch 201, Loss: 1.5450693368911743\n",
      "Epoch 251, Loss: 1.536190152168274\n",
      "Epoch 301, Loss: 1.5284041166305542\n",
      "Epoch 351, Loss: 1.5213805437088013\n",
      "Epoch 401, Loss: 1.5149470567703247\n",
      "Epoch 451, Loss: 1.5089898109436035\n",
      "Train RMSE: 1.1773094625133305\n",
      "Test RMSE: 1.247444345125131\n",
      "R2 Train: 0.026657315143418958\n",
      "R2 Test: -0.0012266971871810473\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 5.57620906829834\n",
      "Epoch 51, Loss: 1.601841688156128\n",
      "Epoch 101, Loss: 1.573503017425537\n",
      "Epoch 151, Loss: 1.5569430589675903\n",
      "Epoch 201, Loss: 1.544707179069519\n",
      "Epoch 251, Loss: 1.5347251892089844\n",
      "Epoch 301, Loss: 1.5262045860290527\n",
      "Epoch 351, Loss: 1.5188086032867432\n",
      "Epoch 401, Loss: 1.5123029947280884\n",
      "Epoch 451, Loss: 1.5065053701400757\n",
      "Train RMSE: 1.179967889873916\n",
      "Test RMSE: 1.2492304083799686\n",
      "R2 Train: 0.02225663319514648\n",
      "R2 Test: -0.004095818221782954\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 2.4111087322235107\n",
      "Epoch 51, Loss: 1.371698260307312\n",
      "Epoch 101, Loss: 1.361203908920288\n",
      "Epoch 151, Loss: 1.3531628847122192\n",
      "Epoch 201, Loss: 1.3467084169387817\n",
      "Epoch 251, Loss: 1.3415625095367432\n",
      "Epoch 301, Loss: 1.3374838829040527\n",
      "Epoch 351, Loss: 1.3342576026916504\n",
      "Epoch 401, Loss: 1.3317062854766846\n",
      "Epoch 451, Loss: 1.3296904563903809\n",
      "Train RMSE: 1.1697163193930555\n",
      "Test RMSE: 1.2573978078633168\n",
      "R2 Train: 0.0391721160841374\n",
      "R2 Test: -0.017268184135455833\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 2.472701072692871\n",
      "Epoch 51, Loss: 1.357495665550232\n",
      "Epoch 101, Loss: 1.3458235263824463\n",
      "Epoch 151, Loss: 1.3370509147644043\n",
      "Epoch 201, Loss: 1.3288209438323975\n",
      "Epoch 251, Loss: 1.3218839168548584\n",
      "Epoch 301, Loss: 1.3162068128585815\n",
      "Epoch 351, Loss: 1.3115215301513672\n",
      "Epoch 401, Loss: 1.307648777961731\n",
      "Epoch 451, Loss: 1.304474115371704\n",
      "Train RMSE: 1.1759820658576994\n",
      "Test RMSE: 1.2578121743928639\n",
      "R2 Train: 0.028850932887870107\n",
      "R2 Test: -0.01793876163959429\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 2.3208982944488525\n",
      "Epoch 51, Loss: 1.3903549909591675\n",
      "Epoch 101, Loss: 1.378720760345459\n",
      "Epoch 151, Loss: 1.366665005683899\n",
      "Epoch 201, Loss: 1.3567973375320435\n",
      "Epoch 251, Loss: 1.3484829664230347\n",
      "Epoch 301, Loss: 1.3420857191085815\n",
      "Epoch 351, Loss: 1.3370634317398071\n",
      "Epoch 401, Loss: 1.3330870866775513\n",
      "Epoch 451, Loss: 1.3299311399459839\n",
      "Train RMSE: 1.1695603268951855\n",
      "Test RMSE: 1.259031343431941\n",
      "R2 Train: 0.039428369579242784\n",
      "R2 Test: -0.019913048266706257\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 2.3950352668762207\n",
      "Epoch 51, Loss: 1.3514313697814941\n",
      "Epoch 101, Loss: 1.33733069896698\n",
      "Epoch 151, Loss: 1.3267064094543457\n",
      "Epoch 201, Loss: 1.317562222480774\n",
      "Epoch 251, Loss: 1.3103084564208984\n",
      "Epoch 301, Loss: 1.304697036743164\n",
      "Epoch 351, Loss: 1.3003871440887451\n",
      "Epoch 401, Loss: 1.2970798015594482\n",
      "Epoch 451, Loss: 1.2945401668548584\n",
      "Train RMSE: 1.1697140668623132\n",
      "Test RMSE: 1.2574220303365642\n",
      "R2 Train: 0.03917581662627223\n",
      "R2 Test: -0.01730737775987734\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 2.3998262882232666\n",
      "Epoch 51, Loss: 1.3502720594406128\n",
      "Epoch 101, Loss: 1.33885657787323\n",
      "Epoch 151, Loss: 1.3292648792266846\n",
      "Epoch 201, Loss: 1.3201391696929932\n",
      "Epoch 251, Loss: 1.3123021125793457\n",
      "Epoch 301, Loss: 1.3057780265808105\n",
      "Epoch 351, Loss: 1.3004097938537598\n",
      "Epoch 401, Loss: 1.2960158586502075\n",
      "Epoch 451, Loss: 1.2924283742904663\n",
      "Train RMSE: 1.1674062982552007\n",
      "Test RMSE: 1.2570465786617306\n",
      "R2 Train: 0.04296336197224482\n",
      "R2 Test: -0.01669995602445873\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.1871352594782407\n",
      "GensimLDA Average Test RMSE: 1.2461765548029347\n",
      "GensimLDA Average R2 Train: 0.010342526532590268\n",
      "GensimLDA Average R2 Test: 0.0008073823252617851\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.1923670070537058\n",
      "Mallet_LDA Average Test RMSE: 1.2514809289048738\n",
      "Mallet_LDA Average R2 Train: 0.0015997320717434428\n",
      "Mallet_LDA Average R2 Test: -0.007717983581410426\n",
      "\n",
      "CTM Average Train RMSE: 1.1930155751843212\n",
      "CTM Average Test RMSE: 1.2483687415625888\n",
      "CTM Average R2 Train: 0.0005139598692290636\n",
      "CTM Average R2 Test: -0.002711147615535925\n",
      "\n",
      "BERTopic Average Train RMSE: 1.179274632285877\n",
      "BERTopic Average Test RMSE: 1.247314172738482\n",
      "BERTopic Average R2 Train: 0.02340443887316279\n",
      "BERTopic Average R2 Test: -0.0010203950516863402\n",
      "\n",
      "NMF Average Train RMSE: 1.1704758154526909\n",
      "NMF Average Test RMSE: 1.2577419869372835\n",
      "NMF Average R2 Train: 0.03791811942995347\n",
      "NMF Average R2 Test: -0.01782546556521849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# facebook_2020\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# change this\n",
    "dataset = 'facebook_2020' # folder and dataset name\n",
    "\n",
    "cols = ['age', 'UCLA_3item_sum', 'stress', 'depression',\n",
    "        'Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness_to_newexp'] # outcome columns\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "# START\n",
    "\n",
    "train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "df = pd.read_csv(dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "df['isFemale'] = df['sex'] == 'Female'\n",
    "\n",
    "def func(x):\n",
    "    education = x['education']\n",
    "    bachelor_plus = [\"Master's degree or equivalent\", \"Professional degree (MD, JD, etc.), doctorate degree or higher\"]\n",
    "    bachelor_minus = [\"High school diploma or equivalent\", \"Vocational/Technical Training\"]\n",
    "\n",
    "    if education in bachelor_plus:\n",
    "        return 'bachelor+'\n",
    "    elif education in bachelor_minus:\n",
    "        return \"bachelor-\"\n",
    "    else:\n",
    "        return 'bachelor'\n",
    "\n",
    "df['education_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "label_encoder = LabelEncoder()\n",
    "df['education_bucket'] = label_encoder.fit_transform(df['education_bucket'])\n",
    "\n",
    "def func(x):\n",
    "    income = x['income']\n",
    "    under = [\"$10,000 - $19,999\", \"$20,000 - $34,999\", \"$35,000 - $49,999\"]\n",
    "    over = [\"$100,000 - $149,999\", \"More than $150,000\"]\n",
    "\n",
    "    if income in under:\n",
    "        return '<50k+'\n",
    "    elif income in over:\n",
    "        return \"100k+\"\n",
    "    else:\n",
    "        return '$50,000 - $99,999'\n",
    "\n",
    "df['income_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "label_encoder = LabelEncoder()\n",
    "df['income_bucket'] = label_encoder.fit_transform(df['income_bucket'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['marital_status'] = label_encoder.fit_transform(df['marital_status'])\n",
    "\n",
    "merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "all_user_ids = list(user_ids.keys())\n",
    "\n",
    "labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "\n",
    "all_ys = []\n",
    "for key in user_ids.keys():\n",
    "    all_ys.append(labels_data[labels_data['user_id'] == key][cols].iloc[0])\n",
    "\n",
    "for outcome in cols:\n",
    "    print(outcome)\n",
    "    print()\n",
    "\n",
    "    y = [i[outcome] for i in all_ys]\n",
    "    \n",
    "    # REGRESSION\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for model in ['GensimLDA', 'Mallet_LDA', 'CTM', 'BERTopic', 'NMF']:\n",
    "        print(model)\n",
    "        \n",
    "        train_error_list = []\n",
    "        test_error_list = []\n",
    "\n",
    "        r2s_train = []\n",
    "        r2s_test = []\n",
    "\n",
    "        for run in range(1, 6):\n",
    "            print(\"Run: \" + str(run))\n",
    "            print()\n",
    "\n",
    "            topics = []\n",
    "            with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    topic_list = [item.strip() for item in row if item.strip()]\n",
    "                    topics.append(topic_list)\n",
    "\n",
    "            X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "            # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "            X_train = X[:round(0.80 * len(X))]\n",
    "            X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "            y_train = y[:round(0.80 * len(X))]\n",
    "            y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            X_test = np.array(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            torch.manual_seed(42) # for reproducibility\n",
    "\n",
    "            X_train_NN = torch.tensor(X_train, dtype = torch.float32)\n",
    "            y_train_NN = torch.tensor(y_train, dtype = torch.float32).reshape(-1, 1)\n",
    "            X_test_NN = torch.tensor(X_test, dtype = torch.float32)\n",
    "            y_test_NN = torch.tensor(y_test, dtype = torch.float32).reshape(-1, 1)\n",
    "\n",
    "            model_NN = nn.Sequential(\n",
    "                nn.Linear(X_train_NN.shape[1], 1),\n",
    "            )\n",
    "\n",
    "            loss_fn = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model_NN.parameters(), lr = lr)\n",
    "\n",
    "            n_epochs = 500\n",
    "            batch_size = 64\n",
    "            batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "            best_mse = np.inf\n",
    "            best_weights = None\n",
    "            history = []\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                model_NN.train()\n",
    "                with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                    bar.set_description(f\"Epoch {epoch}\")\n",
    "                    for start in bar:\n",
    "\n",
    "                        X_batch = X_train_NN[start : start+batch_size]\n",
    "                        y_batch = y_train_NN[start : start+batch_size]\n",
    "\n",
    "                        y_pred = model_NN(X_batch)\n",
    "                        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                        bar.set_postfix(mse=float(loss))\n",
    "\n",
    "                model_NN.eval()\n",
    "                y_pred = model_NN(X_test_NN)\n",
    "                mse = loss_fn(y_pred, y_test_NN)\n",
    "                mse = float(mse)\n",
    "                history.append(mse)\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_weights = copy.deepcopy(model_NN.state_dict())\n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "            model_NN.load_state_dict(best_weights)\n",
    "\n",
    "            y_pred_train = model_NN(X_train_NN).detach().numpy()\n",
    "            y_pred_test = model_NN(X_test_NN).detach().numpy()\n",
    "\n",
    "            train_error = np.sqrt(mean_squared_error(y_pred_train, y_train))\n",
    "            test_error = np.sqrt(mean_squared_error(y_pred_test, y_test))\n",
    "\n",
    "            train_error_list.append(train_error)\n",
    "            test_error_list.append(test_error)\n",
    "\n",
    "            print(f'Train RMSE: {train_error}')\n",
    "            print(f'Test RMSE: {test_error}')\n",
    "\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "            r2s_train.append(r2_train)\n",
    "            r2s_test.append(r2_test)\n",
    "            \n",
    "            print(f'R2 Train: {r2_train}')\n",
    "            print(f'R2 Test: {r2_test}')\n",
    "\n",
    "            all_scores[model] = {\n",
    "                'Train RMSE': train_error_list,\n",
    "                'Test RMSE': test_error_list,\n",
    "                'R2 Train': r2s_train,\n",
    "                'R2 Test': r2s_test,\n",
    "\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': y_pred_train,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_pred_test\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    for m in all_scores.keys():\n",
    "        print(f'{m} Average Train RMSE: {np.mean(all_scores[m][\"Train RMSE\"])}')\n",
    "        print(f'{m} Average Test RMSE: {np.mean(all_scores[m][\"Test RMSE\"])}')\n",
    "        print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "        print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "        print()\n",
    "\n",
    "    with open('all_results/' + dataset + '_all_scores_' + outcome + '_NN.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 9.151412818922838\n",
      "Test RMSE: 10.808739626343064\n",
      "R2 Train: 0.39961933730010935\n",
      "R2 Test: 0.18601798905532063\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 9.151412818922838\n",
      "Test RMSE: 10.808739626343064\n",
      "R2 Train: 0.39961933730010935\n",
      "R2 Test: 0.18601798905532063\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 9.151412818922838\n",
      "Test RMSE: 10.808739626343064\n",
      "R2 Train: 0.39961933730010935\n",
      "R2 Test: 0.18601798905532063\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 9.151412818922838\n",
      "Test RMSE: 10.808739626343064\n",
      "R2 Train: 0.39961933730010935\n",
      "R2 Test: 0.18601798905532063\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 9.151412818922838\n",
      "Test RMSE: 10.808739626343064\n",
      "R2 Train: 0.39961933730010935\n",
      "R2 Test: 0.18601798905532063\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 11.165818661144902\n",
      "Test RMSE: 12.186993375025711\n",
      "R2 Train: 0.1062181254218314\n",
      "R2 Test: -0.03480339957278589\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 11.119262384038457\n",
      "Test RMSE: 13.2936821705826\n",
      "R2 Train: 0.11365589748113492\n",
      "R2 Test: -0.2312756174111712\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 11.231907798990019\n",
      "Test RMSE: 13.188333748171553\n",
      "R2 Train: 0.09560643858121975\n",
      "R2 Test: -0.21183796662225385\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 11.230360061068662\n",
      "Test RMSE: 12.323741581661215\n",
      "R2 Train: 0.09585566921831445\n",
      "R2 Test: -0.05815639907548875\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 11.072510103911752\n",
      "Test RMSE: 11.97626320902693\n",
      "R2 Train: 0.12109370936624775\n",
      "R2 Test: 0.0006735990872610564\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 8.768147918848863\n",
      "Test RMSE: 10.82910818801665\n",
      "R2 Train: 0.4488546623818137\n",
      "R2 Test: 0.1829472768088417\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 8.981420318111585\n",
      "Test RMSE: 10.411811187994553\n",
      "R2 Train: 0.4217169740617217\n",
      "R2 Test: 0.24470386142109146\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 8.73323639873573\n",
      "Test RMSE: 10.309275209303024\n",
      "R2 Train: 0.4532348384022128\n",
      "R2 Test: 0.2595069895876245\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 8.765249929677418\n",
      "Test RMSE: 10.045958693996631\n",
      "R2 Train: 0.4492189238621205\n",
      "R2 Test: 0.29685082265723184\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 8.973505885676678\n",
      "Test RMSE: 10.68924376706642\n",
      "R2 Train: 0.42273569164616864\n",
      "R2 Test: 0.2039164330170128\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 9.016494788956756\n",
      "Test RMSE: 10.753631220996608\n",
      "R2 Train: 0.41719150365707325\n",
      "R2 Test: 0.19429701134807909\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 9.156463471798382\n",
      "Test RMSE: 10.846113110189515\n",
      "R2 Train: 0.39895645580942063\n",
      "R2 Test: 0.18037923029562497\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 9.09557095331191\n",
      "Test RMSE: 10.663865220278522\n",
      "R2 Train: 0.4069240208655226\n",
      "R2 Test: 0.2076920902752344\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 9.045480448344064\n",
      "Test RMSE: 10.69314876354581\n",
      "R2 Train: 0.41343832853137985\n",
      "R2 Test: 0.20333467598532862\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 8.929347920769555\n",
      "Test RMSE: 10.551677008855131\n",
      "R2 Train: 0.4284030636126954\n",
      "R2 Test: 0.22427520315620764\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 11.467720847909339\n",
      "Test RMSE: 12.491633835522396\n",
      "R2 Train: 0.057232445909423446\n",
      "R2 Test: -0.08718433834447237\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 11.410965207241606\n",
      "Test RMSE: 12.944784951238653\n",
      "R2 Train: 0.06654117757948819\n",
      "R2 Test: -0.16749324566545054\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 11.429784369642862\n",
      "Test RMSE: 12.640350299457802\n",
      "R2 Train: 0.06345968557873838\n",
      "R2 Test: -0.11322491061915474\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 11.379630158796537\n",
      "Test RMSE: 12.578164865897076\n",
      "R2 Train: 0.07166078244778984\n",
      "R2 Test: -0.10229861678395569\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 11.453098741510226\n",
      "Test RMSE: 12.677474732421427\n",
      "R2 Train: 0.059635095767192836\n",
      "R2 Test: -0.11977354765444304\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 9.151412818922838\n",
      "GensimLDA Average Test RMSE: 10.808739626343064\n",
      "GensimLDA Average R2 Train: 0.39961933730010935\n",
      "GensimLDA Average R2 Test: 0.18601798905532063\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 11.16397180183076\n",
      "Mallet_LDA Average Test RMSE: 12.593802816893604\n",
      "Mallet_LDA Average R2 Train: 0.10648596801374965\n",
      "Mallet_LDA Average R2 Test: -0.10707995671888773\n",
      "\n",
      "BERTopic Average Train RMSE: 8.844312090210057\n",
      "BERTopic Average Test RMSE: 10.457079409275456\n",
      "BERTopic Average R2 Train: 0.43915221807080745\n",
      "BERTopic Average R2 Test: 0.23758507669836043\n",
      "\n",
      "NMF Average Train RMSE: 9.048671516636134\n",
      "NMF Average Test RMSE: 10.701687064773118\n",
      "NMF Average R2 Train: 0.4129826744952183\n",
      "NMF Average R2 Test: 0.20199564221209493\n",
      "\n",
      "CTM Average Train RMSE: 11.428239865020114\n",
      "CTM Average Test RMSE: 12.666481736907471\n",
      "CTM Average R2 Train: 0.06370583745652654\n",
      "CTM Average R2 Test: -0.11799493181349527\n",
      "\n",
      "UCLA_3item_sum\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 2.2099667285996345\n",
      "Test RMSE: 2.4455624947963877\n",
      "R2 Train: 0.07090852126335923\n",
      "R2 Test: -0.04400939359373868\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 2.2099667285996345\n",
      "Test RMSE: 2.4455624947963877\n",
      "R2 Train: 0.07090852126335923\n",
      "R2 Test: -0.04400939359373868\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 2.2099667285996345\n",
      "Test RMSE: 2.4455624947963877\n",
      "R2 Train: 0.07090852126335923\n",
      "R2 Test: -0.04400939359373868\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 2.2099667285996345\n",
      "Test RMSE: 2.4455624947963877\n",
      "R2 Train: 0.07090852126335923\n",
      "R2 Test: -0.04400939359373868\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 2.2099667285996345\n",
      "Test RMSE: 2.4455624947963877\n",
      "R2 Train: 0.07090852126335923\n",
      "R2 Test: -0.04400939359373868\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 2.232343613830932\n",
      "Test RMSE: 2.467659959223679\n",
      "R2 Train: 0.05199834678520787\n",
      "R2 Test: -0.06296142410345129\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 2.232640083478999\n",
      "Test RMSE: 2.5205301996559752\n",
      "R2 Train: 0.05174652858156947\n",
      "R2 Test: -0.10899780366467016\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 2.2388719759214215\n",
      "Test RMSE: 2.4767204961209894\n",
      "R2 Train: 0.04644548515891689\n",
      "R2 Test: -0.07078153100205653\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 2.2403231849177843\n",
      "Test RMSE: 2.497917540032026\n",
      "R2 Train: 0.04520892014889111\n",
      "R2 Test: -0.08918855850021212\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 2.2372570293376715\n",
      "Test RMSE: 2.4152419330029704\n",
      "R2 Train: 0.04782062784408403\n",
      "R2 Test: -0.018282208734193617\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 2.192487772104262\n",
      "Test RMSE: 2.4277033350636255\n",
      "R2 Train: 0.08554704845740235\n",
      "R2 Test: -0.028816937172300605\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 2.1940981562771307\n",
      "Test RMSE: 2.3975904292545582\n",
      "R2 Train: 0.08420322214349474\n",
      "R2 Test: -0.003452612741418948\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 2.187165254721092\n",
      "Test RMSE: 2.396800091033769\n",
      "R2 Train: 0.08998153968457523\n",
      "R2 Test: -0.0027911684595207653\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 2.1969278455600887\n",
      "Test RMSE: 2.412466740367021\n",
      "R2 Train: 0.08183952535577188\n",
      "R2 Test: -0.015943473466782487\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 2.197287662671602\n",
      "Test RMSE: 2.386819147946955\n",
      "R2 Train: 0.08153874451322607\n",
      "R2 Test: 0.005543245327515267\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 2.1964834090021936\n",
      "Test RMSE: 2.4118372236787144\n",
      "R2 Train: 0.08221097388219456\n",
      "R2 Test: -0.015413335648353454\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 2.1962503903576875\n",
      "Test RMSE: 2.440142841684084\n",
      "R2 Train: 0.08240569478009019\n",
      "R2 Test: -0.039387226555495314\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 2.194082461234669\n",
      "Test RMSE: 2.4271648346641412\n",
      "R2 Train: 0.0842163240349163\n",
      "R2 Test: -0.02836057425600469\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 2.191665093672124\n",
      "Test RMSE: 2.4221400842302856\n",
      "R2 Train: 0.08623317276661113\n",
      "R2 Test: -0.024107128831895475\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 2.196235029833763\n",
      "Test RMSE: 2.432089231200181\n",
      "R2 Train: 0.08241853000147537\n",
      "R2 Test: -0.032537622577191616\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 2.241181693927321\n",
      "Test RMSE: 2.5224706332269515\n",
      "R2 Train: 0.04447701342504562\n",
      "R2 Test: -0.1107059878429808\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 2.2469906138093783\n",
      "Test RMSE: 2.495479886607014\n",
      "R2 Train: 0.0395173533185047\n",
      "R2 Test: -0.08706377361586792\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 2.2372085551636003\n",
      "Test RMSE: 2.4823992020056793\n",
      "R2 Train: 0.04786188873454078\n",
      "R2 Test: -0.07569740612925213\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 2.227685505181398\n",
      "Test RMSE: 2.5396066089990383\n",
      "R2 Train: 0.05595050493055198\n",
      "R2 Test: -0.12584803110582254\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 2.2481188365591196\n",
      "Test RMSE: 2.458112351515007\n",
      "R2 Train: 0.03855258700788222\n",
      "R2 Test: -0.054751941724606734\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 2.2099667285996345\n",
      "GensimLDA Average Test RMSE: 2.4455624947963877\n",
      "GensimLDA Average R2 Train: 0.07090852126335923\n",
      "GensimLDA Average R2 Test: -0.04400939359373868\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 2.2362871774973616\n",
      "Mallet_LDA Average Test RMSE: 2.475614025607128\n",
      "Mallet_LDA Average R2 Train: 0.04864398170373387\n",
      "Mallet_LDA Average R2 Test: -0.07004230520091674\n",
      "\n",
      "BERTopic Average Train RMSE: 2.1935933382668353\n",
      "BERTopic Average Test RMSE: 2.4042759487331855\n",
      "BERTopic Average R2 Train: 0.08462201603089406\n",
      "BERTopic Average R2 Test: -0.009092189302501507\n",
      "\n",
      "NMF Average Train RMSE: 2.1949432768200876\n",
      "NMF Average Test RMSE: 2.4266748430914817\n",
      "NMF Average R2 Train: 0.08349693909305751\n",
      "NMF Average R2 Test: -0.02796117757378811\n",
      "\n",
      "CTM Average Train RMSE: 2.240237040928163\n",
      "CTM Average Test RMSE: 2.499613736470738\n",
      "CTM Average R2 Train: 0.04527186948330506\n",
      "CTM Average R2 Test: -0.09081342808370603\n",
      "\n",
      "stress\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 6.753146074035162\n",
      "Test RMSE: 7.576019117380918\n",
      "R2 Train: 0.09046241979173841\n",
      "R2 Test: -0.020116638670775178\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 6.753146074035162\n",
      "Test RMSE: 7.576019117380918\n",
      "R2 Train: 0.09046241979173841\n",
      "R2 Test: -0.020116638670775178\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 6.753146074035162\n",
      "Test RMSE: 7.576019117380918\n",
      "R2 Train: 0.09046241979173841\n",
      "R2 Test: -0.020116638670775178\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 6.753146074035162\n",
      "Test RMSE: 7.576019117380918\n",
      "R2 Train: 0.09046241979173841\n",
      "R2 Test: -0.020116638670775178\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 6.753146074035162\n",
      "Test RMSE: 7.576019117380918\n",
      "R2 Train: 0.09046241979173841\n",
      "R2 Test: -0.020116638670775178\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 6.921076823215716\n",
      "Test RMSE: 7.66961600015422\n",
      "R2 Train: 0.04466497575966566\n",
      "R2 Test: -0.045478119833060715\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 6.931197081917973\n",
      "Test RMSE: 7.6216757100963335\n",
      "R2 Train: 0.04186907955160479\n",
      "R2 Test: -0.032449076210507855\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 6.929024371118589\n",
      "Test RMSE: 7.743814698342544\n",
      "R2 Train: 0.04246967280944436\n",
      "R2 Test: -0.06580465272882985\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 6.867909696702184\n",
      "Test RMSE: 7.698330494393909\n",
      "R2 Train: 0.059286205334208364\n",
      "R2 Test: -0.05332116504400419\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 6.939171094844571\n",
      "Test RMSE: 7.831656661891575\n",
      "R2 Train: 0.0396632432239068\n",
      "R2 Test: -0.09012170571341871\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 6.75343473453215\n",
      "Test RMSE: 7.559773152721305\n",
      "R2 Train: 0.09038466249808641\n",
      "R2 Test: -0.015746267021488736\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 6.739514698825199\n",
      "Test RMSE: 7.514711077745943\n",
      "R2 Train: 0.09413055752138688\n",
      "R2 Test: -0.00367309593346854\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 6.777957410033594\n",
      "Test RMSE: 7.587769510360906\n",
      "R2 Train: 0.08376678548709404\n",
      "R2 Test: -0.02328349105990757\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 6.771547916376517\n",
      "Test RMSE: 7.515458306646418\n",
      "R2 Train: 0.08549881593500708\n",
      "R2 Test: -0.003872707288619992\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 6.730884521023068\n",
      "Test RMSE: 7.542748551574985\n",
      "R2 Train: 0.09644906537088416\n",
      "R2 Test: -0.011176499339316859\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 6.70955678860257\n",
      "Test RMSE: 7.647793646507865\n",
      "R2 Train: 0.10216604404831964\n",
      "R2 Test: -0.03953718724881883\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 6.732230484052249\n",
      "Test RMSE: 7.6568027664393075\n",
      "R2 Train: 0.09608766628745391\n",
      "R2 Test: -0.04198778458670316\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 6.7260443247776704\n",
      "Test RMSE: 7.623165294142281\n",
      "R2 Train: 0.09774808970538584\n",
      "R2 Test: -0.032852680356516784\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 6.716054980672801\n",
      "Test RMSE: 7.62569948859736\n",
      "R2 Train: 0.10042610117892503\n",
      "R2 Test: -0.03353950387358151\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 6.722087337942494\n",
      "Test RMSE: 7.643579921815207\n",
      "R2 Train: 0.09880938177249254\n",
      "R2 Test: -0.038391989823743256\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 6.872224756385383\n",
      "Test RMSE: 8.004393358304153\n",
      "R2 Train: 0.058103746174733795\n",
      "R2 Test: -0.1387399399567142\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 6.899667944192093\n",
      "Test RMSE: 7.7745409308732265\n",
      "R2 Train: 0.050566086097939666\n",
      "R2 Test: -0.07427932125702452\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 6.894003816862029\n",
      "Test RMSE: 7.794609301051457\n",
      "R2 Train: 0.052124279120079664\n",
      "R2 Test: -0.07983253922367606\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 6.835828624388748\n",
      "Test RMSE: 7.990980440774838\n",
      "R2 Train: 0.06805411957744867\n",
      "R2 Test: -0.13492677708801737\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 6.911489180366333\n",
      "Test RMSE: 7.753521433883562\n",
      "R2 Train: 0.047309959210741215\n",
      "R2 Test: -0.0684782621181943\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 6.753146074035162\n",
      "GensimLDA Average Test RMSE: 7.576019117380918\n",
      "GensimLDA Average R2 Train: 0.09046241979173841\n",
      "GensimLDA Average R2 Test: -0.020116638670775178\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 6.917675813559808\n",
      "Mallet_LDA Average Test RMSE: 7.713018712975716\n",
      "Mallet_LDA Average R2 Train: 0.045590635335765996\n",
      "Mallet_LDA Average R2 Test: -0.057434943905964264\n",
      "\n",
      "BERTopic Average Train RMSE: 6.754667856158105\n",
      "BERTopic Average Test RMSE: 7.544092119809912\n",
      "BERTopic Average R2 Train: 0.09004597736249172\n",
      "BERTopic Average R2 Test: -0.011550412128560339\n",
      "\n",
      "NMF Average Train RMSE: 6.721194783209557\n",
      "NMF Average Test RMSE: 7.639408223500405\n",
      "NMF Average R2 Train: 0.0990474565985154\n",
      "NMF Average R2 Test: -0.037261829177872706\n",
      "\n",
      "CTM Average Train RMSE: 6.882642864438917\n",
      "CTM Average Test RMSE: 7.8636090929774465\n",
      "CTM Average R2 Train: 0.0552316380361886\n",
      "CTM Average R2 Test: -0.09925136792872528\n",
      "\n",
      "depression\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 5.671037106821275\n",
      "Test RMSE: 6.292369351614618\n",
      "R2 Train: 0.11081857440798659\n",
      "R2 Test: -0.016521525130617665\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 5.671037106821275\n",
      "Test RMSE: 6.292369351614618\n",
      "R2 Train: 0.11081857440798659\n",
      "R2 Test: -0.016521525130617665\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 5.671037106821275\n",
      "Test RMSE: 6.292369351614618\n",
      "R2 Train: 0.11081857440798659\n",
      "R2 Test: -0.016521525130617665\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 5.671037106821275\n",
      "Test RMSE: 6.292369351614618\n",
      "R2 Train: 0.11081857440798659\n",
      "R2 Test: -0.016521525130617665\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 5.671037106821275\n",
      "Test RMSE: 6.292369351614618\n",
      "R2 Train: 0.11081857440798659\n",
      "R2 Test: -0.016521525130617665\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 5.856773236999256\n",
      "Test RMSE: 6.419660577393154\n",
      "R2 Train: 0.0516203583226541\n",
      "R2 Test: -0.058064877622904465\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 5.834209451379718\n",
      "Test RMSE: 6.237420442909532\n",
      "R2 Train: 0.05891373067296968\n",
      "R2 Test: 0.0011547609626711974\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 5.852913151064937\n",
      "Test RMSE: 6.497928500727018\n",
      "R2 Train: 0.05287006370718783\n",
      "R2 Test: -0.08402181451607982\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 5.868550628870956\n",
      "Test RMSE: 6.333717006257466\n",
      "R2 Train: 0.047802327967327085\n",
      "R2 Test: -0.0299247036116983\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 5.830117968051182\n",
      "Test RMSE: 6.32585649581473\n",
      "R2 Train: 0.06023322003074638\n",
      "R2 Test: -0.027369897250080433\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 5.66183821395907\n",
      "Test RMSE: 6.217269376157469\n",
      "R2 Train: 0.1137008869774998\n",
      "R2 Test: 0.007598220756327989\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 5.647677813768852\n",
      "Test RMSE: 6.255396134957394\n",
      "R2 Train: 0.11812865594009803\n",
      "R2 Test: -0.004610701490467495\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 5.657099599589766\n",
      "Test RMSE: 6.157908088983928\n",
      "R2 Train: 0.11518382310275\n",
      "R2 Test: 0.02645827382204191\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 5.637063838662272\n",
      "Test RMSE: 6.203259253265094\n",
      "R2 Train: 0.12144023471028365\n",
      "R2 Test: 0.01206577866201397\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 5.6182776422416225\n",
      "Test RMSE: 6.239213224888913\n",
      "R2 Train: 0.12728629027237348\n",
      "R2 Test: 0.0005804950116877761\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 5.623523224249182\n",
      "Test RMSE: 6.241229912785412\n",
      "R2 Train: 0.1256558872447302\n",
      "R2 Test: -6.568997423639367e-05\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 5.626526595382344\n",
      "Test RMSE: 6.214727656129014\n",
      "R2 Train: 0.12472171092990259\n",
      "R2 Test: 0.008409474612502299\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 5.63309614572247\n",
      "Test RMSE: 6.221611543674578\n",
      "R2 Train: 0.1226765622396242\n",
      "R2 Test: 0.0062115414017783754\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 5.623019296155277\n",
      "Test RMSE: 6.215318837796538\n",
      "R2 Train: 0.12581258147468088\n",
      "R2 Test: 0.008220813723522125\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 5.636767229000834\n",
      "Test RMSE: 6.219233475000632\n",
      "R2 Train: 0.12153268796828476\n",
      "R2 Test: 0.006971102016320563\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 5.815293592075953\n",
      "Test RMSE: 6.635086181192085\n",
      "R2 Train: 0.06500627753439991\n",
      "R2 Test: -0.13026766034067183\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 5.844232332231491\n",
      "Test RMSE: 6.57344282033461\n",
      "R2 Train: 0.05567747464893413\n",
      "R2 Test: -0.10936368289160092\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 5.8550478761791025\n",
      "Test RMSE: 6.492474536332258\n",
      "R2 Train: 0.052179046866616785\n",
      "R2 Test: -0.08220285476539968\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 5.794619500322836\n",
      "Test RMSE: 6.640388810161627\n",
      "R2 Train: 0.07164249810180856\n",
      "R2 Test: -0.13207495711856154\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 5.868991435539945\n",
      "Test RMSE: 6.3970600557928\n",
      "R2 Train: 0.04765927702530326\n",
      "R2 Test: -0.050628121396623094\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 5.671037106821275\n",
      "GensimLDA Average Test RMSE: 6.292369351614618\n",
      "GensimLDA Average R2 Train: 0.11081857440798659\n",
      "GensimLDA Average R2 Test: -0.016521525130617665\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 5.848512887273209\n",
      "Mallet_LDA Average Test RMSE: 6.36291660462038\n",
      "Mallet_LDA Average R2 Train: 0.054287940140177016\n",
      "Mallet_LDA Average R2 Test: -0.03964530640761836\n",
      "\n",
      "BERTopic Average Train RMSE: 5.6443914216443165\n",
      "BERTopic Average Test RMSE: 6.21460921565056\n",
      "BERTopic Average R2 Train: 0.119147978200601\n",
      "BERTopic Average R2 Test: 0.00841841335232083\n",
      "\n",
      "NMF Average Train RMSE: 5.628586498102022\n",
      "NMF Average Test RMSE: 6.222424285077235\n",
      "NMF Average R2 Train: 0.12407988597144452\n",
      "NMF Average R2 Test: 0.005949448355977394\n",
      "\n",
      "CTM Average Train RMSE: 5.835636947269866\n",
      "CTM Average Test RMSE: 6.547690480762677\n",
      "CTM Average R2 Train: 0.05843291483541253\n",
      "CTM Average R2 Test: -0.10090745530257142\n",
      "\n",
      "Extraversion\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.4973040266927462\n",
      "Test RMSE: 1.5476411353120643\n",
      "R2 Train: 0.06006537125747691\n",
      "R2 Test: -0.009364852190689499\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.4973040266927462\n",
      "Test RMSE: 1.5476411353120643\n",
      "R2 Train: 0.06006537125747691\n",
      "R2 Test: -0.009364852190689499\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.4973040266927462\n",
      "Test RMSE: 1.5476411353120643\n",
      "R2 Train: 0.06006537125747691\n",
      "R2 Test: -0.009364852190689499\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.4973040266927462\n",
      "Test RMSE: 1.5476411353120643\n",
      "R2 Train: 0.06006537125747691\n",
      "R2 Test: -0.009364852190689499\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.4973040266927462\n",
      "Test RMSE: 1.5476411353120643\n",
      "R2 Train: 0.06006537125747691\n",
      "R2 Test: -0.009364852190689499\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.501310831663272\n",
      "Test RMSE: 1.611358045058463\n",
      "R2 Train: 0.055028085859371245\n",
      "R2 Test: -0.0941875080734329\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.5068067374676573\n",
      "Test RMSE: 1.567537218037959\n",
      "R2 Train: 0.04809683285415822\n",
      "R2 Test: -0.03548394828709167\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.5182708545004129\n",
      "Test RMSE: 1.6013216377958084\n",
      "R2 Train: 0.03355715465670517\n",
      "R2 Test: -0.08059957648439497\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.5153996828949092\n",
      "Test RMSE: 1.5764225125569273\n",
      "R2 Train: 0.037208939898227045\n",
      "R2 Test: -0.04725611629152682\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.5132444612902851\n",
      "Test RMSE: 1.7414932910293273\n",
      "R2 Train: 0.03994558099771417\n",
      "R2 Test: -0.2780600515174796\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.4901769121377801\n",
      "Test RMSE: 1.5223524764064875\n",
      "R2 Train: 0.06899218652005745\n",
      "R2 Test: 0.02335195535441048\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.4882407775108901\n",
      "Test RMSE: 1.520564101395382\n",
      "R2 Train: 0.07140986653065984\n",
      "R2 Test: 0.025645231153369363\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.4846333317205382\n",
      "Test RMSE: 1.5295995498163701\n",
      "R2 Train: 0.07590615324752492\n",
      "R2 Test: 0.014031266466553838\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.489595270552613\n",
      "Test RMSE: 1.5443182005371767\n",
      "R2 Train: 0.06971882129093765\n",
      "R2 Test: -0.0050350980206732565\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.4846129707588231\n",
      "Test RMSE: 1.6728763545602414\n",
      "R2 Train: 0.07593149999141258\n",
      "R2 Test: -0.17932996674626156\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.4954978918822746\n",
      "Test RMSE: 1.5473681755157307\n",
      "R2 Train: 0.062331610740233745\n",
      "R2 Test: -0.009008837192630237\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.4966150734022516\n",
      "Test RMSE: 1.5653212995534223\n",
      "R2 Train: 0.060930154976224116\n",
      "R2 Test: -0.03255843408113113\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.4932412469713496\n",
      "Test RMSE: 1.5491141191969757\n",
      "R2 Train: 0.06515928190352804\n",
      "R2 Test: -0.01128711395510651\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.4935903228716483\n",
      "Test RMSE: 1.557243752036743\n",
      "R2 Train: 0.06472215426699934\n",
      "R2 Test: -0.021929280922242445\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.4914290711498412\n",
      "Test RMSE: 1.560522009536823\n",
      "R2 Train: 0.06742692323043653\n",
      "R2 Test: -0.026236472567022107\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.5168136029681902\n",
      "Test RMSE: 1.607043346954883\n",
      "R2 Train: 0.03541146732912237\n",
      "R2 Test: -0.08833558955964071\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.512684862908996\n",
      "Test RMSE: 1.626703873109431\n",
      "R2 Train: 0.040655506691650856\n",
      "R2 Test: -0.11512781898377367\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.5114776895527733\n",
      "Test RMSE: 1.6034146683274968\n",
      "R2 Train: 0.042186074016989594\n",
      "R2 Test: -0.08342624910871455\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.5098350667373717\n",
      "Test RMSE: 1.6201589739504378\n",
      "R2 Train: 0.04426678231896031\n",
      "R2 Test: -0.10617263419155964\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.5118357912625884\n",
      "Test RMSE: 1.5958620874428655\n",
      "R2 Train: 0.041732166640931156\n",
      "R2 Test: -0.07324373912502802\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.4973040266927462\n",
      "GensimLDA Average Test RMSE: 1.5476411353120643\n",
      "GensimLDA Average R2 Train: 0.06006537125747691\n",
      "GensimLDA Average R2 Test: -0.009364852190689499\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.5110065135633073\n",
      "Mallet_LDA Average Test RMSE: 1.6196265408956971\n",
      "Mallet_LDA Average R2 Train: 0.04276731885323517\n",
      "Mallet_LDA Average R2 Test: -0.1071174401307852\n",
      "\n",
      "BERTopic Average Train RMSE: 1.4874518525361289\n",
      "BERTopic Average Test RMSE: 1.5579421365431316\n",
      "BERTopic Average R2 Train: 0.07239170551611848\n",
      "BERTopic Average R2 Test: -0.02426732235852023\n",
      "\n",
      "NMF Average Train RMSE: 1.4940747212554732\n",
      "NMF Average Test RMSE: 1.5559138711679388\n",
      "NMF Average R2 Train: 0.06411402502348436\n",
      "NMF Average R2 Test: -0.020204027743626486\n",
      "\n",
      "CTM Average Train RMSE: 1.512529402685984\n",
      "CTM Average Test RMSE: 1.6106365899570227\n",
      "CTM Average R2 Train: 0.040850399399530855\n",
      "CTM Average R2 Test: -0.09326120619374331\n",
      "\n",
      "Agreeableness\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1039776999192796\n",
      "Test RMSE: 1.2117220810694698\n",
      "R2 Train: 0.10854248339386241\n",
      "R2 Test: -0.0712705714715105\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1039776999192796\n",
      "Test RMSE: 1.2117220810694698\n",
      "R2 Train: 0.10854248339386241\n",
      "R2 Test: -0.0712705714715105\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1039776999192796\n",
      "Test RMSE: 1.2117220810694698\n",
      "R2 Train: 0.10854248339386241\n",
      "R2 Test: -0.0712705714715105\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1039776999192796\n",
      "Test RMSE: 1.2117220810694698\n",
      "R2 Train: 0.10854248339386241\n",
      "R2 Test: -0.0712705714715105\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1039776999192796\n",
      "Test RMSE: 1.2117220810694698\n",
      "R2 Train: 0.10854248339386241\n",
      "R2 Test: -0.0712705714715105\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1359042560755284\n",
      "Test RMSE: 1.2083400282521641\n",
      "R2 Train: 0.05623579134842582\n",
      "R2 Test: -0.06529884333414726\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.140957378433171\n",
      "Test RMSE: 1.2364195697410632\n",
      "R2 Train: 0.0478203576396703\n",
      "R2 Test: -0.1153851839151252\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.140049276444217\n",
      "Test RMSE: 1.1938785249085233\n",
      "R2 Train: 0.049335457397457816\n",
      "R2 Test: -0.03995227960165448\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1389734236736442\n",
      "Test RMSE: 1.225080517646606\n",
      "R2 Train: 0.051128875475091595\n",
      "R2 Test: -0.09502087217436883\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1388735340055138\n",
      "Test RMSE: 1.2050457570363167\n",
      "R2 Train: 0.05129530300268348\n",
      "R2 Test: -0.05949815899676825\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1175669181226635\n",
      "Test RMSE: 1.1779854626842368\n",
      "R2 Train: 0.086460933050994\n",
      "R2 Test: -0.012448619292549035\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1156472474869152\n",
      "Test RMSE: 1.191659079307476\n",
      "R2 Train: 0.08959665210721846\n",
      "R2 Test: -0.03608928676888734\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1144891771056085\n",
      "Test RMSE: 1.1805741990220906\n",
      "R2 Train: 0.09148571507696635\n",
      "R2 Test: -0.016903415251745546\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.114402762258522\n",
      "Test RMSE: 1.1788446558600811\n",
      "R2 Train: 0.0916265976999916\n",
      "R2 Test: -0.013926067376231588\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1157935874822704\n",
      "Test RMSE: 1.2365867396199188\n",
      "R2 Train: 0.089357800337861\n",
      "R2 Test: -0.11568681519903601\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1180595768410326\n",
      "Test RMSE: 1.1932577502806074\n",
      "R2 Train: 0.08565532189773328\n",
      "R2 Test: -0.03887108392285099\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1218559631012972\n",
      "Test RMSE: 1.1884265625592734\n",
      "R2 Train: 0.0794354407615\n",
      "R2 Test: -0.030475880197424754\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1183695491362797\n",
      "Test RMSE: 1.1988917075506829\n",
      "R2 Train: 0.0851482633966204\n",
      "R2 Test: -0.04870428657536352\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1193104038491903\n",
      "Test RMSE: 1.2009121284963153\n",
      "R2 Train: 0.08360833479495589\n",
      "R2 Test: -0.052241902954206054\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1205156162867331\n",
      "Test RMSE: 1.1937114448298687\n",
      "R2 Train: 0.08163383114925615\n",
      "R2 Test: -0.039661222937589\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1372469421105456\n",
      "Test RMSE: 1.2318497398382136\n",
      "R2 Train: 0.05400333567153859\n",
      "R2 Test: -0.10715545160646611\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1430295343822165\n",
      "Test RMSE: 1.23534042994947\n",
      "R2 Train: 0.04435860447708995\n",
      "R2 Test: -0.11343903021106283\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1458600613968426\n",
      "Test RMSE: 1.2087038387954179\n",
      "R2 Train: 0.039619762806225256\n",
      "R2 Test: -0.06594042647589937\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1422327800477767\n",
      "Test RMSE: 1.23887483992701\n",
      "R2 Train: 0.045690409173953506\n",
      "R2 Test: -0.1198194247991502\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1378802057639799\n",
      "Test RMSE: 1.2150571905853147\n",
      "R2 Train: 0.052949506334889596\n",
      "R2 Test: -0.07717575645744645\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.1039776999192796\n",
      "GensimLDA Average Test RMSE: 1.2117220810694698\n",
      "GensimLDA Average R2 Train: 0.10854248339386241\n",
      "GensimLDA Average R2 Test: -0.0712705714715105\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.1389515737264149\n",
      "Mallet_LDA Average Test RMSE: 1.2137528795169346\n",
      "Mallet_LDA Average R2 Train: 0.0511631569726658\n",
      "Mallet_LDA Average R2 Test: -0.0750310676044128\n",
      "\n",
      "BERTopic Average Train RMSE: 1.1155799384911957\n",
      "BERTopic Average Test RMSE: 1.1931300272987608\n",
      "BERTopic Average R2 Train: 0.08970553965460629\n",
      "BERTopic Average R2 Test: -0.039010840777689904\n",
      "\n",
      "NMF Average Train RMSE: 1.1196222218429066\n",
      "NMF Average Test RMSE: 1.1950399187433496\n",
      "NMF Average R2 Train: 0.08309623840001315\n",
      "NMF Average R2 Test: -0.04199087531748687\n",
      "\n",
      "CTM Average Train RMSE: 1.1412499047402724\n",
      "CTM Average Test RMSE: 1.2259652078190852\n",
      "CTM Average R2 Train: 0.04732432369273938\n",
      "CTM Average R2 Test: -0.096706017910005\n",
      "\n",
      "Conscientiousness\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1793867854726892\n",
      "Test RMSE: 1.231661591962811\n",
      "R2 Train: 0.07481283653122683\n",
      "R2 Test: 0.034232130830839536\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1793867854726892\n",
      "Test RMSE: 1.231661591962811\n",
      "R2 Train: 0.07481283653122683\n",
      "R2 Test: 0.034232130830839536\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1793867854726892\n",
      "Test RMSE: 1.231661591962811\n",
      "R2 Train: 0.07481283653122683\n",
      "R2 Test: 0.034232130830839536\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1793867854726892\n",
      "Test RMSE: 1.231661591962811\n",
      "R2 Train: 0.07481283653122683\n",
      "R2 Test: 0.034232130830839536\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1793867854726892\n",
      "Test RMSE: 1.231661591962811\n",
      "R2 Train: 0.07481283653122683\n",
      "R2 Test: 0.034232130830839536\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1913517788782395\n",
      "Test RMSE: 1.2976556283088243\n",
      "R2 Train: 0.05594538636080493\n",
      "R2 Test: -0.07203475351621202\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1917384490766845\n",
      "Test RMSE: 1.3816285725928046\n",
      "R2 Train: 0.05533247415405085\n",
      "R2 Test: -0.2152694127447341\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1885267060928186\n",
      "Test RMSE: 1.2977849402678607\n",
      "R2 Train: 0.060417383407553205\n",
      "R2 Test: -0.07224842163928691\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1940852260340358\n",
      "Test RMSE: 1.2700545109873993\n",
      "R2 Train: 0.05160832348609645\n",
      "R2 Test: -0.026915426388024688\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1987747027546285\n",
      "Test RMSE: 1.3113975394847197\n",
      "R2 Train: 0.04414454494311004\n",
      "R2 Test: -0.09486023889939643\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1828254088341064\n",
      "Test RMSE: 1.247333488852314\n",
      "R2 Train: 0.06941001527007151\n",
      "R2 Test: 0.009498539211719725\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1913555486033311\n",
      "Test RMSE: 1.2437332771644094\n",
      "R2 Train: 0.05593941191722174\n",
      "R2 Test: 0.015208108667134734\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1856987261171559\n",
      "Test RMSE: 1.2370661373876646\n",
      "R2 Train: 0.06488334893525638\n",
      "R2 Test: 0.025737933973467686\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1871625249116868\n",
      "Test RMSE: 1.258268789565393\n",
      "R2 Train: 0.06257303598108355\n",
      "R2 Test: -0.007944928304204346\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1812223530699377\n",
      "Test RMSE: 1.4149181512594504\n",
      "R2 Train: 0.07193071990732502\n",
      "R2 Test: -0.2745374211042293\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.184957002130998\n",
      "Test RMSE: 1.264649903934394\n",
      "R2 Train: 0.06605292345103186\n",
      "R2 Test: -0.018194122888305353\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.188287879960094\n",
      "Test RMSE: 1.2586428034509674\n",
      "R2 Train: 0.060794950585561724\n",
      "R2 Test: -0.008544230187373048\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1870676166306982\n",
      "Test RMSE: 1.2662123946249928\n",
      "R2 Train: 0.06272291609182212\n",
      "R2 Test: -0.020711660166079948\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.185700453732025\n",
      "Test RMSE: 1.260526105439515\n",
      "R2 Train: 0.06488062392160454\n",
      "R2 Test: -0.01156464928677714\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1861161158749371\n",
      "Test RMSE: 1.2614890735759907\n",
      "R2 Train: 0.06422487338487837\n",
      "R2 Test: -0.01311079196554199\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.19998895717858\n",
      "Test RMSE: 1.3101330882972595\n",
      "R2 Train: 0.04220716749950759\n",
      "R2 Test: -0.09274992548385685\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1974947054987062\n",
      "Test RMSE: 1.3008327215147155\n",
      "R2 Train: 0.04618469337449871\n",
      "R2 Test: -0.0772905759591136\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.2037243623731662\n",
      "Test RMSE: 1.2835574913449614\n",
      "R2 Train: 0.03623492447126586\n",
      "R2 Test: -0.04886744700997059\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1970574776949645\n",
      "Test RMSE: 1.3294838261561237\n",
      "R2 Train: 0.04688107797845542\n",
      "R2 Test: -0.12526826697369442\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1962368090392954\n",
      "Test RMSE: 1.3034845942907534\n",
      "R2 Train: 0.048187492605496596\n",
      "R2 Test: -0.08168737421334482\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.1793867854726892\n",
      "GensimLDA Average Test RMSE: 1.231661591962811\n",
      "GensimLDA Average R2 Train: 0.07481283653122683\n",
      "GensimLDA Average R2 Test: 0.034232130830839536\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.1928953725672813\n",
      "Mallet_LDA Average Test RMSE: 1.3117042383283217\n",
      "Mallet_LDA Average R2 Train: 0.053489622470323094\n",
      "Mallet_LDA Average R2 Test: -0.09626565063753083\n",
      "\n",
      "BERTopic Average Train RMSE: 1.1856529123072437\n",
      "BERTopic Average Test RMSE: 1.2802639688458464\n",
      "BERTopic Average R2 Train: 0.06494730640219164\n",
      "BERTopic Average R2 Test: -0.0464075535112223\n",
      "\n",
      "NMF Average Train RMSE: 1.1864258136657506\n",
      "NMF Average Test RMSE: 1.262304056205172\n",
      "NMF Average R2 Train: 0.06373525748697972\n",
      "NMF Average R2 Test: -0.014425090898815496\n",
      "\n",
      "CTM Average Train RMSE: 1.1989004623569426\n",
      "CTM Average Test RMSE: 1.3054983443207626\n",
      "CTM Average R2 Train: 0.043939071185844836\n",
      "CTM Average R2 Test: -0.08517271792799605\n",
      "\n",
      "Emotional_Stability\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.3392766568444174\n",
      "Test RMSE: 1.4533952293633388\n",
      "R2 Train: 0.09982831256835967\n",
      "R2 Test: 0.003480049967952925\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.3392766568444174\n",
      "Test RMSE: 1.4533952293633388\n",
      "R2 Train: 0.09982831256835967\n",
      "R2 Test: 0.003480049967952925\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.3392766568444174\n",
      "Test RMSE: 1.4533952293633388\n",
      "R2 Train: 0.09982831256835967\n",
      "R2 Test: 0.003480049967952925\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.3392766568444174\n",
      "Test RMSE: 1.4533952293633388\n",
      "R2 Train: 0.09982831256835967\n",
      "R2 Test: 0.003480049967952925\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.3392766568444174\n",
      "Test RMSE: 1.4533952293633388\n",
      "R2 Train: 0.09982831256835967\n",
      "R2 Test: 0.003480049967952925\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.3746182762960235\n",
      "Test RMSE: 1.5170717642066485\n",
      "R2 Train: 0.05169295225578019\n",
      "R2 Test: -0.08575237384790269\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.376809187659744\n",
      "Test RMSE: 1.5306587708854973\n",
      "R2 Train: 0.048667657973736844\n",
      "R2 Test: -0.1052876201871511\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.3847257668581228\n",
      "Test RMSE: 1.5143208139285638\n",
      "R2 Train: 0.037695984952953676\n",
      "R2 Test: -0.08181829136935326\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.3745895610873766\n",
      "Test RMSE: 1.4968760075529923\n",
      "R2 Train: 0.051732571327905874\n",
      "R2 Test: -0.057037005723312495\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.3806026970810288\n",
      "Test RMSE: 1.7424103118690224\n",
      "R2 Train: 0.04341804168337282\n",
      "R2 Test: -0.4322518840206624\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.3397156083247552\n",
      "Test RMSE: 1.4546638245126329\n",
      "R2 Train: 0.09923814854889801\n",
      "R2 Test: 0.0017396670786017365\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.3435231695454843\n",
      "Test RMSE: 1.4662941483140741\n",
      "R2 Train: 0.0941108221223873\n",
      "R2 Test: -0.014286720868105673\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.3421593624083572\n",
      "Test RMSE: 1.4758904550776328\n",
      "R2 Train: 0.09594902052333887\n",
      "R2 Test: -0.02760636365692215\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.3376514892483633\n",
      "Test RMSE: 1.4463048986467972\n",
      "R2 Train: 0.10201164375913396\n",
      "R2 Test: 0.01317929915811944\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.3337151512722292\n",
      "Test RMSE: 1.6955960157970884\n",
      "R2 Train: 0.10728892973293935\n",
      "R2 Test: -0.35632357882626065\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.3425227395183033\n",
      "Test RMSE: 1.4766502325213116\n",
      "R2 Train: 0.09545942754046499\n",
      "R2 Test: -0.02866464423066084\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.3446410374326183\n",
      "Test RMSE: 1.4835653702860747\n",
      "R2 Train: 0.09260271904839568\n",
      "R2 Test: -0.03832165502698737\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.3433441326138615\n",
      "Test RMSE: 1.4714888847126009\n",
      "R2 Train: 0.09435224231665129\n",
      "R2 Test: -0.021486211481346773\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.3433579729782517\n",
      "Test RMSE: 1.4682708093107268\n",
      "R2 Train: 0.09433358059142083\n",
      "R2 Test: -0.01702321459565259\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.3419721154119488\n",
      "Test RMSE: 1.4705160463956235\n",
      "R2 Train: 0.09620125438902716\n",
      "R2 Test: -0.02013599749937045\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.3692278233531086\n",
      "Test RMSE: 1.5365493562457482\n",
      "R2 Train: 0.05911578642434079\n",
      "R2 Test: -0.11381116476633935\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.3777120672159466\n",
      "Test RMSE: 1.4948409656682948\n",
      "R2 Train: 0.04741952545623307\n",
      "R2 Test: -0.054164820824429194\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.3747550566677493\n",
      "Test RMSE: 1.5045811945587035\n",
      "R2 Train: 0.051504221688064566\n",
      "R2 Test: -0.06794723495661481\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.364601916889701\n",
      "Test RMSE: 1.5681103315427223\n",
      "R2 Train: 0.06546256082490298\n",
      "R2 Test: -0.16003680712141\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.3753588616806922\n",
      "Test RMSE: 1.485671329406358\n",
      "R2 Train: 0.050670862651641\n",
      "R2 Test: -0.04127159577798545\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.3392766568444174\n",
      "GensimLDA Average Test RMSE: 1.4533952293633388\n",
      "GensimLDA Average R2 Train: 0.09982831256835967\n",
      "GensimLDA Average R2 Test: 0.003480049967952925\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.3782690977964593\n",
      "Mallet_LDA Average Test RMSE: 1.560267533688545\n",
      "Mallet_LDA Average R2 Train: 0.04664144163874988\n",
      "Mallet_LDA Average R2 Test: -0.15242943502967637\n",
      "\n",
      "BERTopic Average Train RMSE: 1.3393529561598378\n",
      "BERTopic Average Test RMSE: 1.507749868469645\n",
      "BERTopic Average R2 Train: 0.0997197129373395\n",
      "BERTopic Average R2 Test: -0.07665953942291345\n",
      "\n",
      "NMF Average Train RMSE: 1.3431675995909966\n",
      "NMF Average Test RMSE: 1.4740982686452675\n",
      "NMF Average R2 Train: 0.09458984477719198\n",
      "NMF Average R2 Test: -0.025126344566803606\n",
      "\n",
      "CTM Average Train RMSE: 1.3723311451614395\n",
      "CTM Average Test RMSE: 1.5179506354843653\n",
      "CTM Average R2 Train: 0.05483459140903648\n",
      "CTM Average R2 Test: -0.08744632468935576\n",
      "\n",
      "Openness_to_newexp\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.152920547134979\n",
      "Test RMSE: 1.2821430004815038\n",
      "R2 Train: 0.06656676837037889\n",
      "R2 Test: -0.057701196516431796\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.152920547134979\n",
      "Test RMSE: 1.2821430004815038\n",
      "R2 Train: 0.06656676837037889\n",
      "R2 Test: -0.057701196516431796\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.152920547134979\n",
      "Test RMSE: 1.2821430004815038\n",
      "R2 Train: 0.06656676837037889\n",
      "R2 Test: -0.057701196516431796\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.152920547134979\n",
      "Test RMSE: 1.2821430004815038\n",
      "R2 Train: 0.06656676837037889\n",
      "R2 Test: -0.057701196516431796\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.152920547134979\n",
      "Test RMSE: 1.2821430004815038\n",
      "R2 Train: 0.06656676837037889\n",
      "R2 Test: -0.057701196516431796\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1613529497468085\n",
      "Test RMSE: 1.277694775500401\n",
      "R2 Train: 0.0528626684681357\n",
      "R2 Test: -0.050374819688894235\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1648583923190652\n",
      "Test RMSE: 1.3495396003169176\n",
      "R2 Train: 0.04713633644447446\n",
      "R2 Test: -0.17182114321197361\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1686069544532995\n",
      "Test RMSE: 1.2891083920175652\n",
      "R2 Train: 0.04099376000451249\n",
      "R2 Test: -0.0692245832616667\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1633224642337998\n",
      "Test RMSE: 1.339350794914179\n",
      "R2 Train: 0.04964748320806478\n",
      "R2 Test: -0.15419381740889126\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.168381886444453\n",
      "Test RMSE: 1.4449589410277655\n",
      "R2 Train: 0.04136312429600786\n",
      "R2 Test: -0.3433868022452802\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.154141320972325\n",
      "Test RMSE: 1.2469742889181918\n",
      "R2 Train: 0.06458898393693724\n",
      "R2 Test: -0.000472284127290612\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.154227386405023\n",
      "Test RMSE: 1.2763704589772864\n",
      "R2 Train: 0.06444946972994947\n",
      "R2 Test: -0.048198544317218595\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1478044232079112\n",
      "Test RMSE: 1.2666189077408434\n",
      "R2 Train: 0.0748326705572736\n",
      "R2 Test: -0.03224312158068732\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1437922152287783\n",
      "Test RMSE: 1.265393377106352\n",
      "R2 Train: 0.0812893037406015\n",
      "R2 Test: -0.030246572254214232\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1484653899562973\n",
      "Test RMSE: 1.3706814106217762\n",
      "R2 Train: 0.07376684279503487\n",
      "R2 Test: -0.20882409983817984\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1539368854460095\n",
      "Test RMSE: 1.2671399974127007\n",
      "R2 Train: 0.06492033730470159\n",
      "R2 Test: -0.03309263025453224\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.1551726266077043\n",
      "Test RMSE: 1.2786250529066852\n",
      "R2 Train: 0.0629165272877441\n",
      "R2 Test: -0.05190491232883243\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.151139723073913\n",
      "Test RMSE: 1.2708464409767406\n",
      "R2 Train: 0.06944814049393488\n",
      "R2 Test: -0.039145157497946004\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.1529235213093831\n",
      "Test RMSE: 1.2653006770347264\n",
      "R2 Train: 0.06656195243260588\n",
      "R2 Test: -0.03009563036601004\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.152994607446938\n",
      "Test RMSE: 1.2664009230803908\n",
      "R2 Train: 0.06644684236703557\n",
      "R2 Test: -0.0318878548107937\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.1713818943393748\n",
      "Test RMSE: 1.2915595044237338\n",
      "R2 Train: 0.03643389624222204\n",
      "R2 Test: -0.07329449893936535\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.163861882736748\n",
      "Test RMSE: 1.3129422996320677\n",
      "R2 Train: 0.04876594502825338\n",
      "R2 Test: -0.10912717168362063\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.1652449678373056\n",
      "Test RMSE: 1.2843986812373014\n",
      "R2 Train: 0.04650378764917318\n",
      "R2 Test: -0.06142610854329722\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.163629413459307\n",
      "Test RMSE: 1.2992244892403857\n",
      "R2 Train: 0.04914590524903384\n",
      "R2 Test: -0.08607160701245697\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.1659861401149598\n",
      "Test RMSE: 1.299020562608421\n",
      "R2 Train: 0.04529042954109508\n",
      "R2 Test: -0.0857306935752058\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.152920547134979\n",
      "GensimLDA Average Test RMSE: 1.2821430004815038\n",
      "GensimLDA Average R2 Train: 0.06656676837037889\n",
      "GensimLDA Average R2 Test: -0.057701196516431796\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.1653045294394853\n",
      "Mallet_LDA Average Test RMSE: 1.3401305007553657\n",
      "Mallet_LDA Average R2 Train: 0.046400674484239054\n",
      "Mallet_LDA Average R2 Test: -0.15780023316334119\n",
      "\n",
      "BERTopic Average Train RMSE: 1.1496861471540671\n",
      "BERTopic Average Test RMSE: 1.2852076886728898\n",
      "BERTopic Average R2 Train: 0.07178545415195933\n",
      "BERTopic Average R2 Test: -0.06399692442351812\n",
      "\n",
      "NMF Average Train RMSE: 1.1532334727767894\n",
      "NMF Average Test RMSE: 1.2696626182822486\n",
      "NMF Average R2 Train: 0.06605875997720441\n",
      "NMF Average R2 Test: -0.03722523705162288\n",
      "\n",
      "CTM Average Train RMSE: 1.166020859697539\n",
      "CTM Average Test RMSE: 1.297429107428382\n",
      "CTM Average R2 Train: 0.045227992741955506\n",
      "CTM Average R2 Test: -0.0831300159507892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# facebook_2020\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# change this\n",
    "dataset = 'facebook_2020' # folder and dataset name\n",
    "\n",
    "cols = ['age', 'UCLA_3item_sum', 'stress', 'depression',\n",
    "        'Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness_to_newexp'] # outcome columns\n",
    "\n",
    "for outcome in cols:\n",
    "    print(outcome)\n",
    "    print()\n",
    "\n",
    "    train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "    test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "    temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "    df = pd.read_csv(dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "    df['isFemale'] = df['sex'] == 'Female'\n",
    "\n",
    "    def func(x):\n",
    "        education = x['education']\n",
    "        bachelor_plus = [\"Master's degree or equivalent\", \"Professional degree (MD, JD, etc.), doctorate degree or higher\"]\n",
    "        bachelor_minus = [\"High school diploma or equivalent\", \"Vocational/Technical Training\"]\n",
    "\n",
    "        if education in bachelor_plus:\n",
    "            return 'bachelor+'\n",
    "        elif education in bachelor_minus:\n",
    "            return \"bachelor-\"\n",
    "        else:\n",
    "            return 'bachelor'\n",
    "\n",
    "    df['education_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['education_bucket'] = label_encoder.fit_transform(df['education_bucket'])\n",
    "\n",
    "    def func(x):\n",
    "        income = x['income']\n",
    "        under = [\"$10,000 - $19,999\", \"$20,000 - $34,999\", \"$35,000 - $49,999\"]\n",
    "        over = [\"$100,000 - $149,999\", \"More than $150,000\"]\n",
    "\n",
    "        if income in under:\n",
    "            return '<50k+'\n",
    "        elif income in over:\n",
    "            return \"100k+\"\n",
    "        else:\n",
    "            return '$50,000 - $99,999'\n",
    "\n",
    "    df['income_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['income_bucket'] = label_encoder.fit_transform(df['income_bucket'])\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['marital_status'] = label_encoder.fit_transform(df['marital_status'])\n",
    "\n",
    "    merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "    merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "    user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "    all_user_ids = list(user_ids.keys())\n",
    "\n",
    "    labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "    user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "    y = [] # the outcome we care about\n",
    "\n",
    "    for key in user_ids.keys():\n",
    "        y.append(labels_data[labels_data['user_id'] == key][outcome].iloc[0])\n",
    "\n",
    "    # REGRESSION\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for model in ['GensimLDA', 'Mallet_LDA', 'BERTopic', 'NMF', 'CTM']:\n",
    "        print(model)\n",
    "        \n",
    "        train_error_list = []\n",
    "        test_error_list = []\n",
    "\n",
    "        r2s_train = []\n",
    "        r2s_test = []\n",
    "\n",
    "        for run in range(1, 6):\n",
    "            print(\"Run: \" + str(run))\n",
    "            print()\n",
    "\n",
    "            topics = []\n",
    "            with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    topic_list = [item.strip() for item in row if item.strip()]\n",
    "                    topics.append(topic_list)\n",
    "\n",
    "            X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "            # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "            X_train = X[:round(0.80 * len(X))]\n",
    "            X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "            y_train = y[:round(0.80 * len(X))]\n",
    "            y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            X_test = np.array(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            lr = LinearRegression().fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_train = lr.predict(X_train)\n",
    "            y_pred_test = lr.predict(X_test)\n",
    "\n",
    "            train_error = np.sqrt(mean_squared_error(y_pred_train, y_train))\n",
    "            test_error = np.sqrt(mean_squared_error(y_pred_test, y_test))\n",
    "\n",
    "            train_error_list.append(train_error)\n",
    "            test_error_list.append(test_error)\n",
    "\n",
    "            print(f'Train RMSE: {train_error}')\n",
    "            print(f'Test RMSE: {test_error}')\n",
    "\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "            r2s_train.append(r2_train)\n",
    "            r2s_test.append(r2_test)\n",
    "            \n",
    "            print(f'R2 Train: {r2_train}')\n",
    "            print(f'R2 Test: {r2_test}')\n",
    "\n",
    "            all_scores[model] = {\n",
    "                'Train RMSE': train_error_list,\n",
    "                'Test RMSE': test_error_list,\n",
    "                'R2 Train': r2s_train,\n",
    "                'R2 Test': r2s_test,\n",
    "\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': y_pred_train,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_pred_test\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    for m in all_scores.keys():\n",
    "        print(f'{m} Average Train RMSE: {np.mean(all_scores[m][\"Train RMSE\"])}')\n",
    "        print(f'{m} Average Test RMSE: {np.mean(all_scores[m][\"Test RMSE\"])}')\n",
    "        print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "        print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "        print()\n",
    "\n",
    "    with open('all_results/' + dataset + '_all_scores_' + outcome + '.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital_status\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# facebook_2020\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# change this\n",
    "dataset = 'facebook_2020' # folder and dataset name\n",
    "\n",
    "cols = ['marital_status', 'isFemale', 'education_bucket', 'income_bucket']\n",
    "\n",
    "for outcome in cols:\n",
    "    print(outcome)\n",
    "    print()\n",
    "\n",
    "    train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "    test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "    temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "    df = pd.read_csv(dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "    df['isFemale'] = df['sex'] == 'Female'\n",
    "\n",
    "    def func(x):\n",
    "        education = x['education']\n",
    "        bachelor_plus = [\"Master's degree or equivalent\", \"Professional degree (MD, JD, etc.), doctorate degree or higher\"]\n",
    "        bachelor_minus = [\"High school diploma or equivalent\", \"Vocational/Technical Training\"]\n",
    "\n",
    "        if education in bachelor_plus:\n",
    "            return 'bachelor+'\n",
    "        elif education in bachelor_minus:\n",
    "            return \"bachelor-\"\n",
    "        else:\n",
    "            return 'bachelor'\n",
    "\n",
    "    df['education_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['education_bucket'] = label_encoder.fit_transform(df['education_bucket'])\n",
    "\n",
    "    def func(x):\n",
    "        income = x['income']\n",
    "        under = [\"$10,000 - $19,999\", \"$20,000 - $34,999\", \"$35,000 - $49,999\"]\n",
    "        over = [\"$100,000 - $149,999\", \"More than $150,000\"]\n",
    "\n",
    "        if income in under:\n",
    "            return '<50k+'\n",
    "        elif income in over:\n",
    "            return \"100k+\"\n",
    "        else:\n",
    "            return '$50,000 - $99,999'\n",
    "\n",
    "    df['income_bucket'] = df.apply(lambda x : func(x), axis = 1)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['income_bucket'] = label_encoder.fit_transform(df['income_bucket'])\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['marital_status'] = label_encoder.fit_transform(df['marital_status'])\n",
    "\n",
    "    merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "    merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "    user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "    all_user_ids = list(user_ids.keys())\n",
    "\n",
    "    labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "    user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "    y = [] # the outcome we care about\n",
    "\n",
    "    for key in user_ids.keys():\n",
    "        y.append(labels_data[labels_data['user_id'] == key][outcome].iloc[0])\n",
    "\n",
    "    num_epochs = 500\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    lrs = {\n",
    "        'education_bucket': {\n",
    "            'GensimLDA': 0.05,\n",
    "            'Mallet_LDA': 0.01,\n",
    "            'CTM': 0.05,\n",
    "            'BERTopic': 0.5,\n",
    "            'NMF': 0.001\n",
    "        },\n",
    "\n",
    "        'income_bucket': {\n",
    "            'GensimLDA': 0.05,\n",
    "            'Mallet_LDA': 0.01,\n",
    "            'CTM': 0.2,\n",
    "            'BERTopic': 0.5,\n",
    "            'NMF': 0.001\n",
    "        },\n",
    "\n",
    "        'isFemale': {\n",
    "            'GensimLDA': 0.05,\n",
    "            'Mallet_LDA': 0.1,\n",
    "            'CTM': 0.5,\n",
    "            'BERTopic': 0.5,\n",
    "            'NMF': 0.001\n",
    "        },\n",
    "\n",
    "        'marital_status': {\n",
    "            'GensimLDA': 0.05,\n",
    "            'Mallet_LDA': 0.1,\n",
    "            'CTM': 0.08,\n",
    "            'BERTopic': 0.5,\n",
    "            'NMF': 0.001\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for model in ['GensimLDA', 'Mallet_LDA', 'BERTopic', 'NMF', 'CTM']:\n",
    "        print(model)\n",
    "\n",
    "        train_error_list = []\n",
    "        test_error_list = []\n",
    "\n",
    "        r2s_train = []\n",
    "        r2s_test = []\n",
    "\n",
    "        lr = lrs[outcome][model]\n",
    "\n",
    "        for run in range(1, 6):\n",
    "            print(\"Run: \" + str(run))\n",
    "            print()\n",
    "\n",
    "            topics = []\n",
    "            with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    topic_list = [item.strip() for item in row if item.strip()]\n",
    "                    topics.append(topic_list)\n",
    "\n",
    "            X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "            # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "            X_train = X[:round(0.80 * len(X))]\n",
    "            X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "            y_train = y[:round(0.80 * len(X))]\n",
    "            y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            X_test = np.array(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            train_users = all_user_ids[:round(0.80 * len(X))]\n",
    "            test_users = all_user_ids[round(0.80 * len(X)):]\n",
    "\n",
    "            # Convert arrays to torch tensors\n",
    "            X_train_tensor = torch.tensor(np.array(X_train).astype(np.float32))\n",
    "            y_train_tensor = torch.tensor(np.array(y_train).astype(np.longlong))  # Use long for classification\n",
    "            X_test_tensor = torch.tensor(np.array(X_test).astype(np.float32))\n",
    "            y_test_tensor = torch.tensor(np.array(y_test).astype(np.longlong))\n",
    "\n",
    "            # Create datasets and dataloaders\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=False)\n",
    "            test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "            # Define the model\n",
    "            class LogisticRegressionModel(nn.Module):\n",
    "                def __init__(self, input_size, num_classes):\n",
    "                    super(LogisticRegressionModel, self).__init__()\n",
    "                    self.layer1 = nn.Linear(input_size, num_classes)\n",
    "\n",
    "                def forward(self, x):\n",
    "                    return self.layer1(x)\n",
    "\n",
    "            input_size = X_train.shape[1]\n",
    "            num_classes = len(np.unique(y_train))  # Assuming y_train contains all classes\n",
    "            logit_model = LogisticRegressionModel(input_size, num_classes)\n",
    "\n",
    "            # Loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()  # This includes softmax\n",
    "            optimizer = optim.Adam(logit_model.parameters(), lr=lr)\n",
    "\n",
    "            loss_values = []\n",
    "            early_stopping_triggered = False\n",
    "\n",
    "            # Train the model\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_loss = 0\n",
    "                for inputs, targets in train_loader:\n",
    "                    # Forward pass\n",
    "                    outputs = logit_model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    # Backward and optimize\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * inputs.size(0) \n",
    "                \n",
    "                epoch_loss /= len(train_loader.dataset)\n",
    "                loss_values.append(epoch_loss)\n",
    "\n",
    "                if epoch >= 50:\n",
    "                    # Calculate the percentage change in loss\n",
    "                    loss_change = (loss_values[epoch - 50] - loss_values[epoch]) / loss_values[epoch - 50]\n",
    "                    \n",
    "                    # If change in loss is less than 0.5%, stop training\n",
    "                    if abs(loss_change) < 0.005:\n",
    "                        print(f'Early stopping at epoch {epoch+1} due to minimal loss improvement.')\n",
    "                        early_stopping_triggered = True\n",
    "                        break\n",
    "\n",
    "                if (epoch+1) % 10 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Predict on the test set\n",
    "            logit_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred_train = logit_model(X_train_tensor)\n",
    "                _, predicted_train = torch.max(y_pred_train.data, 1)\n",
    "\n",
    "                y_pred_test = logit_model(X_test_tensor)\n",
    "                _, predicted_test = torch.max(y_pred_test.data, 1)\n",
    "\n",
    "\n",
    "            train_error = 1 - accuracy_score(predicted_train, y_train)\n",
    "            test_error = 1 - accuracy_score(predicted_test, y_test)\n",
    "\n",
    "            train_error_list.append(train_error)\n",
    "            test_error_list.append(test_error)\n",
    "            \n",
    "            print(f'Train Error: {train_error}')\n",
    "            print(f'Test Error: {test_error}')\n",
    "\n",
    "            print()\n",
    "\n",
    "            r2_train = r2_score(y_train, predicted_train)\n",
    "            r2_test = r2_score(y_test, predicted_test)\n",
    "\n",
    "            r2s_train.append(r2_train)\n",
    "            r2s_test.append(r2_test)\n",
    "            \n",
    "            print(f'R2 Train: {r2_train}')\n",
    "            print(f'R2 Test: {r2_test}')\n",
    "\n",
    "            all_scores[model] = {\n",
    "                'Train Error': train_error_list,\n",
    "                'Test Error': test_error_list,\n",
    "                'R2 Train': r2s_train,\n",
    "                'R2 Test': r2s_test,\n",
    "\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': predicted_train.numpy(),\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': predicted_test.numpy()\n",
    "            }\n",
    "\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    for m in all_scores.keys():\n",
    "        print(f'{m} Average Train Error: {np.mean(all_scores[m][\"Train Error\"])}')\n",
    "        print(f'{m} Average Test Error: {np.mean(all_scores[m][\"Test Error\"])}')\n",
    "        print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "        print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "        print()\n",
    "\n",
    "    with open('all_results/' + dataset + '_all_scores_' + outcome + '.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression: age, loneliness, stress, depression, personality ones\n",
    "\n",
    "# Classification: sex (2), marital_status (3), education (3), income (3)\n",
    "\n",
    "# education\n",
    "    ## Bachelors\n",
    "        ### Bachelors\n",
    "\n",
    "    ## Bachelors or higher\n",
    "        ### Masters\n",
    "        ### Professional degree\n",
    "\n",
    "    ## Bachelors or lower\n",
    "        ### High school\n",
    "        ### Vocational\n",
    "\n",
    "# marital_status\n",
    "    ## Married\n",
    "    ## Never married\n",
    "    ## Divorced\n",
    "\n",
    "# isFemale\n",
    "    ## Female\n",
    "    ## Not Female\n",
    "\n",
    "# income\n",
    "    ## <50k\n",
    "    ## 50 - 100k\n",
    "    ## 100k+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
