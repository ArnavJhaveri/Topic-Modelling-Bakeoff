{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Arnav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "GensimLDA\n",
      "Run: 1\n",
      "Epoch [10/500], Loss: 1.4884\n",
      "Epoch [20/500], Loss: 1.4980\n",
      "Epoch [30/500], Loss: 1.5002\n",
      "Epoch [40/500], Loss: 1.5008\n",
      "Epoch [50/500], Loss: 1.5010\n",
      "Early stopping at epoch 54 due to minimal loss improvement.\n",
      "Train Error: 0.5963125\n",
      "Test Error: 0.595125\n",
      "\n",
      "R2 Train: -0.12070727398995484\n",
      "R2 Test: -0.10994996773350585\n",
      "Run: 2\n",
      "Epoch [10/500], Loss: 1.4884\n",
      "Epoch [20/500], Loss: 1.4980\n",
      "Epoch [30/500], Loss: 1.5002\n",
      "Epoch [40/500], Loss: 1.5008\n",
      "Epoch [50/500], Loss: 1.5010\n",
      "Early stopping at epoch 54 due to minimal loss improvement.\n",
      "Train Error: 0.5963125\n",
      "Test Error: 0.595125\n",
      "\n",
      "R2 Train: -0.12070727398995484\n",
      "R2 Test: -0.10994996773350585\n",
      "Run: 3\n",
      "Epoch [10/500], Loss: 1.4884\n",
      "Epoch [20/500], Loss: 1.4980\n",
      "Epoch [30/500], Loss: 1.5002\n",
      "Epoch [40/500], Loss: 1.5008\n",
      "Epoch [50/500], Loss: 1.5010\n",
      "Early stopping at epoch 54 due to minimal loss improvement.\n",
      "Train Error: 0.5963125\n",
      "Test Error: 0.595125\n",
      "\n",
      "R2 Train: -0.12070727398995484\n",
      "R2 Test: -0.10994996773350585\n",
      "Run: 4\n",
      "Epoch [10/500], Loss: 1.4885\n",
      "Epoch [20/500], Loss: 1.4980\n",
      "Epoch [30/500], Loss: 1.5002\n",
      "Epoch [40/500], Loss: 1.5008\n",
      "Epoch [50/500], Loss: 1.5010\n",
      "Early stopping at epoch 54 due to minimal loss improvement.\n",
      "Train Error: 0.5963125\n",
      "Test Error: 0.595125\n",
      "\n",
      "R2 Train: -0.12070727398995484\n",
      "R2 Test: -0.10994996773350585\n",
      "Run: 5\n",
      "Epoch [10/500], Loss: 1.4884\n",
      "Epoch [20/500], Loss: 1.4980\n",
      "Epoch [30/500], Loss: 1.5002\n",
      "Epoch [40/500], Loss: 1.5008\n",
      "Epoch [50/500], Loss: 1.5010\n",
      "Early stopping at epoch 54 due to minimal loss improvement.\n",
      "Train Error: 0.5963125\n",
      "Test Error: 0.595125\n",
      "\n",
      "R2 Train: -0.12070727398995484\n",
      "R2 Test: -0.10994996773350585\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "Epoch [10/500], Loss: 1.4296\n",
      "Epoch [20/500], Loss: 1.4155\n",
      "Epoch [30/500], Loss: 1.4078\n",
      "Epoch [40/500], Loss: 1.4038\n",
      "Epoch [50/500], Loss: 1.4019\n",
      "Epoch [60/500], Loss: 1.4010\n",
      "Epoch [70/500], Loss: 1.4006\n",
      "Early stopping at epoch 76 due to minimal loss improvement.\n",
      "Train Error: 0.5824\n",
      "Test Error: 0.5796250000000001\n",
      "\n",
      "R2 Train: 0.019773524424703925\n",
      "R2 Test: 0.02053736671705264\n",
      "Run: 2\n",
      "Epoch [10/500], Loss: 1.4572\n",
      "Epoch [20/500], Loss: 1.4474\n",
      "Epoch [30/500], Loss: 1.4391\n",
      "Epoch [40/500], Loss: 1.4326\n",
      "Epoch [50/500], Loss: 1.4277\n",
      "Epoch [60/500], Loss: 1.4239\n",
      "Epoch [70/500], Loss: 1.4211\n",
      "Early stopping at epoch 74 due to minimal loss improvement.\n",
      "Train Error: 0.5916250000000001\n",
      "Test Error: 0.5910500000000001\n",
      "\n",
      "R2 Train: -0.017467063293872487\n",
      "R2 Test: -0.022825878115593445\n",
      "Run: 3\n",
      "Epoch [10/500], Loss: 1.4885\n",
      "Epoch [20/500], Loss: 1.4751\n",
      "Epoch [30/500], Loss: 1.4677\n",
      "Epoch [40/500], Loss: 1.4635\n",
      "Epoch [50/500], Loss: 1.4614\n",
      "Epoch [60/500], Loss: 1.4604\n",
      "Epoch [70/500], Loss: 1.4601\n",
      "Early stopping at epoch 76 due to minimal loss improvement.\n",
      "Train Error: 0.59281875\n",
      "Test Error: 0.587125\n",
      "\n",
      "R2 Train: -0.019351157235093064\n",
      "R2 Test: -0.004710230274333371\n",
      "Run: 4\n",
      "Epoch [10/500], Loss: 1.4132\n",
      "Epoch [20/500], Loss: 1.4089\n",
      "Epoch [30/500], Loss: 1.4080\n",
      "Epoch [40/500], Loss: 1.4077\n",
      "Epoch [50/500], Loss: 1.4076\n",
      "Epoch [60/500], Loss: 1.4076\n",
      "Epoch [70/500], Loss: 1.4075\n",
      "Early stopping at epoch 73 due to minimal loss improvement.\n",
      "Train Error: 0.59461875\n",
      "Test Error: 0.590875\n",
      "\n",
      "R2 Train: -0.02908772908083579\n",
      "R2 Test: -0.01954567850647626\n",
      "Run: 5\n",
      "Epoch [10/500], Loss: 1.4567\n",
      "Epoch [20/500], Loss: 1.4475\n",
      "Epoch [30/500], Loss: 1.4493\n",
      "Epoch [40/500], Loss: 1.4536\n",
      "Epoch [50/500], Loss: 1.4585\n",
      "Epoch [60/500], Loss: 1.4632\n",
      "Epoch [70/500], Loss: 1.4674\n",
      "Early stopping at epoch 72 due to minimal loss improvement.\n",
      "Train Error: 0.5831999999999999\n",
      "Test Error: 0.5846\n",
      "\n",
      "R2 Train: 0.018590488694170126\n",
      "R2 Test: 0.017157767119780454\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "Epoch [10/500], Loss: 1.3991\n",
      "Epoch [20/500], Loss: 1.3929\n",
      "Epoch [30/500], Loss: 1.3900\n",
      "Epoch [40/500], Loss: 1.3881\n",
      "Epoch [50/500], Loss: 1.3867\n",
      "Early stopping at epoch 56 due to minimal loss improvement.\n",
      "Train Error: 0.59158125\n",
      "Test Error: 0.591375\n",
      "\n",
      "R2 Train: -0.0735360715280351\n",
      "R2 Test: -0.053788974425895475\n",
      "Run: 2\n",
      "Epoch [10/500], Loss: 1.4241\n",
      "Epoch [20/500], Loss: 1.4185\n",
      "Epoch [30/500], Loss: 1.4170\n",
      "Epoch [40/500], Loss: 1.4167\n",
      "Epoch [50/500], Loss: 1.4167\n",
      "Early stopping at epoch 56 due to minimal loss improvement.\n",
      "Train Error: 0.59128125\n",
      "Test Error: 0.591125\n",
      "\n",
      "R2 Train: -0.06852851288027284\n",
      "R2 Test: -0.04967629991597966\n",
      "Run: 3\n",
      "Epoch [10/500], Loss: 1.4054\n",
      "Epoch [20/500], Loss: 1.3960\n",
      "Epoch [30/500], Loss: 1.3920\n",
      "Epoch [40/500], Loss: 1.3900\n",
      "Epoch [50/500], Loss: 1.3889\n",
      "Early stopping at epoch 56 due to minimal loss improvement.\n",
      "Train Error: 0.59068125\n",
      "Test Error: 0.5905\n",
      "\n",
      "R2 Train: -0.06701998583763435\n",
      "R2 Test: -0.045315125435676284\n",
      "Run: 4\n",
      "Epoch [10/500], Loss: 1.4135\n",
      "Epoch [20/500], Loss: 1.4061\n",
      "Epoch [30/500], Loss: 1.4044\n",
      "Epoch [40/500], Loss: 1.4040\n",
      "Epoch [50/500], Loss: 1.4039\n",
      "Early stopping at epoch 57 due to minimal loss improvement.\n",
      "Train Error: 0.5914375000000001\n",
      "Test Error: 0.590325\n",
      "\n",
      "R2 Train: -0.07154243724139464\n",
      "R2 Test: -0.04828470008180874\n",
      "Run: 5\n",
      "Epoch [10/500], Loss: 1.4043\n",
      "Epoch [20/500], Loss: 1.4031\n",
      "Epoch [30/500], Loss: 1.4030\n",
      "Epoch [40/500], Loss: 1.4031\n",
      "Epoch [50/500], Loss: 1.4033\n",
      "Early stopping at epoch 56 due to minimal loss improvement.\n",
      "Train Error: 0.591675\n",
      "Test Error: 0.5910500000000001\n",
      "\n",
      "R2 Train: -0.07212769565835186\n",
      "R2 Test: -0.055863949178632355\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "Epoch [10/500], Loss: 1.5669\n",
      "Epoch [20/500], Loss: 1.5821\n",
      "Epoch [30/500], Loss: 1.5841\n",
      "Epoch [40/500], Loss: 1.5841\n",
      "Epoch [50/500], Loss: 1.5838\n",
      "Early stopping at epoch 59 due to minimal loss improvement.\n",
      "Train Error: 0.5852375000000001\n",
      "Test Error: 0.582425\n",
      "\n",
      "R2 Train: 0.03861759356106409\n",
      "R2 Test: 0.061415611845822116\n",
      "Run: 2\n",
      "Epoch [10/500], Loss: 1.5071\n",
      "Epoch [20/500], Loss: 1.5086\n",
      "Epoch [30/500], Loss: 1.5065\n",
      "Epoch [40/500], Loss: 1.5051\n",
      "Epoch [50/500], Loss: 1.5043\n",
      "Early stopping at epoch 58 due to minimal loss improvement.\n",
      "Train Error: 0.5887687500000001\n",
      "Test Error: 0.5845750000000001\n",
      "\n",
      "R2 Train: 0.021607542779446876\n",
      "R2 Test: 0.037522339693048856\n",
      "Run: 3\n",
      "Epoch [10/500], Loss: 1.4767\n",
      "Epoch [20/500], Loss: 1.4736\n",
      "Epoch [30/500], Loss: 1.4705\n",
      "Epoch [40/500], Loss: 1.4687\n",
      "Epoch [50/500], Loss: 1.4677\n",
      "Early stopping at epoch 59 due to minimal loss improvement.\n",
      "Train Error: 0.5782499999999999\n",
      "Test Error: 0.5768249999999999\n",
      "\n",
      "R2 Train: 0.039124608874150035\n",
      "R2 Test: 0.038392089589405654\n",
      "Run: 4\n",
      "Epoch [10/500], Loss: 1.5076\n",
      "Epoch [20/500], Loss: 1.5095\n",
      "Epoch [30/500], Loss: 1.5076\n",
      "Epoch [40/500], Loss: 1.5063\n",
      "Epoch [50/500], Loss: 1.5055\n",
      "Early stopping at epoch 58 due to minimal loss improvement.\n",
      "Train Error: 0.5886312499999999\n",
      "Test Error: 0.585025\n",
      "\n",
      "R2 Train: 0.021322737881355414\n",
      "R2 Test: 0.03750991469452958\n",
      "Run: 5\n",
      "Epoch [10/500], Loss: 1.4848\n",
      "Epoch [20/500], Loss: 1.4841\n",
      "Epoch [30/500], Loss: 1.4828\n",
      "Epoch [40/500], Loss: 1.4822\n",
      "Epoch [50/500], Loss: 1.4820\n",
      "Early stopping at epoch 59 due to minimal loss improvement.\n",
      "Train Error: 0.5833625\n",
      "Test Error: 0.580525\n",
      "\n",
      "R2 Train: 0.03950956494519675\n",
      "R2 Test: 0.05153773802291273\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "Epoch [10/500], Loss: 1.2728\n",
      "Epoch [20/500], Loss: 1.2757\n",
      "Epoch [30/500], Loss: 1.2773\n",
      "Epoch [40/500], Loss: 1.2782\n",
      "Epoch [50/500], Loss: 1.2788\n",
      "Early stopping at epoch 57 due to minimal loss improvement.\n",
      "Train Error: 0.4979122129292778\n",
      "Test Error: 0.4921361238216688\n",
      "\n",
      "R2 Train: 0.4344187703840593\n",
      "R2 Test: 0.4425425283350305\n",
      "Run: 2\n",
      "Epoch [10/500], Loss: 1.2604\n",
      "Epoch [20/500], Loss: 1.2608\n",
      "Epoch [30/500], Loss: 1.2614\n",
      "Epoch [40/500], Loss: 1.2620\n",
      "Epoch [50/500], Loss: 1.2625\n",
      "Early stopping at epoch 57 due to minimal loss improvement.\n",
      "Train Error: 0.49615572141169406\n",
      "Test Error: 0.49041082189383145\n",
      "\n",
      "R2 Train: 0.43793095844930774\n",
      "R2 Test: 0.4480477595077885\n",
      "Run: 3\n",
      "Epoch [10/500], Loss: 1.3023\n",
      "Epoch [20/500], Loss: 1.2890\n",
      "Epoch [30/500], Loss: 1.2827\n",
      "Epoch [40/500], Loss: 1.2794\n",
      "Epoch [50/500], Loss: 1.2773\n",
      "Early stopping at epoch 56 due to minimal loss improvement.\n",
      "Train Error: 0.4997999724962182\n",
      "Test Error: 0.4923361588277949\n",
      "\n",
      "R2 Train: 0.4410268176262513\n",
      "R2 Test: 0.4536648351061783\n",
      "Run: 4\n",
      "Epoch [10/500], Loss: 1.3512\n",
      "Epoch [20/500], Loss: 1.3353\n",
      "Epoch [30/500], Loss: 1.3283\n",
      "Epoch [40/500], Loss: 1.3255\n",
      "Epoch [50/500], Loss: 1.3243\n",
      "Early stopping at epoch 55 due to minimal loss improvement.\n",
      "Train Error: 0.49976871819875235\n",
      "Test Error: 0.49453654389518165\n",
      "\n",
      "R2 Train: 0.44502733130484984\n",
      "R2 Test: 0.45626211121251337\n",
      "Run: 5\n",
      "Epoch [10/500], Loss: 1.2037\n",
      "Epoch [20/500], Loss: 1.2074\n",
      "Epoch [30/500], Loss: 1.2091\n",
      "Epoch [40/500], Loss: 1.2100\n",
      "Epoch [50/500], Loss: 1.2105\n",
      "Early stopping at epoch 56 due to minimal loss improvement.\n",
      "Train Error: 0.49779344659890734\n",
      "Test Error: 0.49601180206536144\n",
      "\n",
      "R2 Train: 0.4264553065355816\n",
      "R2 Test: 0.43956001031818637\n",
      "\n",
      "\n",
      "GensimLDA Average Train Error: 0.5963125\n",
      "GensimLDA Average Test Error: 0.595125\n",
      "GensimLDA Average R2 Train: -0.12070727398995484\n",
      "GensimLDA Average R2 Test: -0.10994996773350585\n",
      "\n",
      "BERTopic Average Train Error: 0.5889325\n",
      "BERTopic Average Test Error: 0.586655\n",
      "BERTopic Average R2 Train: -0.005508387298185457\n",
      "BERTopic Average R2 Test: -0.0018773306119139964\n",
      "\n",
      "NMF Average Train Error: 0.59133125\n",
      "NMF Average Test Error: 0.590875\n",
      "NMF Average R2 Train: -0.07055094062913776\n",
      "NMF Average R2 Test: -0.050585809807598504\n",
      "\n",
      "Mallet_LDA Average Train Error: 0.58485\n",
      "Mallet_LDA Average Test Error: 0.5818749999999999\n",
      "Mallet_LDA Average R2 Train: 0.03203640960824263\n",
      "Mallet_LDA Average R2 Test: 0.04527553876914379\n",
      "\n",
      "CTM Average Train Error: 0.49828601432696995\n",
      "CTM Average Test Error: 0.49308629010076765\n",
      "CTM Average R2 Train: 0.43697183686000995\n",
      "CTM Average R2 Test: 0.44801544889593936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# amazon\n",
    "\n",
    "\"\"\"\n",
    "1. Dataset passed in must be a CSV file that can be opened using pd.read_csv()\n",
    "2. perc determines how much of the dataset is used\n",
    "3. num_topics_word is set to 500 top words\n",
    "4. num_topics_list is set to [100] but can be changed or appended to, to get more results\n",
    "5. embedding_model is set to paraphrase-distilroberta-base-v2 (but can be changed to other embedding models)\n",
    "6. num_epochs_ctm is set to 25 epochs\n",
    "7. text_column, label_column, and id_column must all be set according to your database\n",
    "  7.1 text_column is the actual text\n",
    "  7.2 label_column is what you regress over / predict\n",
    "  7.3 id_column is the unique identifier for each message\n",
    "8. dataset_path must be set to where the dataset is stored\n",
    "9. the name of the folder (be sure to include '/' at the end) -- usually the name of the dataset (i.e. twitter, amazon, etc.)\n",
    "10. logit should be set to True if target variable is discrete and False if continous\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import errno\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.sparse\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.utils import deaccent\n",
    "import abc\n",
    "import re\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "is_multi = False\n",
    "is_combined = not is_multi\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "language = 'en'\n",
    "stopwords = list(stop_words.words('english'))\n",
    "\n",
    "num_topics_word = 500\n",
    "num_topics_list = [100]\n",
    "embedding_model = 'paraphrase-distilroberta-base-v2'\n",
    "num_epochs_ctm = 25\n",
    "\n",
    "# # change these accordingly\n",
    "text_column = 'text'\n",
    "label_column = 'label'\n",
    "id_column = 'Unnamed: 0'\n",
    "dataset_path = 'amazon.csv'\n",
    "folder = 'amazon/'\n",
    "\n",
    "dataset = pd.read_csv(dataset_path)[[id_column, text_column, label_column]].rename(columns = {id_column: 'message_id', text_column: 'message', label_column: 'label'}).dropna()\n",
    "\n",
    "# shuffles to avoid biases in data\n",
    "arr = sklearn.utils.shuffle(np.arange(len(dataset)), random_state=42)\n",
    "dataset = dataset.iloc[arr].reset_index(drop=True)\n",
    "\n",
    "train, test, message, test_message = train_test_split(dataset, dataset['message'], test_size = 0.20, random_state = 42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "message.reset_index(drop=True, inplace=True)\n",
    "test_message.reset_index(drop=True, inplace=True)\n",
    "\n",
    "message = list(message)\n",
    "test_message = list(test_message)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "class WhiteSpacePreprocessingStopwords():\n",
    "    \"\"\"\n",
    "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents, stopwords_list=None, vocabulary_size=2000, max_df=1.0, min_words=1,\n",
    "                remove_numbers=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param documents: list of strings\n",
    "        :param stopwords_list: list of the stopwords to remove\n",
    "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
    "        :param max_df : float or int, default=1.0\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly higher than the given threshold (corpus-specific\n",
    "        stop words).\n",
    "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
    "        documents, integer absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "        :param min_words: int, default=1. Documents with less words than the parameter\n",
    "        will be removed\n",
    "        :param remove_numbers: bool, default=True. If true, numbers are removed from docs\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        if stopwords_list is not None:\n",
    "            self.stopwords = set(stopwords_list)\n",
    "        else:\n",
    "            self.stopwords = []\n",
    "\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.max_df = max_df\n",
    "        self.min_words = min_words\n",
    "        self.remove_numbers = remove_numbers\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
    "        list of unpreprocessed documents.\n",
    "\n",
    "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
    "        \"\"\"\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        if self.remove_numbers:\n",
    "            preprocessed_docs_tmp = [doc.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
    "                                    for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                                for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, max_df=self.max_df)\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                                for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0 and len(doc) >= self.min_words:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "                retained_indices.append(i)\n",
    "\n",
    "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
    "\n",
    "def get_bag_of_words(data, min_length):\n",
    "    \"\"\"\n",
    "    Creates the bag of words\n",
    "    \"\"\"\n",
    "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
    "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
    "\n",
    "    vect = scipy.sparse.csr_matrix(vect)\n",
    "    return vect\n",
    "\n",
    "\n",
    "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from an input file, assumes one document per line\n",
    "    \"\"\"\n",
    "\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "\n",
    "    if max_seq_length is not None:\n",
    "        model.max_seq_length = max_seq_length\n",
    "\n",
    "    with open(text_file, encoding=\"utf-8\") as filino:\n",
    "        texts = list(map(lambda x: x, filino.readlines()))\n",
    "\n",
    "    check_max_local_length(max_seq_length, texts)\n",
    "\n",
    "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from a list\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "\n",
    "    if max_seq_length is not None:\n",
    "        model.max_seq_length = max_seq_length\n",
    "\n",
    "    check_max_local_length(max_seq_length, texts)\n",
    "\n",
    "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "def check_max_local_length(max_seq_length, texts):\n",
    "    max_local_length = np.max([len(t.split()) for t in texts])\n",
    "    if max_local_length > max_seq_length:\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "\n",
    "\n",
    "class TopicModelDataPreparation:\n",
    "\n",
    "    def __init__(self, contextualized_model=None, show_warning=True, max_seq_length=128):\n",
    "        self.contextualized_model = contextualized_model\n",
    "        self.vocab = []\n",
    "        self.id2token = {}\n",
    "        self.vectorizer = None\n",
    "        self.label_encoder = None\n",
    "        self.show_warning = show_warning\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def load(self, contextualized_embeddings, bow_embeddings, id2token, labels=None):\n",
    "        return CTMDataset(\n",
    "            X_contextual=contextualized_embeddings, X_bow=bow_embeddings, idx2token=id2token, labels=labels)\n",
    "\n",
    "    def fit(self, text_for_contextual, text_for_bow, labels=None, custom_embeddings=None):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if custom_embeddings is not None:\n",
    "            assert len(text_for_contextual) == len(custom_embeddings)\n",
    "\n",
    "            if text_for_bow is not None:\n",
    "                assert len(custom_embeddings) == len(text_for_bow)\n",
    "\n",
    "            if type(custom_embeddings).__module__ != 'numpy':\n",
    "                raise TypeError(\"contextualized_embeddings must be a numpy.ndarray type object\")\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            assert len(text_for_contextual) == len(text_for_bow)\n",
    "\n",
    "        if self.contextualized_model is None and custom_embeddings is None:\n",
    "            raise Exception(\"A contextualized model or contextualized embeddings must be defined\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "\n",
    "        # if the user is passing custom embeddings we don't need to create the embeddings using the model\n",
    "\n",
    "        if custom_embeddings is None:\n",
    "            train_contextualized_embeddings = bert_embeddings_from_list(\n",
    "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
    "        else:\n",
    "            train_contextualized_embeddings = custom_embeddings\n",
    "        self.vocab = self.vectorizer.get_feature_names_out()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        if labels:\n",
    "            self.label_encoder = OneHotEncoder()\n",
    "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        return CTMDataset(\n",
    "            X_contextual=train_contextualized_embeddings, X_bow=train_bow_embeddings,\n",
    "            idx2token=self.id2token, labels=encoded_labels)\n",
    "\n",
    "    def transform(self, text_for_contextual, text_for_bow=None, custom_embeddings=None, labels=None):\n",
    "        \"\"\"\n",
    "        This method create the input for the prediction. Essentially, it creates the embeddings with the contextualized\n",
    "        model of choice and with trained vectorizer.\n",
    "\n",
    "        If text_for_bow is missing, it should be because we are using ZeroShotTM\n",
    "\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if custom_embeddings is not None:\n",
    "            assert len(text_for_contextual) == len(custom_embeddings)\n",
    "\n",
    "            if text_for_bow is not None:\n",
    "                assert len(custom_embeddings) == len(text_for_bow)\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            assert len(text_for_contextual) == len(text_for_bow)\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
    "        else:\n",
    "            # dummy matrix\n",
    "            if self.show_warning:\n",
    "                warnings.simplefilter('always', DeprecationWarning)\n",
    "                warnings.warn(\n",
    "                    \"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n",
    "                    \"are using ZeroShotTM in a cross-lingual setting\")\n",
    "\n",
    "            # we just need an object that is matrix-like so that pytorch does not complain\n",
    "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
    "\n",
    "        if custom_embeddings is None:\n",
    "            test_contextualized_embeddings = bert_embeddings_from_list(\n",
    "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
    "        else:\n",
    "            test_contextualized_embeddings = custom_embeddings\n",
    "\n",
    "        if labels:\n",
    "            encoded_labels = self.label_encoder.transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "\n",
    "        return CTMDataset(X_contextual=test_contextualized_embeddings, X_bow=test_bow_embeddings,\n",
    "                        idx2token=self.id2token, labels=encoded_labels)\n",
    "\n",
    "documents = [line.strip() for line in (message + test_message) if not isinstance(line, float)]\n",
    "test_documents = [line.strip() for line in test_message if not isinstance(line, float)]\n",
    "\n",
    "sp_train = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp_train.preprocess()\n",
    "labels = pd.concat([train, test]).reset_index()['label'][retained_indices]\n",
    "\n",
    "sp_test = WhiteSpacePreprocessingStopwords(test_documents, stopwords_list=stopwords)\n",
    "test_preprocessed_documents, test_unpreprocessed_corpus, test_vocab, test_retained_indices = sp_test.preprocess()\n",
    "test_labels = test['label'][test_retained_indices]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = 'amazon' # folder and dataset name\n",
    "# cols = ['gender', 'age', 'politics'] # outcome columns\n",
    "cols = ['label_x'] # outcome columns\n",
    "outcome = 'label'\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "df = pd.read_csv(dataset + \".csv\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "for model in ['GensimLDA', 'BERTopic', 'NMF', 'Mallet_LDA', 'CTM']:\n",
    "    print(model)\n",
    "\n",
    "    train_error_list = []\n",
    "    test_error_list = []\n",
    "\n",
    "    r2s_train = []\n",
    "    r2s_test = []\n",
    "\n",
    "    if model == 'GensimLDA':\n",
    "        lr = 0.05\n",
    "    elif model == 'BERTopic':\n",
    "        lr = 0.05\n",
    "    elif model == 'NMF':\n",
    "        lr = 0.003\n",
    "    elif model == 'Mallet_LDA':\n",
    "        lr = 0.003\n",
    "    else:\n",
    "        lr = 0.003\n",
    "        df = df.iloc[retained_indices]\n",
    "\n",
    "    for run in range(1, 6):\n",
    "        print(\"Run: \" + str(run))\n",
    "\n",
    "        # loading in distributions that were saved during topic extraction\n",
    "        test_distribution = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_topic_distribution_test.pkl', 'rb'))\n",
    "        train_distribution = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_topic_distribution_train.pkl', 'rb'))\n",
    "\n",
    "        if model == 'CTM':\n",
    "            train_distribution = train_distribution[:round(len(train_distribution) * 0.80)]\n",
    "        \n",
    "        topics = []\n",
    "        with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                topic_list = [item.strip() for item in row if item.strip()]\n",
    "                topics.append(topic_list)\n",
    "\n",
    "        temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "        distribution = np.concatenate([train_distribution, test_distribution]) # concatenating train and test distributions\n",
    "\n",
    "        merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id', 'message'] + cols]\n",
    "        merged.columns = ['message_id', 'message', 'label']\n",
    "\n",
    "        X = distribution\n",
    "        y = merged[outcome].reset_index(drop=True) # the outcome we care about\n",
    "\n",
    "        # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "        X_train = X[:round(0.80 * len(X))]\n",
    "        X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "        y_train = y[:round(0.80 * len(X))]\n",
    "        y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "        if model == 'CTM':\n",
    "            X_train = train_distribution\n",
    "            X_test = test_distribution\n",
    "            y_train = np.array(labels[:round(len(labels) * 0.80)])\n",
    "            y_test = np.array(test_labels)\n",
    "\n",
    "        # Convert arrays to torch tensors\n",
    "        X_train_tensor = torch.tensor(np.array(X_train).astype(np.float32))\n",
    "        y_train_tensor = torch.tensor(np.array(y_train).astype(np.longlong))  # Use long for classification\n",
    "        X_test_tensor = torch.tensor(np.array(X_test).astype(np.float32))\n",
    "        y_test_tensor = torch.tensor(np.array(y_test).astype(np.longlong))\n",
    "\n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Define the model\n",
    "        class LogisticRegressionModel(nn.Module):\n",
    "            def __init__(self, input_size, num_classes):\n",
    "                super(LogisticRegressionModel, self).__init__()\n",
    "                self.layer1 = nn.Linear(input_size, num_classes)\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.layer1(x)\n",
    "\n",
    "        input_size = X_train.shape[1]\n",
    "        num_classes = len(np.unique(y_train))  # Assuming y_train contains all classes\n",
    "        logit_model = LogisticRegressionModel(input_size, num_classes)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()  # This includes softmax\n",
    "        optimizer = optim.Adam(logit_model.parameters(), lr=lr)\n",
    "\n",
    "        loss_values = []\n",
    "        early_stopping_triggered = False\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = logit_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item() * inputs.size(0) \n",
    "            \n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            loss_values.append(epoch_loss)\n",
    "\n",
    "            if epoch >= 50:\n",
    "                # Calculate the percentage change in loss\n",
    "                loss_change = (loss_values[epoch - 50] - loss_values[epoch]) / loss_values[epoch - 50]\n",
    "                \n",
    "                # If change in loss is less than 1%, stop training\n",
    "                if abs(loss_change) < 0.01:\n",
    "                    print(f'Early stopping at epoch {epoch+1} due to minimal loss improvement.')\n",
    "                    early_stopping_triggered = True\n",
    "                    break\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Predict on the test set\n",
    "        logit_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = logit_model(X_train_tensor)\n",
    "            _, predicted_train = torch.max(y_pred_train.data, 1)\n",
    "            y_pred_test = logit_model(X_test_tensor)\n",
    "            _, predicted_test = torch.max(y_pred_test.data, 1)\n",
    "\n",
    "        train_error = 1 - accuracy_score(predicted_train, y_train)\n",
    "        test_error = 1 - accuracy_score(predicted_test, y_test)\n",
    "\n",
    "        train_error_list.append(train_error)\n",
    "        test_error_list.append(test_error)\n",
    "        \n",
    "        print(f'Train Error: {train_error}')\n",
    "        print(f'Test Error: {test_error}')\n",
    "\n",
    "        print()\n",
    "\n",
    "        r2_train = r2_score(y_train, predicted_train)\n",
    "        r2_test = r2_score(y_test, predicted_test)\n",
    "\n",
    "        r2s_train.append(r2_train)\n",
    "        r2s_test.append(r2_test)\n",
    "        \n",
    "        print(f'R2 Train: {r2_train}')\n",
    "        print(f'R2 Test: {r2_test}')\n",
    "\n",
    "        all_scores[model] = {\n",
    "            'Train Error': train_error_list,\n",
    "            'Test Error': test_error_list,\n",
    "            'R2 Train': r2s_train,\n",
    "            'R2 Test': r2s_test,\n",
    "\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': predicted_train.numpy(),\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': predicted_test.numpy()\n",
    "        }\n",
    "    \n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "for m in all_scores.keys():\n",
    "    print(f'{m} Average Train Error: {np.mean(all_scores[m][\"Train Error\"])}')\n",
    "    print(f'{m} Average Test Error: {np.mean(all_scores[m][\"Test Error\"])}')\n",
    "    print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "    print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "    print()\n",
    "\n",
    "with open('all_results/' + dataset + '_all_scores_' + outcome + '.pkl', 'wb') as f:\n",
    "     pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GensimLDA': [0.404875, 0.404875, 0.404875, 0.404875, 0.404875],\n",
       " 'BERTopic': [0.4203, 0.408925, 0.412875, 0.409125, 0.4154],\n",
       " 'NMF': [0.4086, 0.408875, 0.4096, 0.4097, 0.409],\n",
       " 'Mallet_LDA': [0.417625, 0.415425, 0.423175, 0.41495, 0.419475],\n",
       " 'CTM': [0.507888880554097,\n",
       "  0.5096391868577,\n",
       "  0.5076388367964394,\n",
       "  0.5054634561048184,\n",
       "  0.5040132023104044]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
