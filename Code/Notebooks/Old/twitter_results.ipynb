{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 667.018798828125\n",
      "Epoch 51, Loss: 161.36044311523438\n",
      "Epoch 101, Loss: 157.0452117919922\n",
      "Epoch 151, Loss: 155.96665954589844\n",
      "Epoch 201, Loss: 154.83111572265625\n",
      "Epoch 251, Loss: 153.5720672607422\n",
      "Epoch 301, Loss: 152.25775146484375\n",
      "Epoch 351, Loss: 150.9462432861328\n",
      "Epoch 401, Loss: 149.67233276367188\n",
      "Epoch 451, Loss: 148.45370483398438\n",
      "Train RMSE: 10.87511655500615\n",
      "Test RMSE: 11.67916234966243\n",
      "R2 Train: 0.09906862983194431\n",
      "R2 Test: 0.040796828751383574\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 667.018798828125\n",
      "Epoch 51, Loss: 161.36044311523438\n",
      "Epoch 101, Loss: 157.0452117919922\n",
      "Epoch 151, Loss: 155.96665954589844\n",
      "Epoch 201, Loss: 154.83111572265625\n",
      "Epoch 251, Loss: 153.5720672607422\n",
      "Epoch 301, Loss: 152.25775146484375\n",
      "Epoch 351, Loss: 150.9462432861328\n",
      "Epoch 401, Loss: 149.67233276367188\n",
      "Epoch 451, Loss: 148.45370483398438\n",
      "Train RMSE: 10.87511655500615\n",
      "Test RMSE: 11.67916234966243\n",
      "R2 Train: 0.09906862983194431\n",
      "R2 Test: 0.040796828751383574\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 667.018798828125\n",
      "Epoch 51, Loss: 161.36044311523438\n",
      "Epoch 101, Loss: 157.0452117919922\n",
      "Epoch 151, Loss: 155.96665954589844\n",
      "Epoch 201, Loss: 154.83111572265625\n",
      "Epoch 251, Loss: 153.5720672607422\n",
      "Epoch 301, Loss: 152.25775146484375\n",
      "Epoch 351, Loss: 150.9462432861328\n",
      "Epoch 401, Loss: 149.67233276367188\n",
      "Epoch 451, Loss: 148.45370483398438\n",
      "Train RMSE: 10.87511655500615\n",
      "Test RMSE: 11.67916234966243\n",
      "R2 Train: 0.09906862983194431\n",
      "R2 Test: 0.040796828751383574\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 667.018798828125\n",
      "Epoch 51, Loss: 161.36044311523438\n",
      "Epoch 101, Loss: 157.0452117919922\n",
      "Epoch 151, Loss: 155.96665954589844\n",
      "Epoch 201, Loss: 154.83111572265625\n",
      "Epoch 251, Loss: 153.5720672607422\n",
      "Epoch 301, Loss: 152.25775146484375\n",
      "Epoch 351, Loss: 150.9462432861328\n",
      "Epoch 401, Loss: 149.67233276367188\n",
      "Epoch 451, Loss: 148.45370483398438\n",
      "Train RMSE: 10.87511655500615\n",
      "Test RMSE: 11.67916234966243\n",
      "R2 Train: 0.09906862983194431\n",
      "R2 Test: 0.040796828751383574\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 667.018798828125\n",
      "Epoch 51, Loss: 161.36044311523438\n",
      "Epoch 101, Loss: 157.0452117919922\n",
      "Epoch 151, Loss: 155.96665954589844\n",
      "Epoch 201, Loss: 154.83111572265625\n",
      "Epoch 251, Loss: 153.5720672607422\n",
      "Epoch 301, Loss: 152.25775146484375\n",
      "Epoch 351, Loss: 150.9462432861328\n",
      "Epoch 401, Loss: 149.67233276367188\n",
      "Epoch 451, Loss: 148.45370483398438\n",
      "Train RMSE: 10.87511655500615\n",
      "Test RMSE: 11.67916234966243\n",
      "R2 Train: 0.09906862983194431\n",
      "R2 Test: 0.040796828751383574\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 803.0933227539062\n",
      "Epoch 51, Loss: 156.08326721191406\n",
      "Epoch 101, Loss: 156.24716186523438\n",
      "Epoch 151, Loss: 156.84242248535156\n",
      "Epoch 201, Loss: 157.0700225830078\n",
      "Epoch 251, Loss: 156.96165466308594\n",
      "Epoch 301, Loss: 156.66200256347656\n",
      "Epoch 351, Loss: 156.27099609375\n",
      "Epoch 401, Loss: 155.84356689453125\n",
      "Epoch 451, Loss: 155.40675354003906\n",
      "Train RMSE: 11.307955829644014\n",
      "Test RMSE: 12.050585548803527\n",
      "R2 Train: 0.025925721381033773\n",
      "R2 Test: -0.02118285637662365\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 803.0302124023438\n",
      "Epoch 51, Loss: 154.51852416992188\n",
      "Epoch 101, Loss: 153.57122802734375\n",
      "Epoch 151, Loss: 153.3494873046875\n",
      "Epoch 201, Loss: 152.9603271484375\n",
      "Epoch 251, Loss: 152.34823608398438\n",
      "Epoch 301, Loss: 151.60714721679688\n",
      "Epoch 351, Loss: 150.8161163330078\n",
      "Epoch 401, Loss: 150.02415466308594\n",
      "Epoch 451, Loss: 149.25885009765625\n",
      "Train RMSE: 11.198715298127336\n",
      "Test RMSE: 12.041591216182946\n",
      "R2 Train: 0.04465490891576829\n",
      "R2 Test: -0.0196590415319553\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 803.2969970703125\n",
      "Epoch 51, Loss: 155.8874053955078\n",
      "Epoch 101, Loss: 156.15248107910156\n",
      "Epoch 151, Loss: 157.08799743652344\n",
      "Epoch 201, Loss: 157.84097290039062\n",
      "Epoch 251, Loss: 158.32305908203125\n",
      "Epoch 301, Loss: 158.61434936523438\n",
      "Epoch 351, Loss: 158.78857421875\n",
      "Epoch 401, Loss: 158.89505004882812\n",
      "Epoch 451, Loss: 158.96725463867188\n",
      "Train RMSE: 11.375566371076202\n",
      "Test RMSE: 12.09155530985776\n",
      "R2 Train: 0.014242872115152183\n",
      "R2 Test: -0.028138325474434733\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 803.3236694335938\n",
      "Epoch 51, Loss: 155.24810791015625\n",
      "Epoch 101, Loss: 154.85479736328125\n",
      "Epoch 151, Loss: 155.15924072265625\n",
      "Epoch 201, Loss: 155.28729248046875\n",
      "Epoch 251, Loss: 155.16839599609375\n",
      "Epoch 301, Loss: 154.89207458496094\n",
      "Epoch 351, Loss: 154.53065490722656\n",
      "Epoch 401, Loss: 154.12603759765625\n",
      "Epoch 451, Loss: 153.7010040283203\n",
      "Train RMSE: 11.197341659477967\n",
      "Test RMSE: 12.030886693052814\n",
      "R2 Train: 0.04488926052166664\n",
      "R2 Test: -0.01784697000382196\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 803.0782470703125\n",
      "Epoch 51, Loss: 155.27134704589844\n",
      "Epoch 101, Loss: 154.32907104492188\n",
      "Epoch 151, Loss: 153.8715362548828\n",
      "Epoch 201, Loss: 153.18637084960938\n",
      "Epoch 251, Loss: 152.27012634277344\n",
      "Epoch 301, Loss: 151.22988891601562\n",
      "Epoch 351, Loss: 150.14353942871094\n",
      "Epoch 401, Loss: 149.05503845214844\n",
      "Epoch 451, Loss: 147.98760986328125\n",
      "Train RMSE: 11.25729479751965\n",
      "Test RMSE: 12.071498290779786\n",
      "R2 Train: 0.03463411525642168\n",
      "R2 Test: -0.02473027969601671\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 665.3207397460938\n",
      "Epoch 51, Loss: 156.93096923828125\n",
      "Epoch 101, Loss: 156.32040405273438\n",
      "Epoch 151, Loss: 155.91851806640625\n",
      "Epoch 201, Loss: 155.55255126953125\n",
      "Epoch 251, Loss: 155.20530700683594\n",
      "Epoch 301, Loss: 154.8744659423828\n",
      "Epoch 351, Loss: 154.55880737304688\n",
      "Epoch 401, Loss: 154.25746154785156\n",
      "Epoch 451, Loss: 153.96951293945312\n",
      "Train RMSE: 11.693397298723568\n",
      "Test RMSE: 11.935791506938333\n",
      "R2 Train: -0.04161033419256688\n",
      "R2 Test: -0.0018199199861954618\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 665.3682250976562\n",
      "Epoch 51, Loss: 156.84388732910156\n",
      "Epoch 101, Loss: 156.1036834716797\n",
      "Epoch 151, Loss: 155.59417724609375\n",
      "Epoch 201, Loss: 155.1468048095703\n",
      "Epoch 251, Loss: 154.740234375\n",
      "Epoch 301, Loss: 154.3683319091797\n",
      "Epoch 351, Loss: 154.02687072753906\n",
      "Epoch 401, Loss: 153.7123565673828\n",
      "Epoch 451, Loss: 153.42172241210938\n",
      "Train RMSE: 11.693470782610333\n",
      "Test RMSE: 11.935764470212092\n",
      "R2 Train: -0.041623425651686974\n",
      "R2 Test: -0.001815381384758341\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 665.354248046875\n",
      "Epoch 51, Loss: 157.26939392089844\n",
      "Epoch 101, Loss: 157.21241760253906\n",
      "Epoch 151, Loss: 157.3585205078125\n",
      "Epoch 201, Loss: 157.5034637451172\n",
      "Epoch 251, Loss: 157.63323974609375\n",
      "Epoch 301, Loss: 157.75001525878906\n",
      "Epoch 351, Loss: 157.85650634765625\n",
      "Epoch 401, Loss: 157.95486450195312\n",
      "Epoch 451, Loss: 158.04664611816406\n",
      "Train RMSE: 11.693481328859207\n",
      "Test RMSE: 11.935839260929098\n",
      "R2 Train: -0.04162530451640345\n",
      "R2 Test: -0.001827936378720807\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 665.354248046875\n",
      "Epoch 51, Loss: 157.26939392089844\n",
      "Epoch 101, Loss: 157.21241760253906\n",
      "Epoch 151, Loss: 157.3585205078125\n",
      "Epoch 201, Loss: 157.5034637451172\n",
      "Epoch 251, Loss: 157.63323974609375\n",
      "Epoch 301, Loss: 157.75001525878906\n",
      "Epoch 351, Loss: 157.85650634765625\n",
      "Epoch 401, Loss: 157.95486450195312\n",
      "Epoch 451, Loss: 158.04664611816406\n",
      "Train RMSE: 11.693481328859207\n",
      "Test RMSE: 11.935839260929098\n",
      "R2 Train: -0.04162530451640345\n",
      "R2 Test: -0.001827936378720807\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 665.2857055664062\n",
      "Epoch 51, Loss: 157.21701049804688\n",
      "Epoch 101, Loss: 157.09005737304688\n",
      "Epoch 151, Loss: 157.15127563476562\n",
      "Epoch 201, Loss: 157.1995849609375\n",
      "Epoch 251, Loss: 157.2215576171875\n",
      "Epoch 301, Loss: 157.21998596191406\n",
      "Epoch 351, Loss: 157.19834899902344\n",
      "Epoch 401, Loss: 157.15975952148438\n",
      "Epoch 451, Loss: 157.10678100585938\n",
      "Train RMSE: 11.693411219315344\n",
      "Test RMSE: 11.935742792788547\n",
      "R2 Train: -0.04161281419739571\n",
      "R2 Test: -0.0018117424462271714\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 838.6974487304688\n",
      "Epoch 51, Loss: 162.38548278808594\n",
      "Epoch 101, Loss: 160.04946899414062\n",
      "Epoch 151, Loss: 157.77134704589844\n",
      "Epoch 201, Loss: 155.57447814941406\n",
      "Epoch 251, Loss: 153.4779052734375\n",
      "Epoch 301, Loss: 151.5050506591797\n",
      "Epoch 351, Loss: 149.66867065429688\n",
      "Epoch 401, Loss: 147.97158813476562\n",
      "Epoch 451, Loss: 146.4094696044922\n",
      "Train RMSE: 10.76678125110782\n",
      "Test RMSE: 11.81400499708911\n",
      "R2 Train: 0.116928949042156\n",
      "R2 Test: 0.018519861942195903\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 837.887451171875\n",
      "Epoch 51, Loss: 162.69366455078125\n",
      "Epoch 101, Loss: 161.45745849609375\n",
      "Epoch 151, Loss: 160.16014099121094\n",
      "Epoch 201, Loss: 158.69708251953125\n",
      "Epoch 251, Loss: 157.13351440429688\n",
      "Epoch 301, Loss: 155.5459442138672\n",
      "Epoch 351, Loss: 153.98727416992188\n",
      "Epoch 401, Loss: 152.48851013183594\n",
      "Epoch 451, Loss: 151.06512451171875\n",
      "Train RMSE: 11.863207135758104\n",
      "Test RMSE: 11.886841296731829\n",
      "R2 Train: -0.07208222498307104\n",
      "R2 Test: 0.006380413846153732\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 836.0191650390625\n",
      "Epoch 51, Loss: 162.49171447753906\n",
      "Epoch 101, Loss: 160.79733276367188\n",
      "Epoch 151, Loss: 159.09742736816406\n",
      "Epoch 201, Loss: 157.20074462890625\n",
      "Epoch 251, Loss: 155.2440948486328\n",
      "Epoch 301, Loss: 153.35586547851562\n",
      "Epoch 351, Loss: 151.59417724609375\n",
      "Epoch 401, Loss: 149.97251892089844\n",
      "Epoch 451, Loss: 148.48361206054688\n",
      "Train RMSE: 10.792999430556616\n",
      "Test RMSE: 11.836637470249244\n",
      "R2 Train: 0.11262298162736384\n",
      "R2 Test: 0.014755753110447856\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 837.7274780273438\n",
      "Epoch 51, Loss: 162.7815704345703\n",
      "Epoch 101, Loss: 160.46189880371094\n",
      "Epoch 151, Loss: 158.09390258789062\n",
      "Epoch 201, Loss: 155.73045349121094\n",
      "Epoch 251, Loss: 153.43275451660156\n",
      "Epoch 301, Loss: 151.25048828125\n",
      "Epoch 351, Loss: 149.2113800048828\n",
      "Epoch 401, Loss: 147.3263702392578\n",
      "Epoch 451, Loss: 145.5951690673828\n",
      "Train RMSE: 10.705323692831334\n",
      "Test RMSE: 11.775929102650823\n",
      "R2 Train: 0.12698144232176967\n",
      "R2 Test: 0.02483618095833795\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 837.46875\n",
      "Epoch 51, Loss: 163.35409545898438\n",
      "Epoch 101, Loss: 162.20870971679688\n",
      "Epoch 151, Loss: 161.03086853027344\n",
      "Epoch 201, Loss: 159.7276611328125\n",
      "Epoch 251, Loss: 158.3575897216797\n",
      "Epoch 301, Loss: 156.98402404785156\n",
      "Epoch 351, Loss: 155.6474151611328\n",
      "Epoch 401, Loss: 154.36952209472656\n",
      "Epoch 451, Loss: 153.15963745117188\n",
      "Train RMSE: 10.858373982719137\n",
      "Test RMSE: 11.844491242762686\n",
      "R2 Train: 0.10184051690951323\n",
      "R2 Test: 0.01344787301337802\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 430.171630859375\n",
      "Epoch 51, Loss: 109.0796127319336\n",
      "Epoch 101, Loss: 92.55352783203125\n",
      "Epoch 151, Loss: 90.58927917480469\n",
      "Epoch 201, Loss: 90.58311462402344\n",
      "Epoch 251, Loss: 90.76222229003906\n",
      "Epoch 301, Loss: 90.8997573852539\n",
      "Epoch 351, Loss: 90.99015808105469\n",
      "Epoch 401, Loss: 91.05091094970703\n",
      "Epoch 451, Loss: 91.09483337402344\n",
      "Train RMSE: 9.640637742250265\n",
      "Test RMSE: 10.884625355795329\n",
      "R2 Train: 0.2919965126811732\n",
      "R2 Test: 0.16686729027786174\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 429.1767272949219\n",
      "Epoch 51, Loss: 108.84660339355469\n",
      "Epoch 101, Loss: 91.02375793457031\n",
      "Epoch 151, Loss: 86.76152038574219\n",
      "Epoch 201, Loss: 85.10991668701172\n",
      "Epoch 251, Loss: 84.33631896972656\n",
      "Epoch 301, Loss: 83.94511413574219\n",
      "Epoch 351, Loss: 83.73970031738281\n",
      "Epoch 401, Loss: 83.6304702758789\n",
      "Epoch 451, Loss: 83.57351684570312\n",
      "Train RMSE: 9.604708442850571\n",
      "Test RMSE: 10.771363866678998\n",
      "R2 Train: 0.2972639374925977\n",
      "R2 Test: 0.1841156383937831\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 428.7510986328125\n",
      "Epoch 51, Loss: 106.81069946289062\n",
      "Epoch 101, Loss: 88.86729431152344\n",
      "Epoch 151, Loss: 85.68456268310547\n",
      "Epoch 201, Loss: 84.8001937866211\n",
      "Epoch 251, Loss: 84.4433822631836\n",
      "Epoch 301, Loss: 84.27055358886719\n",
      "Epoch 351, Loss: 84.18042755126953\n",
      "Epoch 401, Loss: 84.13257598876953\n",
      "Epoch 451, Loss: 84.10806274414062\n",
      "Train RMSE: 9.572265465589377\n",
      "Test RMSE: 10.745673820715139\n",
      "R2 Train: 0.3020033515049826\n",
      "R2 Test: 0.1880028177238694\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 430.2998046875\n",
      "Epoch 51, Loss: 107.64950561523438\n",
      "Epoch 101, Loss: 90.97625732421875\n",
      "Epoch 151, Loss: 89.0243911743164\n",
      "Epoch 201, Loss: 88.89114379882812\n",
      "Epoch 251, Loss: 88.90728759765625\n",
      "Epoch 301, Loss: 88.89122009277344\n",
      "Epoch 351, Loss: 88.84856414794922\n",
      "Epoch 401, Loss: 88.79618835449219\n",
      "Epoch 451, Loss: 88.74383544921875\n",
      "Train RMSE: 9.58651002364616\n",
      "Test RMSE: 10.76131224985069\n",
      "R2 Train: 0.29992441800977143\n",
      "R2 Test: 0.18563766115668134\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 428.21429443359375\n",
      "Epoch 51, Loss: 109.65528106689453\n",
      "Epoch 101, Loss: 93.65066528320312\n",
      "Epoch 151, Loss: 90.55792999267578\n",
      "Epoch 201, Loss: 89.52568054199219\n",
      "Epoch 251, Loss: 89.05242919921875\n",
      "Epoch 301, Loss: 88.79744720458984\n",
      "Epoch 351, Loss: 88.64395141601562\n",
      "Epoch 401, Loss: 88.54261016845703\n",
      "Epoch 451, Loss: 88.47013854980469\n",
      "Train RMSE: 9.567955468001943\n",
      "Test RMSE: 10.840036562411633\n",
      "R2 Train: 0.3026317683850994\n",
      "R2 Test: 0.17367915416773727\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 10.87511655500615\n",
      "GensimLDA Average Test RMSE: 11.67916234966243\n",
      "GensimLDA Average R2 Train: 0.09906862983194431\n",
      "GensimLDA Average R2 Test: 0.040796828751383574\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 11.267374791169033\n",
      "Mallet_LDA Average Test RMSE: 12.057223411735366\n",
      "Mallet_LDA Average R2 Train: 0.03286937563800851\n",
      "Mallet_LDA Average R2 Test: -0.02231149461657047\n",
      "\n",
      "CTM Average Train RMSE: 11.693448391673531\n",
      "CTM Average Test RMSE: 11.935795458359433\n",
      "CTM Average R2 Train: -0.04161943661489129\n",
      "CTM Average R2 Test: -0.0018205833149245176\n",
      "\n",
      "BERTopic Average Train RMSE: 10.997337098594603\n",
      "BERTopic Average Test RMSE: 11.831580821896738\n",
      "BERTopic Average R2 Train: 0.07725833298354634\n",
      "BERTopic Average R2 Test: 0.015588016574102692\n",
      "\n",
      "NMF Average Train RMSE: 9.594415428467661\n",
      "NMF Average Test RMSE: 10.800602371090358\n",
      "NMF Average R2 Train: 0.2987639976147249\n",
      "NMF Average R2 Test: 0.17966051234398656\n",
      "\n",
      "politics\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 4.388339042663574\n",
      "Epoch 51, Loss: 3.2905452251434326\n",
      "Epoch 101, Loss: 3.258669137954712\n",
      "Epoch 151, Loss: 3.244906425476074\n",
      "Epoch 201, Loss: 3.2359323501586914\n",
      "Epoch 251, Loss: 3.229609966278076\n",
      "Epoch 301, Loss: 3.225001335144043\n",
      "Epoch 351, Loss: 3.2215561866760254\n",
      "Epoch 401, Loss: 3.2189249992370605\n",
      "Epoch 451, Loss: 3.2168760299682617\n",
      "Train RMSE: 1.7564726151142733\n",
      "Test RMSE: 1.830317128514923\n",
      "R2 Train: 0.020841711008495634\n",
      "R2 Test: 0.024172790856195614\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 4.388339042663574\n",
      "Epoch 51, Loss: 3.2905452251434326\n",
      "Epoch 101, Loss: 3.258669137954712\n",
      "Epoch 151, Loss: 3.244906425476074\n",
      "Epoch 201, Loss: 3.2359323501586914\n",
      "Epoch 251, Loss: 3.229609966278076\n",
      "Epoch 301, Loss: 3.225001335144043\n",
      "Epoch 351, Loss: 3.2215561866760254\n",
      "Epoch 401, Loss: 3.2189249992370605\n",
      "Epoch 451, Loss: 3.2168760299682617\n",
      "Train RMSE: 1.7564726151142733\n",
      "Test RMSE: 1.830317128514923\n",
      "R2 Train: 0.020841711008495634\n",
      "R2 Test: 0.024172790856195614\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 4.388339042663574\n",
      "Epoch 51, Loss: 3.2905452251434326\n",
      "Epoch 101, Loss: 3.258669137954712\n",
      "Epoch 151, Loss: 3.244906425476074\n",
      "Epoch 201, Loss: 3.2359323501586914\n",
      "Epoch 251, Loss: 3.229609966278076\n",
      "Epoch 301, Loss: 3.225001335144043\n",
      "Epoch 351, Loss: 3.2215561866760254\n",
      "Epoch 401, Loss: 3.2189249992370605\n",
      "Epoch 451, Loss: 3.2168760299682617\n",
      "Train RMSE: 1.7564726151142733\n",
      "Test RMSE: 1.830317128514923\n",
      "R2 Train: 0.020841711008495634\n",
      "R2 Test: 0.024172790856195614\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 4.388339042663574\n",
      "Epoch 51, Loss: 3.2905452251434326\n",
      "Epoch 101, Loss: 3.258669137954712\n",
      "Epoch 151, Loss: 3.244906425476074\n",
      "Epoch 201, Loss: 3.2359323501586914\n",
      "Epoch 251, Loss: 3.229609966278076\n",
      "Epoch 301, Loss: 3.225001335144043\n",
      "Epoch 351, Loss: 3.2215561866760254\n",
      "Epoch 401, Loss: 3.2189249992370605\n",
      "Epoch 451, Loss: 3.2168760299682617\n",
      "Train RMSE: 1.7564726151142733\n",
      "Test RMSE: 1.830317128514923\n",
      "R2 Train: 0.020841711008495634\n",
      "R2 Test: 0.024172790856195614\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 4.388339042663574\n",
      "Epoch 51, Loss: 3.2905452251434326\n",
      "Epoch 101, Loss: 3.258669137954712\n",
      "Epoch 151, Loss: 3.244906425476074\n",
      "Epoch 201, Loss: 3.2359323501586914\n",
      "Epoch 251, Loss: 3.229609966278076\n",
      "Epoch 301, Loss: 3.225001335144043\n",
      "Epoch 351, Loss: 3.2215561866760254\n",
      "Epoch 401, Loss: 3.2189249992370605\n",
      "Epoch 451, Loss: 3.2168760299682617\n",
      "Train RMSE: 1.7564726151142733\n",
      "Test RMSE: 1.830317128514923\n",
      "R2 Train: 0.020841711008495634\n",
      "R2 Test: 0.024172790856195614\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 3.08471417427063\n",
      "Epoch 51, Loss: 3.0924577713012695\n",
      "Epoch 101, Loss: 3.030242443084717\n",
      "Epoch 151, Loss: 2.9903223514556885\n",
      "Epoch 201, Loss: 2.967019557952881\n",
      "Epoch 251, Loss: 2.9526867866516113\n",
      "Epoch 301, Loss: 2.943527936935425\n",
      "Epoch 351, Loss: 2.937676191329956\n",
      "Epoch 401, Loss: 2.9340240955352783\n",
      "Epoch 451, Loss: 2.9318320751190186\n",
      "Train RMSE: 1.7694325980442696\n",
      "Test RMSE: 1.8358612535657048\n",
      "R2 Train: 0.006339133309321676\n",
      "R2 Test: 0.01825217552748304\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 3.108069658279419\n",
      "Epoch 51, Loss: 3.0998713970184326\n",
      "Epoch 101, Loss: 3.012293815612793\n",
      "Epoch 151, Loss: 2.9564778804779053\n",
      "Epoch 201, Loss: 2.9230377674102783\n",
      "Epoch 251, Loss: 2.899965286254883\n",
      "Epoch 301, Loss: 2.882446765899658\n",
      "Epoch 351, Loss: 2.8688783645629883\n",
      "Epoch 401, Loss: 2.85823130607605\n",
      "Epoch 451, Loss: 2.8497395515441895\n",
      "Train RMSE: 1.7701842089757511\n",
      "Test RMSE: 1.8350255665807411\n",
      "R2 Train: 0.005494789205690798\n",
      "R2 Test: 0.019145758467290053\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 3.0990004539489746\n",
      "Epoch 51, Loss: 3.214357376098633\n",
      "Epoch 101, Loss: 3.2024118900299072\n",
      "Epoch 151, Loss: 3.188094139099121\n",
      "Epoch 201, Loss: 3.17809796333313\n",
      "Epoch 251, Loss: 3.1719019412994385\n",
      "Epoch 301, Loss: 3.168525218963623\n",
      "Epoch 351, Loss: 3.1667628288269043\n",
      "Epoch 401, Loss: 3.165925979614258\n",
      "Epoch 451, Loss: 3.1656477451324463\n",
      "Train RMSE: 1.7694920480455747\n",
      "Test RMSE: 1.8350401766653228\n",
      "R2 Train: 0.006272361472670074\n",
      "R2 Test: 0.0191301396980047\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 3.0951731204986572\n",
      "Epoch 51, Loss: 3.192448139190674\n",
      "Epoch 101, Loss: 3.1832549571990967\n",
      "Epoch 151, Loss: 3.171902894973755\n",
      "Epoch 201, Loss: 3.162618637084961\n",
      "Epoch 251, Loss: 3.1554226875305176\n",
      "Epoch 301, Loss: 3.1497702598571777\n",
      "Epoch 351, Loss: 3.1451308727264404\n",
      "Epoch 401, Loss: 3.141216993331909\n",
      "Epoch 451, Loss: 3.137843370437622\n",
      "Train RMSE: 1.7698590100958846\n",
      "Test RMSE: 1.8358500229953951\n",
      "R2 Train: 0.005860154880151236\n",
      "R2 Test: 0.01826418684285236\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 3.1003921031951904\n",
      "Epoch 51, Loss: 3.1007959842681885\n",
      "Epoch 101, Loss: 3.027663469314575\n",
      "Epoch 151, Loss: 2.9905335903167725\n",
      "Epoch 201, Loss: 2.9716789722442627\n",
      "Epoch 251, Loss: 2.96000075340271\n",
      "Epoch 301, Loss: 2.951697587966919\n",
      "Epoch 351, Loss: 2.9453537464141846\n",
      "Epoch 401, Loss: 2.940315008163452\n",
      "Epoch 451, Loss: 2.936220407485962\n",
      "Train RMSE: 1.7699758979413445\n",
      "Test RMSE: 1.8349066001539407\n",
      "R2 Train: 0.005728837412323329\n",
      "R2 Test: 0.019272933743681442\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 3.78210186958313\n",
      "Epoch 51, Loss: 3.2412657737731934\n",
      "Epoch 101, Loss: 3.2184395790100098\n",
      "Epoch 151, Loss: 3.2153513431549072\n",
      "Epoch 201, Loss: 3.2121834754943848\n",
      "Epoch 251, Loss: 3.2079875469207764\n",
      "Epoch 301, Loss: 3.203094482421875\n",
      "Epoch 351, Loss: 3.1978158950805664\n",
      "Epoch 401, Loss: 3.192355155944824\n",
      "Epoch 451, Loss: 3.1868369579315186\n",
      "Train RMSE: 1.7736743487176045\n",
      "Test RMSE: 1.852212810865942\n",
      "R2 Train: 0.00156934024932176\n",
      "R2 Test: 0.0006859283229653235\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 3.7811050415039062\n",
      "Epoch 51, Loss: 3.1959636211395264\n",
      "Epoch 101, Loss: 3.1487953662872314\n",
      "Epoch 151, Loss: 3.1330652236938477\n",
      "Epoch 201, Loss: 3.1230154037475586\n",
      "Epoch 251, Loss: 3.1152353286743164\n",
      "Epoch 301, Loss: 3.1087491512298584\n",
      "Epoch 351, Loss: 3.1031301021575928\n",
      "Epoch 401, Loss: 3.0981481075286865\n",
      "Epoch 451, Loss: 3.0936663150787354\n",
      "Train RMSE: 1.7773788276734281\n",
      "Test RMSE: 1.8529003884493664\n",
      "R2 Train: -0.002605640201615911\n",
      "R2 Test: -5.613920192337751e-05\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 3.7818193435668945\n",
      "Epoch 51, Loss: 3.3080501556396484\n",
      "Epoch 101, Loss: 3.302649974822998\n",
      "Epoch 151, Loss: 3.3032126426696777\n",
      "Epoch 201, Loss: 3.3026318550109863\n",
      "Epoch 251, Loss: 3.3014795780181885\n",
      "Epoch 301, Loss: 3.30015230178833\n",
      "Epoch 351, Loss: 3.298856496810913\n",
      "Epoch 401, Loss: 3.2976973056793213\n",
      "Epoch 451, Loss: 3.2967264652252197\n",
      "Train RMSE: 1.7706959680502898\n",
      "Test RMSE: 1.8477198960510197\n",
      "R2 Train: 0.004919684499810062\n",
      "R2 Test: 0.0055281229935492116\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 3.7818193435668945\n",
      "Epoch 51, Loss: 3.3080501556396484\n",
      "Epoch 101, Loss: 3.302649974822998\n",
      "Epoch 151, Loss: 3.3032126426696777\n",
      "Epoch 201, Loss: 3.3026318550109863\n",
      "Epoch 251, Loss: 3.3014795780181885\n",
      "Epoch 301, Loss: 3.30015230178833\n",
      "Epoch 351, Loss: 3.298856496810913\n",
      "Epoch 401, Loss: 3.2976973056793213\n",
      "Epoch 451, Loss: 3.2967264652252197\n",
      "Train RMSE: 1.7706959680502898\n",
      "Test RMSE: 1.8477198960510197\n",
      "R2 Train: 0.004919684499810062\n",
      "R2 Test: 0.0055281229935492116\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 3.7821550369262695\n",
      "Epoch 51, Loss: 3.28751540184021\n",
      "Epoch 101, Loss: 3.2520291805267334\n",
      "Epoch 151, Loss: 3.2266438007354736\n",
      "Epoch 201, Loss: 3.2047507762908936\n",
      "Epoch 251, Loss: 3.1859164237976074\n",
      "Epoch 301, Loss: 3.169565439224243\n",
      "Epoch 351, Loss: 3.155156373977661\n",
      "Epoch 401, Loss: 3.142271041870117\n",
      "Epoch 451, Loss: 3.1306071281433105\n",
      "Train RMSE: 1.7695970574967144\n",
      "Test RMSE: 1.850541262617678\n",
      "R2 Train: 0.006154413623922106\n",
      "R2 Test: 0.0024887966982466425\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 3.2318708896636963\n",
      "Epoch 51, Loss: 3.2667131423950195\n",
      "Epoch 101, Loss: 3.290332555770874\n",
      "Epoch 151, Loss: 3.2808737754821777\n",
      "Epoch 201, Loss: 3.2716927528381348\n",
      "Epoch 251, Loss: 3.2640886306762695\n",
      "Epoch 301, Loss: 3.2576537132263184\n",
      "Epoch 351, Loss: 3.252067804336548\n",
      "Epoch 401, Loss: 3.2471375465393066\n",
      "Epoch 451, Loss: 3.242738962173462\n",
      "Train RMSE: 1.7529705741480641\n",
      "Test RMSE: 1.839051087078021\n",
      "R2 Train: 0.024742294487444028\n",
      "R2 Test: 0.014837611693698793\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 3.2260820865631104\n",
      "Epoch 51, Loss: 3.2674551010131836\n",
      "Epoch 101, Loss: 3.2898144721984863\n",
      "Epoch 151, Loss: 3.2792060375213623\n",
      "Epoch 201, Loss: 3.2684640884399414\n",
      "Epoch 251, Loss: 3.2593653202056885\n",
      "Epoch 301, Loss: 3.251608371734619\n",
      "Epoch 351, Loss: 3.244877338409424\n",
      "Epoch 401, Loss: 3.238945960998535\n",
      "Epoch 451, Loss: 3.2336626052856445\n",
      "Train RMSE: 1.7592950339935307\n",
      "Test RMSE: 1.8414343750758226\n",
      "R2 Train: 0.017692427401917787\n",
      "R2 Test: 0.01228254719301003\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 3.2136316299438477\n",
      "Epoch 51, Loss: 3.268592119216919\n",
      "Epoch 101, Loss: 3.2872228622436523\n",
      "Epoch 151, Loss: 3.268977403640747\n",
      "Epoch 201, Loss: 3.2491698265075684\n",
      "Epoch 251, Loss: 3.230552911758423\n",
      "Epoch 301, Loss: 3.2132492065429688\n",
      "Epoch 351, Loss: 3.1971607208251953\n",
      "Epoch 401, Loss: 3.1821818351745605\n",
      "Epoch 451, Loss: 3.1682114601135254\n",
      "Train RMSE: 1.7506285455973296\n",
      "Test RMSE: 1.8397266139112074\n",
      "R2 Train: 0.0273465088638023\n",
      "R2 Test: 0.01411373201399635\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 3.218019723892212\n",
      "Epoch 51, Loss: 3.273127555847168\n",
      "Epoch 101, Loss: 3.2930054664611816\n",
      "Epoch 151, Loss: 3.2784974575042725\n",
      "Epoch 201, Loss: 3.2636098861694336\n",
      "Epoch 251, Loss: 3.250274896621704\n",
      "Epoch 301, Loss: 3.238302707672119\n",
      "Epoch 351, Loss: 3.227465867996216\n",
      "Epoch 401, Loss: 3.217597246170044\n",
      "Epoch 451, Loss: 3.2085723876953125\n",
      "Train RMSE: 1.761737496956251\n",
      "Test RMSE: 1.8423177148700798\n",
      "R2 Train: 0.014963021270564814\n",
      "R2 Test: 0.011334699785576663\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 3.2167587280273438\n",
      "Epoch 51, Loss: 3.276524066925049\n",
      "Epoch 101, Loss: 3.2958669662475586\n",
      "Epoch 151, Loss: 3.281627893447876\n",
      "Epoch 201, Loss: 3.2674694061279297\n",
      "Epoch 251, Loss: 3.2549946308135986\n",
      "Epoch 301, Loss: 3.243898868560791\n",
      "Epoch 351, Loss: 3.233876943588257\n",
      "Epoch 401, Loss: 3.2247235774993896\n",
      "Epoch 451, Loss: 3.2162935733795166\n",
      "Train RMSE: 1.7609148830405859\n",
      "Test RMSE: 1.8432333958322002\n",
      "R2 Train: 0.015882699654343524\n",
      "R2 Test: 0.010351669588770318\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch 1, Loss: 3.4224579334259033\n",
      "Epoch 51, Loss: 2.916395425796509\n",
      "Epoch 101, Loss: 2.934216260910034\n",
      "Epoch 151, Loss: 2.928677558898926\n",
      "Epoch 201, Loss: 2.923818349838257\n",
      "Epoch 251, Loss: 2.920461654663086\n",
      "Epoch 301, Loss: 2.918156385421753\n",
      "Epoch 351, Loss: 2.9165468215942383\n",
      "Epoch 401, Loss: 2.9154021739959717\n",
      "Epoch 451, Loss: 2.914569139480591\n",
      "Train RMSE: 1.7239402121783312\n",
      "Test RMSE: 1.8252139838783572\n",
      "R2 Train: 0.05677666074473797\n",
      "R2 Test: 0.029606652767367958\n",
      "Run: 2\n",
      "\n",
      "Epoch 1, Loss: 3.4192488193511963\n",
      "Epoch 51, Loss: 2.92199969291687\n",
      "Epoch 101, Loss: 2.9304683208465576\n",
      "Epoch 151, Loss: 2.927786111831665\n",
      "Epoch 201, Loss: 2.9256677627563477\n",
      "Epoch 251, Loss: 2.9244561195373535\n",
      "Epoch 301, Loss: 2.923776865005493\n",
      "Epoch 351, Loss: 2.923387289047241\n",
      "Epoch 401, Loss: 2.923149824142456\n",
      "Epoch 451, Loss: 2.922988176345825\n",
      "Train RMSE: 1.7239946838221112\n",
      "Test RMSE: 1.8125653941780129\n",
      "R2 Train: 0.05671705341370359\n",
      "R2 Test: 0.043009550295945\n",
      "Run: 3\n",
      "\n",
      "Epoch 1, Loss: 3.4254651069641113\n",
      "Epoch 51, Loss: 2.8406713008880615\n",
      "Epoch 101, Loss: 2.8521368503570557\n",
      "Epoch 151, Loss: 2.852954626083374\n",
      "Epoch 201, Loss: 2.854031562805176\n",
      "Epoch 251, Loss: 2.8555705547332764\n",
      "Epoch 301, Loss: 2.857264280319214\n",
      "Epoch 351, Loss: 2.8589437007904053\n",
      "Epoch 401, Loss: 2.860530376434326\n",
      "Epoch 451, Loss: 2.861999273300171\n",
      "Train RMSE: 1.7230849507511965\n",
      "Test RMSE: 1.8178632633149339\n",
      "R2 Train: 0.05771231086478201\n",
      "R2 Test: 0.037407082499704036\n",
      "Run: 4\n",
      "\n",
      "Epoch 1, Loss: 3.412506580352783\n",
      "Epoch 51, Loss: 2.906709671020508\n",
      "Epoch 101, Loss: 2.9256279468536377\n",
      "Epoch 151, Loss: 2.925269842147827\n",
      "Epoch 201, Loss: 2.924281358718872\n",
      "Epoch 251, Loss: 2.923767328262329\n",
      "Epoch 301, Loss: 2.923598289489746\n",
      "Epoch 351, Loss: 2.923631429672241\n",
      "Epoch 401, Loss: 2.9237775802612305\n",
      "Epoch 451, Loss: 2.923981189727783\n",
      "Train RMSE: 1.7166001772484736\n",
      "Test RMSE: 1.8137012858019863\n",
      "R2 Train: 0.0647915020110087\n",
      "R2 Test: 0.04180972815458883\n",
      "Run: 5\n",
      "\n",
      "Epoch 1, Loss: 3.4428257942199707\n",
      "Epoch 51, Loss: 2.951077461242676\n",
      "Epoch 101, Loss: 2.9742000102996826\n",
      "Epoch 151, Loss: 2.975376844406128\n",
      "Epoch 201, Loss: 2.9763998985290527\n",
      "Epoch 251, Loss: 2.9780380725860596\n",
      "Epoch 301, Loss: 2.9799633026123047\n",
      "Epoch 351, Loss: 2.9819507598876953\n",
      "Epoch 401, Loss: 2.983875036239624\n",
      "Epoch 451, Loss: 2.985673427581787\n",
      "Train RMSE: 1.7191897036148363\n",
      "Test RMSE: 1.8176164282761837\n",
      "R2 Train: 0.06196781164545795\n",
      "R2 Test: 0.037668472378410534\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.7564726151142733\n",
      "GensimLDA Average Test RMSE: 1.830317128514923\n",
      "GensimLDA Average R2 Train: 0.020841711008495634\n",
      "GensimLDA Average R2 Test: 0.024172790856195614\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.7697887526205647\n",
      "Mallet_LDA Average Test RMSE: 1.8353367239922211\n",
      "Mallet_LDA Average R2 Train: 0.005939055256031422\n",
      "Mallet_LDA Average R2 Test: 0.01881303885586232\n",
      "\n",
      "CTM Average Train RMSE: 1.7724084339976653\n",
      "CTM Average Test RMSE: 1.8502188508070052\n",
      "CTM Average R2 Train: 0.0029914965342496156\n",
      "CTM Average R2 Test: 0.0028349663612774025\n",
      "\n",
      "BERTopic Average Train RMSE: 1.7571093067471524\n",
      "BERTopic Average Test RMSE: 1.8411526373534663\n",
      "BERTopic Average R2 Train: 0.02012539033561449\n",
      "BERTopic Average R2 Test: 0.01258405205501043\n",
      "\n",
      "NMF Average Train RMSE: 1.7213619455229896\n",
      "NMF Average Test RMSE: 1.8173920710898948\n",
      "NMF Average R2 Train: 0.059593067735938046\n",
      "NMF Average R2 Test: 0.03790029721920327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# facebook_2020\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# change this\n",
    "dataset = 'twitter' # folder and dataset name\n",
    "\n",
    "cols = ['age', 'politics'] # outcome columns\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "# START\n",
    "train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "df = pd.read_csv(dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "\n",
    "merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "all_user_ids = list(user_ids.keys())\n",
    "\n",
    "labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "\n",
    "all_ys = []\n",
    "for key in user_ids.keys():\n",
    "    all_ys.append(labels_data[labels_data['user_id'] == key][cols].iloc[0])\n",
    "\n",
    "for outcome in cols:\n",
    "    print(outcome)\n",
    "    print()\n",
    "\n",
    "    y = [i[outcome] for i in all_ys]\n",
    "    \n",
    "    # REGRESSION\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for model in ['GensimLDA', 'Mallet_LDA', 'CTM', 'BERTopic', 'NMF']:\n",
    "        print(model)\n",
    "        \n",
    "        train_error_list = []\n",
    "        test_error_list = []\n",
    "\n",
    "        r2s_train = []\n",
    "        r2s_test = []\n",
    "\n",
    "        for run in range(1, 6):\n",
    "            print(\"Run: \" + str(run))\n",
    "            print()\n",
    "\n",
    "            topics = []\n",
    "            with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    topic_list = [item.strip() for item in row if item.strip()]\n",
    "                    topics.append(topic_list)\n",
    "\n",
    "            X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "            # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "            X_train = X[:round(0.80 * len(X))]\n",
    "            X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "            y_train = y[:round(0.80 * len(X))]\n",
    "            y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            X_test = np.array(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            torch.manual_seed(42) # for reproducibility\n",
    "\n",
    "            X_train_NN = torch.tensor(X_train, dtype = torch.float32)\n",
    "            y_train_NN = torch.tensor(y_train, dtype = torch.float32).reshape(-1, 1)\n",
    "            X_test_NN = torch.tensor(X_test, dtype = torch.float32)\n",
    "            y_test_NN = torch.tensor(y_test, dtype = torch.float32).reshape(-1, 1)\n",
    "\n",
    "            model_NN = nn.Sequential(\n",
    "                nn.Linear(X_train_NN.shape[1], 1),\n",
    "            )\n",
    "\n",
    "            loss_fn = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model_NN.parameters(), lr = lr)\n",
    "\n",
    "            n_epochs = 500\n",
    "            batch_size = 64\n",
    "            batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "            best_mse = np.inf\n",
    "            best_weights = None\n",
    "            history = []\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                model_NN.train()\n",
    "                with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                    bar.set_description(f\"Epoch {epoch}\")\n",
    "                    for start in bar:\n",
    "\n",
    "                        X_batch = X_train_NN[start : start+batch_size]\n",
    "                        y_batch = y_train_NN[start : start+batch_size]\n",
    "\n",
    "                        y_pred = model_NN(X_batch)\n",
    "                        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                        bar.set_postfix(mse=float(loss))\n",
    "\n",
    "                model_NN.eval()\n",
    "                y_pred = model_NN(X_test_NN)\n",
    "                mse = loss_fn(y_pred, y_test_NN)\n",
    "                mse = float(mse)\n",
    "                history.append(mse)\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_weights = copy.deepcopy(model_NN.state_dict())\n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "            model_NN.load_state_dict(best_weights)\n",
    "\n",
    "            y_pred_train = model_NN(X_train_NN).detach().numpy()\n",
    "            y_pred_test = model_NN(X_test_NN).detach().numpy()\n",
    "\n",
    "            train_error = np.sqrt(mean_squared_error(y_pred_train, y_train))\n",
    "            test_error = np.sqrt(mean_squared_error(y_pred_test, y_test))\n",
    "\n",
    "            train_error_list.append(train_error)\n",
    "            test_error_list.append(test_error)\n",
    "\n",
    "            print(f'Train RMSE: {train_error}')\n",
    "            print(f'Test RMSE: {test_error}')\n",
    "\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "            r2s_train.append(r2_train)\n",
    "            r2s_test.append(r2_test)\n",
    "            \n",
    "            print(f'R2 Train: {r2_train}')\n",
    "            print(f'R2 Test: {r2_test}')\n",
    "\n",
    "            all_scores[model] = {\n",
    "                'Train RMSE': train_error_list,\n",
    "                'Test RMSE': test_error_list,\n",
    "                'R2 Train': r2s_train,\n",
    "                'R2 Test': r2s_test,\n",
    "\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': y_pred_train,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_pred_test\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    for m in all_scores.keys():\n",
    "        print(f'{m} Average Train RMSE: {np.mean(all_scores[m][\"Train RMSE\"])}')\n",
    "        print(f'{m} Average Test RMSE: {np.mean(all_scores[m][\"Test RMSE\"])}')\n",
    "        print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "        print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "        print()\n",
    "\n",
    "    with open('all_results/' + dataset + '_all_scores_' + outcome + '_NN.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 9.289588312408732\n",
      "Test RMSE: 11.770695179989234\n",
      "R2 Train: 0.34261951993788076\n",
      "R2 Test: 0.025702829818300188\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 9.289588312408732\n",
      "Test RMSE: 11.770695179989234\n",
      "R2 Train: 0.34261951993788076\n",
      "R2 Test: 0.025702829818300188\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 9.289588312408732\n",
      "Test RMSE: 11.770695179989234\n",
      "R2 Train: 0.34261951993788076\n",
      "R2 Test: 0.025702829818300188\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 9.289588312408732\n",
      "Test RMSE: 11.770695179989234\n",
      "R2 Train: 0.34261951993788076\n",
      "R2 Test: 0.025702829818300188\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 9.289588312408732\n",
      "Test RMSE: 11.770695179989234\n",
      "R2 Train: 0.34261951993788076\n",
      "R2 Test: 0.025702829818300188\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 10.83387344606651\n",
      "Test RMSE: 16.93232891143951\n",
      "R2 Train: 0.10588910893488712\n",
      "R2 Test: -1.0161396331627022\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 10.628074595879513\n",
      "Test RMSE: 13.536683173980881\n",
      "R2 Train: 0.13953530348901488\n",
      "R2 Test: -0.28858105994239525\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 10.748097700281301\n",
      "Test RMSE: 14.317720265814906\n",
      "R2 Train: 0.11999106895352063\n",
      "R2 Test: -0.44156742842780083\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 10.828728561681514\n",
      "Test RMSE: 14.080303205776731\n",
      "R2 Train: 0.10673811366392427\n",
      "R2 Test: -0.39415554024728827\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 10.6124918821359\n",
      "Test RMSE: 15.152612881973955\n",
      "R2 Train: 0.1420566530257238\n",
      "R2 Test: -0.6145900469735848\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 8.896050691458923\n",
      "Test RMSE: 12.388242863754998\n",
      "R2 Train: 0.39713736617153217\n",
      "R2 Test: -0.07921167908938043\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 8.730821950926599\n",
      "Test RMSE: 14.414691995385963\n",
      "R2 Train: 0.4193236583370128\n",
      "R2 Test: -0.4611605866578392\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 8.798133762585868\n",
      "Test RMSE: 11.548096373606601\n",
      "R2 Train: 0.4103354901158962\n",
      "R2 Test: 0.0622047815447937\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 8.822424409057676\n",
      "Test RMSE: 12.432297021796257\n",
      "R2 Train: 0.40707500213538805\n",
      "R2 Test: -0.08690095321950597\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 8.797169820926632\n",
      "Test RMSE: 15.176187054786917\n",
      "R2 Train: 0.4104646927544855\n",
      "R2 Test: -0.6196178575132614\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 9.0842796183837\n",
      "Test RMSE: 11.521387962056751\n",
      "R2 Train: 0.37135588283731247\n",
      "R2 Test: 0.06653762662814577\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 9.09520855102808\n",
      "Test RMSE: 11.38376209738752\n",
      "R2 Train: 0.36984237989387636\n",
      "R2 Test: 0.08870531546787674\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 9.091345108629607\n",
      "Test RMSE: 11.343526300772961\n",
      "R2 Train: 0.37037762008685193\n",
      "R2 Test: 0.09513585543864178\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 9.075204567110607\n",
      "Test RMSE: 11.446230834974266\n",
      "R2 Train: 0.3726112664767963\n",
      "R2 Test: 0.07867635957198038\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 9.04291793012602\n",
      "Test RMSE: 11.36901674544344\n",
      "R2 Train: 0.377067417191996\n",
      "R2 Test: 0.0910645808930105\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 11.272661297498862\n",
      "Test RMSE: 12.754563767070655\n",
      "R2 Train: 0.031996817448404435\n",
      "R2 Test: -0.14398000497072427\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 11.264696728466655\n",
      "Test RMSE: 12.438894616283392\n",
      "R2 Train: 0.0333641972281391\n",
      "R2 Test: -0.0880548565514705\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 11.28493633287968\n",
      "Test RMSE: 12.846153841477092\n",
      "R2 Train: 0.029887512036292807\n",
      "R2 Test: -0.16046875620377432\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 11.28493633287968\n",
      "Test RMSE: 12.846153841477092\n",
      "R2 Train: 0.029887512036292807\n",
      "R2 Test: -0.16046875620377432\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 11.278475942929122\n",
      "Test RMSE: 12.866710668152328\n",
      "R2 Train: 0.03099793230502046\n",
      "R2 Test: -0.16418576638276483\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 9.289588312408732\n",
      "GensimLDA Average Test RMSE: 11.770695179989234\n",
      "GensimLDA Average R2 Train: 0.34261951993788076\n",
      "GensimLDA Average R2 Test: 0.025702829818300188\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 10.730253237208947\n",
      "Mallet_LDA Average Test RMSE: 14.803929687797197\n",
      "Mallet_LDA Average R2 Train: 0.12284204961341413\n",
      "Mallet_LDA Average R2 Test: -0.5510067417507543\n",
      "\n",
      "BERTopic Average Train RMSE: 8.80892012699114\n",
      "BERTopic Average Test RMSE: 13.191903061866148\n",
      "BERTopic Average R2 Train: 0.40886724190286294\n",
      "BERTopic Average R2 Test: -0.23693725898703866\n",
      "\n",
      "NMF Average Train RMSE: 9.077791155055603\n",
      "NMF Average Test RMSE: 11.412784788126988\n",
      "NMF Average R2 Train: 0.37225091329736654\n",
      "NMF Average R2 Test: 0.08402394759993104\n",
      "\n",
      "CTM Average Train RMSE: 11.2771413269308\n",
      "CTM Average Test RMSE: 12.750495346892112\n",
      "CTM Average R2 Train: 0.03122679421082992\n",
      "CTM Average R2 Test: -0.14343162806250165\n",
      "\n",
      "politics\n",
      "\n",
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.7077866886322155\n",
      "Test RMSE: 1.8881570632801639\n",
      "R2 Train: 0.07437007491315395\n",
      "R2 Test: -0.03847601561215841\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.7077866886322155\n",
      "Test RMSE: 1.8881570632801639\n",
      "R2 Train: 0.07437007491315395\n",
      "R2 Test: -0.03847601561215841\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.7077866886322155\n",
      "Test RMSE: 1.8881570632801639\n",
      "R2 Train: 0.07437007491315395\n",
      "R2 Test: -0.03847601561215841\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.7077866886322155\n",
      "Test RMSE: 1.8881570632801639\n",
      "R2 Train: 0.07437007491315395\n",
      "R2 Test: -0.03847601561215841\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.7077866886322155\n",
      "Test RMSE: 1.8881570632801639\n",
      "R2 Train: 0.07437007491315395\n",
      "R2 Test: -0.03847601561215841\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.7296078950449163\n",
      "Test RMSE: 2.0595278789145133\n",
      "R2 Train: 0.05056452138505452\n",
      "R2 Test: -0.2355365174255435\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.7078409189031205\n",
      "Test RMSE: 2.033357526280408\n",
      "R2 Train: 0.07431128776025298\n",
      "R2 Test: -0.20433617249289449\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.7222084044847785\n",
      "Test RMSE: 1.9504222732822385\n",
      "R2 Train: 0.058670764677206444\n",
      "R2 Test: -0.10809636746789963\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.7288482373656453\n",
      "Test RMSE: 2.0448920749192983\n",
      "R2 Train: 0.05139833761110246\n",
      "R2 Test: -0.21803850943402114\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.7243731653156715\n",
      "Test RMSE: 1.9967132394903588\n",
      "R2 Train: 0.05630283597912411\n",
      "R2 Test: -0.1613192654361404\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.689721940293994\n",
      "Test RMSE: 2.1311494896471777\n",
      "R2 Train: 0.0938488939358737\n",
      "R2 Test: -0.3229641153056262\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.6825783100919507\n",
      "Test RMSE: 2.399279968133248\n",
      "R2 Train: 0.1014945602163897\n",
      "R2 Test: -0.676803190462758\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.6845952468814718\n",
      "Test RMSE: 1.8813919252295401\n",
      "R2 Train: 0.09933915996938436\n",
      "R2 Test: -0.031047769393242053\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.697635500663769\n",
      "Test RMSE: 2.8349333785985147\n",
      "R2 Train: 0.08534137185564494\n",
      "R2 Test: -1.3410244537435103\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.6859579176805397\n",
      "Test RMSE: 1.9294509351308964\n",
      "R2 Train: 0.09788147971929251\n",
      "R2 Test: -0.08439551799791212\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.694800542412135\n",
      "Test RMSE: 1.8855116516919446\n",
      "R2 Train: 0.08839368069866627\n",
      "R2 Test: -0.03556813040206319\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.6929352405729559\n",
      "Test RMSE: 1.8669994428900878\n",
      "R2 Train: 0.09039920896896936\n",
      "R2 Test: -0.015333258445470621\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.6921889638600647\n",
      "Test RMSE: 1.870209583086706\n",
      "R2 Train: 0.091200969432502\n",
      "R2 Test: -0.018827811387695403\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.6941124988174159\n",
      "Test RMSE: 1.859219605777926\n",
      "R2 Train: 0.08913370591692771\n",
      "R2 Test: -0.0068890465773532306\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.6888870948562422\n",
      "Test RMSE: 1.8739257977213302\n",
      "R2 Train: 0.09474408173777904\n",
      "R2 Test: -0.022880773728672255\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Train RMSE: 1.7460082636873477\n",
      "Test RMSE: 1.913780314369136\n",
      "R2 Train: 0.032473813701889176\n",
      "R2 Test: -0.06685255476783292\n",
      "Run: 2\n",
      "\n",
      "Train RMSE: 1.7468618359167767\n",
      "Test RMSE: 1.934390988582335\n",
      "R2 Train: 0.031527592118839154\n",
      "R2 Test: -0.08995547245188606\n",
      "Run: 3\n",
      "\n",
      "Train RMSE: 1.7494897632047126\n",
      "Test RMSE: 1.927688168032365\n",
      "R2 Train: 0.02861151783780469\n",
      "R2 Test: -0.08241499234070693\n",
      "Run: 4\n",
      "\n",
      "Train RMSE: 1.7494897632047126\n",
      "Test RMSE: 1.927688168032365\n",
      "R2 Train: 0.02861151783780469\n",
      "R2 Test: -0.08241499234070693\n",
      "Run: 5\n",
      "\n",
      "Train RMSE: 1.735965919178292\n",
      "Test RMSE: 1.9328665560485419\n",
      "R2 Train: 0.04357145801262918\n",
      "R2 Test: -0.08823823030232547\n",
      "\n",
      "\n",
      "GensimLDA Average Train RMSE: 1.7077866886322155\n",
      "GensimLDA Average Test RMSE: 1.888157063280164\n",
      "GensimLDA Average R2 Train: 0.07437007491315395\n",
      "GensimLDA Average R2 Test: -0.03847601561215841\n",
      "\n",
      "Mallet_LDA Average Train RMSE: 1.7225757242228266\n",
      "Mallet_LDA Average Test RMSE: 2.0169825985773633\n",
      "Mallet_LDA Average R2 Train: 0.0582495494825481\n",
      "Mallet_LDA Average R2 Test: -0.18546536645129982\n",
      "\n",
      "BERTopic Average Train RMSE: 1.6880977831223452\n",
      "BERTopic Average Test RMSE: 2.2352411393478753\n",
      "BERTopic Average R2 Train: 0.09558109313931704\n",
      "BERTopic Average R2 Test: -0.4912470093806098\n",
      "\n",
      "NMF Average Train RMSE: 1.6925848681037627\n",
      "NMF Average Test RMSE: 1.8711732162335988\n",
      "NMF Average R2 Train: 0.09077432935096888\n",
      "NMF Average R2 Test: -0.01989980410825094\n",
      "\n",
      "CTM Average Train RMSE: 1.7455631090383683\n",
      "CTM Average Test RMSE: 1.9272828390129484\n",
      "CTM Average R2 Train: 0.03295917990179338\n",
      "CTM Average R2 Test: -0.08197524844069166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# change this\n",
    "dataset = 'twitter' # folder and dataset name\n",
    "\n",
    "# cols = ['gender', 'age', 'politics', 'age_group_label'] # outcome columns\n",
    "cols = ['age', 'politics'] # outcome columns\n",
    "\n",
    "for outcome in cols:\n",
    "    print(outcome)\n",
    "    print()\n",
    "\n",
    "    train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "    test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "    temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "    df = pd.read_csv(dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "\n",
    "    merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "    merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "    user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "    all_user_ids = list(user_ids.keys())\n",
    "\n",
    "    labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "    user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "    y = [] # the outcome we care about\n",
    "\n",
    "    for key in user_ids.keys():\n",
    "        y.append(labels_data[labels_data['user_id'] == key][outcome].iloc[0])\n",
    "\n",
    "    # REGRESSION\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for model in ['GensimLDA', 'Mallet_LDA', 'BERTopic', 'NMF', 'CTM']:\n",
    "        print(model)\n",
    "\n",
    "        train_error_list = []\n",
    "        test_error_list = []\n",
    "\n",
    "        r2s_train = []\n",
    "        r2s_test = []\n",
    "\n",
    "        for run in range(1, 6):\n",
    "            print(\"Run: \" + str(run))\n",
    "            print()\n",
    "\n",
    "            topics = []\n",
    "            with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    topic_list = [item.strip() for item in row if item.strip()]\n",
    "                    topics.append(topic_list)\n",
    "\n",
    "            X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "            # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "            X_train = X[:round(0.80 * len(X))]\n",
    "            X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "            y_train = y[:round(0.80 * len(X))]\n",
    "            y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            X_test = np.array(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "            y_pred_train = lr.predict(X_train)\n",
    "            y_pred_test = lr.predict(X_test)\n",
    "\n",
    "            train_error = np.sqrt(mean_squared_error(y_pred_train, y_train))\n",
    "            test_error = np.sqrt(mean_squared_error(y_pred_test, y_test))\n",
    "\n",
    "            train_error_list.append(train_error)\n",
    "            test_error_list.append(test_error)\n",
    "\n",
    "            print(f'Train RMSE: {train_error}')\n",
    "            print(f'Test RMSE: {test_error}')\n",
    "\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "            r2s_train.append(r2_train)\n",
    "            r2s_test.append(r2_test)\n",
    "            \n",
    "            print(f'R2 Train: {r2_train}')\n",
    "            print(f'R2 Test: {r2_test}')\n",
    "\n",
    "            all_scores[model] = {\n",
    "                'Train RMSE': train_error_list,\n",
    "                'Test RMSE': test_error_list,\n",
    "                'R2 Train': r2s_train,\n",
    "                'R2 Test': r2s_test,\n",
    "\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': y_pred_train,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_pred_test\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    for m in all_scores.keys():\n",
    "        print(f'{m} Average Train RMSE: {np.mean(all_scores[m][\"Train RMSE\"])}')\n",
    "        print(f'{m} Average Test RMSE: {np.mean(all_scores[m][\"Test RMSE\"])}')\n",
    "        print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "        print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "        print()\n",
    "\n",
    "    with open('all_results/' + dataset + '_all_scores_' + outcome + '.pkl', 'wb') as f:\n",
    "        pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GensimLDA\n",
      "Run: 1\n",
      "\n",
      "Epoch [10/500], Loss: 0.6517\n",
      "Epoch [20/500], Loss: 0.6454\n",
      "Epoch [30/500], Loss: 0.6408\n",
      "Epoch [40/500], Loss: 0.6371\n",
      "Epoch [50/500], Loss: 0.6338\n",
      "Epoch [60/500], Loss: 0.6308\n",
      "Epoch [70/500], Loss: 0.6281\n",
      "Epoch [80/500], Loss: 0.6255\n",
      "Epoch [90/500], Loss: 0.6230\n",
      "Epoch [100/500], Loss: 0.6206\n",
      "Epoch [110/500], Loss: 0.6183\n",
      "Epoch [120/500], Loss: 0.6161\n",
      "Epoch [130/500], Loss: 0.6140\n",
      "Epoch [140/500], Loss: 0.6119\n",
      "Epoch [150/500], Loss: 0.6098\n",
      "Epoch [160/500], Loss: 0.6079\n",
      "Epoch [170/500], Loss: 0.6060\n",
      "Epoch [180/500], Loss: 0.6041\n",
      "Epoch [190/500], Loss: 0.6023\n",
      "Epoch [200/500], Loss: 0.6005\n",
      "Epoch [210/500], Loss: 0.5988\n",
      "Epoch [220/500], Loss: 0.5971\n",
      "Epoch [230/500], Loss: 0.5954\n",
      "Epoch [240/500], Loss: 0.5938\n",
      "Epoch [250/500], Loss: 0.5923\n",
      "Epoch [260/500], Loss: 0.5907\n",
      "Epoch [270/500], Loss: 0.5893\n",
      "Epoch [280/500], Loss: 0.5878\n",
      "Epoch [290/500], Loss: 0.5864\n",
      "Epoch [300/500], Loss: 0.5850\n",
      "Epoch [310/500], Loss: 0.5836\n",
      "Epoch [320/500], Loss: 0.5823\n",
      "Epoch [330/500], Loss: 0.5810\n",
      "Epoch [340/500], Loss: 0.5797\n",
      "Epoch [350/500], Loss: 0.5785\n",
      "Epoch [360/500], Loss: 0.5773\n",
      "Epoch [370/500], Loss: 0.5761\n",
      "Epoch [380/500], Loss: 0.5749\n",
      "Epoch [390/500], Loss: 0.5738\n",
      "Epoch [400/500], Loss: 0.5727\n",
      "Epoch [410/500], Loss: 0.5716\n",
      "Epoch [420/500], Loss: 0.5705\n",
      "Epoch [430/500], Loss: 0.5694\n",
      "Epoch [440/500], Loss: 0.5684\n",
      "Epoch [450/500], Loss: 0.5674\n",
      "Epoch [460/500], Loss: 0.5664\n",
      "Epoch [470/500], Loss: 0.5655\n",
      "Epoch [480/500], Loss: 0.5645\n",
      "Epoch [490/500], Loss: 0.5636\n",
      "Epoch [500/500], Loss: 0.5627\n",
      "Train Error: 0.2758007117437722\n",
      "Test Error: 0.30156472261735423\n",
      "\n",
      "R2 Train: -0.21780117477061522\n",
      "R2 Test: -0.38573686657368644\n",
      "Run: 2\n",
      "\n",
      "Epoch [10/500], Loss: 0.6517\n",
      "Epoch [20/500], Loss: 0.6453\n",
      "Epoch [30/500], Loss: 0.6408\n",
      "Epoch [40/500], Loss: 0.6371\n",
      "Epoch [50/500], Loss: 0.6338\n",
      "Epoch [60/500], Loss: 0.6308\n",
      "Epoch [70/500], Loss: 0.6281\n",
      "Epoch [80/500], Loss: 0.6255\n",
      "Epoch [90/500], Loss: 0.6230\n",
      "Epoch [100/500], Loss: 0.6206\n",
      "Epoch [110/500], Loss: 0.6183\n",
      "Epoch [120/500], Loss: 0.6161\n",
      "Epoch [130/500], Loss: 0.6140\n",
      "Epoch [140/500], Loss: 0.6119\n",
      "Epoch [150/500], Loss: 0.6098\n",
      "Epoch [160/500], Loss: 0.6079\n",
      "Epoch [170/500], Loss: 0.6060\n",
      "Epoch [180/500], Loss: 0.6041\n",
      "Epoch [190/500], Loss: 0.6023\n",
      "Epoch [200/500], Loss: 0.6005\n",
      "Epoch [210/500], Loss: 0.5988\n",
      "Epoch [220/500], Loss: 0.5971\n",
      "Epoch [230/500], Loss: 0.5954\n",
      "Epoch [240/500], Loss: 0.5938\n",
      "Epoch [250/500], Loss: 0.5923\n",
      "Epoch [260/500], Loss: 0.5907\n",
      "Epoch [270/500], Loss: 0.5893\n",
      "Epoch [280/500], Loss: 0.5878\n",
      "Epoch [290/500], Loss: 0.5864\n",
      "Epoch [300/500], Loss: 0.5850\n",
      "Epoch [310/500], Loss: 0.5836\n",
      "Epoch [320/500], Loss: 0.5823\n",
      "Epoch [330/500], Loss: 0.5810\n",
      "Epoch [340/500], Loss: 0.5797\n",
      "Epoch [350/500], Loss: 0.5785\n",
      "Epoch [360/500], Loss: 0.5773\n",
      "Epoch [370/500], Loss: 0.5761\n",
      "Epoch [380/500], Loss: 0.5749\n",
      "Epoch [390/500], Loss: 0.5738\n",
      "Epoch [400/500], Loss: 0.5727\n",
      "Epoch [410/500], Loss: 0.5716\n",
      "Epoch [420/500], Loss: 0.5705\n",
      "Epoch [430/500], Loss: 0.5695\n",
      "Epoch [440/500], Loss: 0.5684\n",
      "Epoch [450/500], Loss: 0.5674\n",
      "Epoch [460/500], Loss: 0.5664\n",
      "Epoch [470/500], Loss: 0.5655\n",
      "Epoch [480/500], Loss: 0.5645\n",
      "Epoch [490/500], Loss: 0.5636\n",
      "Epoch [500/500], Loss: 0.5627\n",
      "Train Error: 0.2758007117437722\n",
      "Test Error: 0.3029871977240398\n",
      "\n",
      "R2 Train: -0.21780117477061522\n",
      "R2 Test: -0.3922733612273359\n",
      "Run: 3\n",
      "\n",
      "Epoch [10/500], Loss: 0.6518\n",
      "Epoch [20/500], Loss: 0.6454\n",
      "Epoch [30/500], Loss: 0.6408\n",
      "Epoch [40/500], Loss: 0.6371\n",
      "Epoch [50/500], Loss: 0.6338\n",
      "Epoch [60/500], Loss: 0.6309\n",
      "Epoch [70/500], Loss: 0.6281\n",
      "Epoch [80/500], Loss: 0.6255\n",
      "Epoch [90/500], Loss: 0.6230\n",
      "Epoch [100/500], Loss: 0.6206\n",
      "Epoch [110/500], Loss: 0.6183\n",
      "Epoch [120/500], Loss: 0.6161\n",
      "Epoch [130/500], Loss: 0.6140\n",
      "Epoch [140/500], Loss: 0.6119\n",
      "Epoch [150/500], Loss: 0.6099\n",
      "Epoch [160/500], Loss: 0.6079\n",
      "Epoch [170/500], Loss: 0.6060\n",
      "Epoch [180/500], Loss: 0.6041\n",
      "Epoch [190/500], Loss: 0.6023\n",
      "Epoch [200/500], Loss: 0.6005\n",
      "Epoch [210/500], Loss: 0.5988\n",
      "Epoch [220/500], Loss: 0.5971\n",
      "Epoch [230/500], Loss: 0.5954\n",
      "Epoch [240/500], Loss: 0.5938\n",
      "Epoch [250/500], Loss: 0.5923\n",
      "Epoch [260/500], Loss: 0.5907\n",
      "Epoch [270/500], Loss: 0.5893\n",
      "Epoch [280/500], Loss: 0.5878\n",
      "Epoch [290/500], Loss: 0.5864\n",
      "Epoch [300/500], Loss: 0.5850\n",
      "Epoch [310/500], Loss: 0.5836\n",
      "Epoch [320/500], Loss: 0.5823\n",
      "Epoch [330/500], Loss: 0.5810\n",
      "Epoch [340/500], Loss: 0.5797\n",
      "Epoch [350/500], Loss: 0.5785\n",
      "Epoch [360/500], Loss: 0.5773\n",
      "Epoch [370/500], Loss: 0.5761\n",
      "Epoch [380/500], Loss: 0.5749\n",
      "Epoch [390/500], Loss: 0.5738\n",
      "Epoch [400/500], Loss: 0.5727\n",
      "Epoch [410/500], Loss: 0.5716\n",
      "Epoch [420/500], Loss: 0.5705\n",
      "Epoch [430/500], Loss: 0.5695\n",
      "Epoch [440/500], Loss: 0.5684\n",
      "Epoch [450/500], Loss: 0.5674\n",
      "Epoch [460/500], Loss: 0.5664\n",
      "Epoch [470/500], Loss: 0.5655\n",
      "Epoch [480/500], Loss: 0.5645\n",
      "Epoch [490/500], Loss: 0.5636\n",
      "Epoch [500/500], Loss: 0.5627\n",
      "Train Error: 0.2758007117437722\n",
      "Test Error: 0.30156472261735423\n",
      "\n",
      "R2 Train: -0.21780117477061522\n",
      "R2 Test: -0.38573686657368644\n",
      "Run: 4\n",
      "\n",
      "Epoch [10/500], Loss: 0.6517\n",
      "Epoch [20/500], Loss: 0.6454\n",
      "Epoch [30/500], Loss: 0.6408\n",
      "Epoch [40/500], Loss: 0.6371\n",
      "Epoch [50/500], Loss: 0.6338\n",
      "Epoch [60/500], Loss: 0.6308\n",
      "Epoch [70/500], Loss: 0.6281\n",
      "Epoch [80/500], Loss: 0.6255\n",
      "Epoch [90/500], Loss: 0.6230\n",
      "Epoch [100/500], Loss: 0.6206\n",
      "Epoch [110/500], Loss: 0.6183\n",
      "Epoch [120/500], Loss: 0.6161\n",
      "Epoch [130/500], Loss: 0.6140\n",
      "Epoch [140/500], Loss: 0.6119\n",
      "Epoch [150/500], Loss: 0.6098\n",
      "Epoch [160/500], Loss: 0.6079\n",
      "Epoch [170/500], Loss: 0.6059\n",
      "Epoch [180/500], Loss: 0.6041\n",
      "Epoch [190/500], Loss: 0.6023\n",
      "Epoch [200/500], Loss: 0.6005\n",
      "Epoch [210/500], Loss: 0.5988\n",
      "Epoch [220/500], Loss: 0.5971\n",
      "Epoch [230/500], Loss: 0.5954\n",
      "Epoch [240/500], Loss: 0.5938\n",
      "Epoch [250/500], Loss: 0.5923\n",
      "Epoch [260/500], Loss: 0.5907\n",
      "Epoch [270/500], Loss: 0.5892\n",
      "Epoch [280/500], Loss: 0.5878\n",
      "Epoch [290/500], Loss: 0.5864\n",
      "Epoch [300/500], Loss: 0.5850\n",
      "Epoch [310/500], Loss: 0.5836\n",
      "Epoch [320/500], Loss: 0.5823\n",
      "Epoch [330/500], Loss: 0.5810\n",
      "Epoch [340/500], Loss: 0.5797\n",
      "Epoch [350/500], Loss: 0.5785\n",
      "Epoch [360/500], Loss: 0.5773\n",
      "Epoch [370/500], Loss: 0.5761\n",
      "Epoch [380/500], Loss: 0.5749\n",
      "Epoch [390/500], Loss: 0.5738\n",
      "Epoch [400/500], Loss: 0.5727\n",
      "Epoch [410/500], Loss: 0.5716\n",
      "Epoch [420/500], Loss: 0.5705\n",
      "Epoch [430/500], Loss: 0.5694\n",
      "Epoch [440/500], Loss: 0.5684\n",
      "Epoch [450/500], Loss: 0.5674\n",
      "Epoch [460/500], Loss: 0.5664\n",
      "Epoch [470/500], Loss: 0.5655\n",
      "Epoch [480/500], Loss: 0.5645\n",
      "Epoch [490/500], Loss: 0.5636\n",
      "Epoch [500/500], Loss: 0.5627\n",
      "Train Error: 0.2758007117437722\n",
      "Test Error: 0.30156472261735423\n",
      "\n",
      "R2 Train: -0.21780117477061522\n",
      "R2 Test: -0.38573686657368644\n",
      "Run: 5\n",
      "\n",
      "Epoch [10/500], Loss: 0.6517\n",
      "Epoch [20/500], Loss: 0.6454\n",
      "Epoch [30/500], Loss: 0.6408\n",
      "Epoch [40/500], Loss: 0.6371\n",
      "Epoch [50/500], Loss: 0.6338\n",
      "Epoch [60/500], Loss: 0.6308\n",
      "Epoch [70/500], Loss: 0.6281\n",
      "Epoch [80/500], Loss: 0.6255\n",
      "Epoch [90/500], Loss: 0.6230\n",
      "Epoch [100/500], Loss: 0.6206\n",
      "Epoch [110/500], Loss: 0.6183\n",
      "Epoch [120/500], Loss: 0.6161\n",
      "Epoch [130/500], Loss: 0.6140\n",
      "Epoch [140/500], Loss: 0.6119\n",
      "Epoch [150/500], Loss: 0.6098\n",
      "Epoch [160/500], Loss: 0.6079\n",
      "Epoch [170/500], Loss: 0.6059\n",
      "Epoch [180/500], Loss: 0.6041\n",
      "Epoch [190/500], Loss: 0.6023\n",
      "Epoch [200/500], Loss: 0.6005\n",
      "Epoch [210/500], Loss: 0.5988\n",
      "Epoch [220/500], Loss: 0.5971\n",
      "Epoch [230/500], Loss: 0.5954\n",
      "Epoch [240/500], Loss: 0.5938\n",
      "Epoch [250/500], Loss: 0.5923\n",
      "Epoch [260/500], Loss: 0.5907\n",
      "Epoch [270/500], Loss: 0.5892\n",
      "Epoch [280/500], Loss: 0.5878\n",
      "Epoch [290/500], Loss: 0.5864\n",
      "Epoch [300/500], Loss: 0.5850\n",
      "Epoch [310/500], Loss: 0.5836\n",
      "Epoch [320/500], Loss: 0.5823\n",
      "Epoch [330/500], Loss: 0.5810\n",
      "Epoch [340/500], Loss: 0.5797\n",
      "Epoch [350/500], Loss: 0.5785\n",
      "Epoch [360/500], Loss: 0.5773\n",
      "Epoch [370/500], Loss: 0.5761\n",
      "Epoch [380/500], Loss: 0.5749\n",
      "Epoch [390/500], Loss: 0.5738\n",
      "Epoch [400/500], Loss: 0.5727\n",
      "Epoch [410/500], Loss: 0.5716\n",
      "Epoch [420/500], Loss: 0.5705\n",
      "Epoch [430/500], Loss: 0.5694\n",
      "Epoch [440/500], Loss: 0.5684\n",
      "Epoch [450/500], Loss: 0.5674\n",
      "Epoch [460/500], Loss: 0.5664\n",
      "Epoch [470/500], Loss: 0.5655\n",
      "Epoch [480/500], Loss: 0.5645\n",
      "Epoch [490/500], Loss: 0.5636\n",
      "Epoch [500/500], Loss: 0.5627\n",
      "Train Error: 0.2758007117437722\n",
      "Test Error: 0.3029871977240398\n",
      "\n",
      "R2 Train: -0.21780117477061522\n",
      "R2 Test: -0.3922733612273359\n",
      "\n",
      "Mallet_LDA\n",
      "Run: 1\n",
      "\n",
      "Epoch [10/500], Loss: 0.6498\n",
      "Epoch [20/500], Loss: 0.6399\n",
      "Epoch [30/500], Loss: 0.6314\n",
      "Epoch [40/500], Loss: 0.6240\n",
      "Epoch [50/500], Loss: 0.6175\n",
      "Epoch [60/500], Loss: 0.6118\n",
      "Epoch [70/500], Loss: 0.6067\n",
      "Epoch [80/500], Loss: 0.6021\n",
      "Epoch [90/500], Loss: 0.5979\n",
      "Epoch [100/500], Loss: 0.5942\n",
      "Epoch [110/500], Loss: 0.5909\n",
      "Epoch [120/500], Loss: 0.5878\n",
      "Epoch [130/500], Loss: 0.5851\n",
      "Epoch [140/500], Loss: 0.5826\n",
      "Epoch [150/500], Loss: 0.5803\n",
      "Epoch [160/500], Loss: 0.5782\n",
      "Epoch [170/500], Loss: 0.5763\n",
      "Epoch [180/500], Loss: 0.5746\n",
      "Epoch [190/500], Loss: 0.5730\n",
      "Early stopping at epoch 195 due to minimal loss improvement.\n",
      "Train Error: 0.31957295373665484\n",
      "Test Error: 0.32005689900426737\n",
      "\n",
      "R2 Train: -0.4110780063793711\n",
      "R2 Test: -0.4707112970711296\n",
      "Run: 2\n",
      "\n",
      "Epoch [10/500], Loss: 0.6516\n",
      "Epoch [20/500], Loss: 0.6416\n",
      "Epoch [30/500], Loss: 0.6337\n",
      "Epoch [40/500], Loss: 0.6274\n",
      "Epoch [50/500], Loss: 0.6224\n",
      "Epoch [60/500], Loss: 0.6182\n",
      "Epoch [70/500], Loss: 0.6147\n",
      "Epoch [80/500], Loss: 0.6118\n",
      "Epoch [90/500], Loss: 0.6092\n",
      "Epoch [100/500], Loss: 0.6070\n",
      "Epoch [110/500], Loss: 0.6051\n",
      "Epoch [120/500], Loss: 0.6035\n",
      "Epoch [130/500], Loss: 0.6021\n",
      "Epoch [140/500], Loss: 0.6008\n",
      "Epoch [150/500], Loss: 0.5997\n",
      "Epoch [160/500], Loss: 0.5988\n",
      "Epoch [170/500], Loss: 0.5979\n",
      "Epoch [180/500], Loss: 0.5972\n",
      "Epoch [190/500], Loss: 0.5965\n",
      "Epoch [200/500], Loss: 0.5959\n",
      "Epoch [210/500], Loss: 0.5954\n",
      "Early stopping at epoch 211 due to minimal loss improvement.\n",
      "Train Error: 0.3209964412811388\n",
      "Test Error: 0.3186344238975818\n",
      "\n",
      "R2 Train: -0.4173634317975419\n",
      "R2 Test: -0.4641748024174801\n",
      "Run: 3\n",
      "\n",
      "Epoch [10/500], Loss: 0.6582\n",
      "Epoch [20/500], Loss: 0.6513\n",
      "Epoch [30/500], Loss: 0.6451\n",
      "Epoch [40/500], Loss: 0.6397\n",
      "Epoch [50/500], Loss: 0.6352\n",
      "Epoch [60/500], Loss: 0.6313\n",
      "Epoch [70/500], Loss: 0.6281\n",
      "Epoch [80/500], Loss: 0.6253\n",
      "Epoch [90/500], Loss: 0.6229\n",
      "Epoch [100/500], Loss: 0.6208\n",
      "Epoch [110/500], Loss: 0.6189\n",
      "Epoch [120/500], Loss: 0.6173\n",
      "Epoch [130/500], Loss: 0.6158\n",
      "Epoch [140/500], Loss: 0.6144\n",
      "Epoch [150/500], Loss: 0.6131\n",
      "Epoch [160/500], Loss: 0.6119\n",
      "Epoch [170/500], Loss: 0.6108\n",
      "Epoch [180/500], Loss: 0.6097\n",
      "Epoch [190/500], Loss: 0.6087\n",
      "Epoch [200/500], Loss: 0.6078\n",
      "Early stopping at epoch 201 due to minimal loss improvement.\n",
      "Train Error: 0.3188612099644128\n",
      "Test Error: 0.3271692745376956\n",
      "\n",
      "R2 Train: -0.4079352936702856\n",
      "R2 Test: -0.5033937703393769\n",
      "Run: 4\n",
      "\n",
      "Epoch [10/500], Loss: 0.6615\n",
      "Epoch [20/500], Loss: 0.6571\n",
      "Epoch [30/500], Loss: 0.6524\n",
      "Epoch [40/500], Loss: 0.6479\n",
      "Epoch [50/500], Loss: 0.6438\n",
      "Epoch [60/500], Loss: 0.6400\n",
      "Epoch [70/500], Loss: 0.6365\n",
      "Epoch [80/500], Loss: 0.6332\n",
      "Epoch [90/500], Loss: 0.6303\n",
      "Epoch [100/500], Loss: 0.6275\n",
      "Epoch [110/500], Loss: 0.6250\n",
      "Epoch [120/500], Loss: 0.6227\n",
      "Epoch [130/500], Loss: 0.6206\n",
      "Epoch [140/500], Loss: 0.6186\n",
      "Epoch [150/500], Loss: 0.6168\n",
      "Epoch [160/500], Loss: 0.6152\n",
      "Epoch [170/500], Loss: 0.6136\n",
      "Epoch [180/500], Loss: 0.6122\n",
      "Epoch [190/500], Loss: 0.6109\n",
      "Epoch [200/500], Loss: 0.6097\n",
      "Epoch [210/500], Loss: 0.6086\n",
      "Early stopping at epoch 217 due to minimal loss improvement.\n",
      "Train Error: 0.3042704626334519\n",
      "Test Error: 0.3257467994310099\n",
      "\n",
      "R2 Train: -0.3435096831340336\n",
      "R2 Test: -0.49685727568572746\n",
      "Run: 5\n",
      "\n",
      "Epoch [10/500], Loss: 0.6604\n",
      "Epoch [20/500], Loss: 0.6555\n",
      "Epoch [30/500], Loss: 0.6507\n",
      "Epoch [40/500], Loss: 0.6464\n",
      "Epoch [50/500], Loss: 0.6426\n",
      "Epoch [60/500], Loss: 0.6394\n",
      "Epoch [70/500], Loss: 0.6365\n",
      "Epoch [80/500], Loss: 0.6340\n",
      "Epoch [90/500], Loss: 0.6317\n",
      "Epoch [100/500], Loss: 0.6298\n",
      "Epoch [110/500], Loss: 0.6280\n",
      "Epoch [120/500], Loss: 0.6265\n",
      "Epoch [130/500], Loss: 0.6251\n",
      "Epoch [140/500], Loss: 0.6239\n",
      "Epoch [150/500], Loss: 0.6228\n",
      "Epoch [160/500], Loss: 0.6219\n",
      "Epoch [170/500], Loss: 0.6210\n",
      "Epoch [180/500], Loss: 0.6203\n",
      "Early stopping at epoch 189 due to minimal loss improvement.\n",
      "Train Error: 0.32135231316725976\n",
      "Test Error: 0.32005689900426737\n",
      "\n",
      "R2 Train: -0.4189347881520846\n",
      "R2 Test: -0.4707112970711296\n",
      "\n",
      "BERTopic\n",
      "Run: 1\n",
      "\n",
      "Epoch [10/500], Loss: 0.6586\n",
      "Epoch [20/500], Loss: 0.6563\n",
      "Epoch [30/500], Loss: 0.6533\n",
      "Epoch [40/500], Loss: 0.6501\n",
      "Epoch [50/500], Loss: 0.6470\n",
      "Epoch [60/500], Loss: 0.6439\n",
      "Epoch [70/500], Loss: 0.6411\n",
      "Epoch [80/500], Loss: 0.6384\n",
      "Epoch [90/500], Loss: 0.6358\n",
      "Epoch [100/500], Loss: 0.6334\n",
      "Epoch [110/500], Loss: 0.6312\n",
      "Epoch [120/500], Loss: 0.6290\n",
      "Epoch [130/500], Loss: 0.6270\n",
      "Epoch [140/500], Loss: 0.6252\n",
      "Epoch [150/500], Loss: 0.6234\n",
      "Epoch [160/500], Loss: 0.6217\n",
      "Epoch [170/500], Loss: 0.6201\n",
      "Epoch [180/500], Loss: 0.6186\n",
      "Epoch [190/500], Loss: 0.6172\n",
      "Epoch [200/500], Loss: 0.6158\n",
      "Epoch [210/500], Loss: 0.6145\n",
      "Epoch [220/500], Loss: 0.6133\n",
      "Epoch [230/500], Loss: 0.6121\n",
      "Epoch [240/500], Loss: 0.6110\n",
      "Epoch [250/500], Loss: 0.6099\n",
      "Epoch [260/500], Loss: 0.6089\n",
      "Epoch [270/500], Loss: 0.6079\n",
      "Epoch [280/500], Loss: 0.6070\n",
      "Epoch [290/500], Loss: 0.6061\n",
      "Epoch [300/500], Loss: 0.6052\n",
      "Epoch [310/500], Loss: 0.6044\n",
      "Epoch [320/500], Loss: 0.6035\n",
      "Epoch [330/500], Loss: 0.6027\n",
      "Epoch [340/500], Loss: 0.6020\n",
      "Epoch [350/500], Loss: 0.6012\n",
      "Epoch [360/500], Loss: 0.6005\n",
      "Epoch [370/500], Loss: 0.5998\n",
      "Epoch [380/500], Loss: 0.5992\n",
      "Epoch [390/500], Loss: 0.5985\n",
      "Epoch [400/500], Loss: 0.5979\n",
      "Epoch [410/500], Loss: 0.5972\n",
      "Epoch [420/500], Loss: 0.5966\n",
      "Epoch [430/500], Loss: 0.5960\n",
      "Epoch [440/500], Loss: 0.5955\n",
      "Epoch [450/500], Loss: 0.5949\n",
      "Epoch [460/500], Loss: 0.5944\n",
      "Epoch [470/500], Loss: 0.5938\n",
      "Epoch [480/500], Loss: 0.5933\n",
      "Epoch [490/500], Loss: 0.5928\n",
      "Epoch [500/500], Loss: 0.5922\n",
      "Train Error: 0.30818505338078295\n",
      "Test Error: 0.2930298719772404\n",
      "\n",
      "R2 Train: -0.3607946030340037\n",
      "R2 Test: -0.34651789865178984\n",
      "Run: 2\n",
      "\n",
      "Epoch [10/500], Loss: 0.6597\n",
      "Epoch [20/500], Loss: 0.6582\n",
      "Epoch [30/500], Loss: 0.6559\n",
      "Epoch [40/500], Loss: 0.6533\n",
      "Epoch [50/500], Loss: 0.6506\n",
      "Epoch [60/500], Loss: 0.6479\n",
      "Epoch [70/500], Loss: 0.6454\n",
      "Epoch [80/500], Loss: 0.6430\n",
      "Epoch [90/500], Loss: 0.6407\n",
      "Epoch [100/500], Loss: 0.6385\n",
      "Epoch [110/500], Loss: 0.6364\n",
      "Epoch [120/500], Loss: 0.6344\n",
      "Epoch [130/500], Loss: 0.6325\n",
      "Epoch [140/500], Loss: 0.6307\n",
      "Epoch [150/500], Loss: 0.6290\n",
      "Epoch [160/500], Loss: 0.6274\n",
      "Epoch [170/500], Loss: 0.6258\n",
      "Epoch [180/500], Loss: 0.6243\n",
      "Epoch [190/500], Loss: 0.6229\n",
      "Epoch [200/500], Loss: 0.6215\n",
      "Epoch [210/500], Loss: 0.6202\n",
      "Epoch [220/500], Loss: 0.6189\n",
      "Epoch [230/500], Loss: 0.6177\n",
      "Epoch [240/500], Loss: 0.6165\n",
      "Epoch [250/500], Loss: 0.6154\n",
      "Epoch [260/500], Loss: 0.6143\n",
      "Epoch [270/500], Loss: 0.6132\n",
      "Epoch [280/500], Loss: 0.6122\n",
      "Epoch [290/500], Loss: 0.6112\n",
      "Epoch [300/500], Loss: 0.6103\n",
      "Epoch [310/500], Loss: 0.6093\n",
      "Epoch [320/500], Loss: 0.6084\n",
      "Epoch [330/500], Loss: 0.6076\n",
      "Epoch [340/500], Loss: 0.6067\n",
      "Epoch [350/500], Loss: 0.6059\n",
      "Epoch [360/500], Loss: 0.6051\n",
      "Epoch [370/500], Loss: 0.6043\n",
      "Epoch [380/500], Loss: 0.6035\n",
      "Epoch [390/500], Loss: 0.6028\n",
      "Epoch [400/500], Loss: 0.6020\n",
      "Epoch [410/500], Loss: 0.6013\n",
      "Epoch [420/500], Loss: 0.6006\n",
      "Epoch [430/500], Loss: 0.5999\n",
      "Epoch [440/500], Loss: 0.5992\n",
      "Epoch [450/500], Loss: 0.5986\n",
      "Epoch [460/500], Loss: 0.5979\n",
      "Epoch [470/500], Loss: 0.5973\n",
      "Early stopping at epoch 477 due to minimal loss improvement.\n",
      "Train Error: 0.31850533807829184\n",
      "Test Error: 0.29729729729729726\n",
      "\n",
      "R2 Train: -0.40636393731574283\n",
      "R2 Test: -0.36612738261273803\n",
      "Run: 3\n",
      "\n",
      "Epoch [10/500], Loss: 0.6608\n",
      "Epoch [20/500], Loss: 0.6592\n",
      "Epoch [30/500], Loss: 0.6566\n",
      "Epoch [40/500], Loss: 0.6539\n",
      "Epoch [50/500], Loss: 0.6515\n",
      "Epoch [60/500], Loss: 0.6493\n",
      "Epoch [70/500], Loss: 0.6473\n",
      "Epoch [80/500], Loss: 0.6456\n",
      "Epoch [90/500], Loss: 0.6440\n",
      "Epoch [100/500], Loss: 0.6426\n",
      "Epoch [110/500], Loss: 0.6414\n",
      "Epoch [120/500], Loss: 0.6402\n",
      "Epoch [130/500], Loss: 0.6392\n",
      "Epoch [140/500], Loss: 0.6382\n",
      "Epoch [150/500], Loss: 0.6373\n",
      "Epoch [160/500], Loss: 0.6364\n",
      "Epoch [170/500], Loss: 0.6356\n",
      "Epoch [180/500], Loss: 0.6348\n",
      "Epoch [190/500], Loss: 0.6341\n",
      "Epoch [200/500], Loss: 0.6333\n",
      "Epoch [210/500], Loss: 0.6326\n",
      "Epoch [220/500], Loss: 0.6320\n",
      "Epoch [230/500], Loss: 0.6313\n",
      "Epoch [240/500], Loss: 0.6306\n",
      "Epoch [250/500], Loss: 0.6300\n",
      "Epoch [260/500], Loss: 0.6293\n",
      "Epoch [270/500], Loss: 0.6287\n",
      "Epoch [280/500], Loss: 0.6281\n",
      "Epoch [290/500], Loss: 0.6275\n",
      "Epoch [300/500], Loss: 0.6268\n",
      "Epoch [310/500], Loss: 0.6262\n",
      "Epoch [320/500], Loss: 0.6256\n",
      "Epoch [330/500], Loss: 0.6250\n",
      "Epoch [340/500], Loss: 0.6244\n",
      "Epoch [350/500], Loss: 0.6237\n",
      "Epoch [360/500], Loss: 0.6231\n",
      "Epoch [370/500], Loss: 0.6225\n",
      "Epoch [380/500], Loss: 0.6219\n",
      "Epoch [390/500], Loss: 0.6212\n",
      "Epoch [400/500], Loss: 0.6206\n",
      "Epoch [410/500], Loss: 0.6200\n",
      "Epoch [420/500], Loss: 0.6194\n",
      "Early stopping at epoch 424 due to minimal loss improvement.\n",
      "Train Error: 0.3145907473309608\n",
      "Test Error: 0.3029871977240398\n",
      "\n",
      "R2 Train: -0.3890790174157728\n",
      "R2 Test: -0.3922733612273359\n",
      "Run: 4\n",
      "\n",
      "Epoch [10/500], Loss: 0.6592\n",
      "Epoch [20/500], Loss: 0.6574\n",
      "Epoch [30/500], Loss: 0.6551\n",
      "Epoch [40/500], Loss: 0.6525\n",
      "Epoch [50/500], Loss: 0.6500\n",
      "Epoch [60/500], Loss: 0.6476\n",
      "Epoch [70/500], Loss: 0.6453\n",
      "Epoch [80/500], Loss: 0.6431\n",
      "Epoch [90/500], Loss: 0.6411\n",
      "Epoch [100/500], Loss: 0.6391\n",
      "Epoch [110/500], Loss: 0.6373\n",
      "Epoch [120/500], Loss: 0.6356\n",
      "Epoch [130/500], Loss: 0.6340\n",
      "Epoch [140/500], Loss: 0.6325\n",
      "Epoch [150/500], Loss: 0.6310\n",
      "Epoch [160/500], Loss: 0.6297\n",
      "Epoch [170/500], Loss: 0.6283\n",
      "Epoch [180/500], Loss: 0.6271\n",
      "Epoch [190/500], Loss: 0.6258\n",
      "Epoch [200/500], Loss: 0.6247\n",
      "Epoch [210/500], Loss: 0.6235\n",
      "Epoch [220/500], Loss: 0.6225\n",
      "Epoch [230/500], Loss: 0.6214\n",
      "Epoch [240/500], Loss: 0.6204\n",
      "Epoch [250/500], Loss: 0.6194\n",
      "Epoch [260/500], Loss: 0.6184\n",
      "Epoch [270/500], Loss: 0.6175\n",
      "Epoch [280/500], Loss: 0.6165\n",
      "Epoch [290/500], Loss: 0.6156\n",
      "Epoch [300/500], Loss: 0.6147\n",
      "Epoch [310/500], Loss: 0.6139\n",
      "Epoch [320/500], Loss: 0.6130\n",
      "Epoch [330/500], Loss: 0.6122\n",
      "Epoch [340/500], Loss: 0.6114\n",
      "Epoch [350/500], Loss: 0.6106\n",
      "Epoch [360/500], Loss: 0.6098\n",
      "Epoch [370/500], Loss: 0.6090\n",
      "Epoch [380/500], Loss: 0.6082\n",
      "Epoch [390/500], Loss: 0.6074\n",
      "Epoch [400/500], Loss: 0.6067\n",
      "Epoch [410/500], Loss: 0.6059\n",
      "Epoch [420/500], Loss: 0.6052\n",
      "Epoch [430/500], Loss: 0.6044\n",
      "Early stopping at epoch 438 due to minimal loss improvement.\n",
      "Train Error: 0.32348754448398576\n",
      "Test Error: 0.30867709815078237\n",
      "\n",
      "R2 Train: -0.42836292627934114\n",
      "R2 Test: -0.4184193398419338\n",
      "Run: 5\n",
      "\n",
      "Epoch [10/500], Loss: 0.6600\n",
      "Epoch [20/500], Loss: 0.6588\n",
      "Epoch [30/500], Loss: 0.6570\n",
      "Epoch [40/500], Loss: 0.6548\n",
      "Epoch [50/500], Loss: 0.6526\n",
      "Epoch [60/500], Loss: 0.6505\n",
      "Epoch [70/500], Loss: 0.6485\n",
      "Epoch [80/500], Loss: 0.6466\n",
      "Epoch [90/500], Loss: 0.6448\n",
      "Epoch [100/500], Loss: 0.6431\n",
      "Epoch [110/500], Loss: 0.6415\n",
      "Epoch [120/500], Loss: 0.6400\n",
      "Epoch [130/500], Loss: 0.6386\n",
      "Epoch [140/500], Loss: 0.6373\n",
      "Epoch [150/500], Loss: 0.6360\n",
      "Epoch [160/500], Loss: 0.6349\n",
      "Epoch [170/500], Loss: 0.6338\n",
      "Epoch [180/500], Loss: 0.6328\n",
      "Epoch [190/500], Loss: 0.6318\n",
      "Epoch [200/500], Loss: 0.6309\n",
      "Epoch [210/500], Loss: 0.6301\n",
      "Epoch [220/500], Loss: 0.6293\n",
      "Epoch [230/500], Loss: 0.6285\n",
      "Epoch [240/500], Loss: 0.6278\n",
      "Epoch [250/500], Loss: 0.6271\n",
      "Epoch [260/500], Loss: 0.6265\n",
      "Epoch [270/500], Loss: 0.6259\n",
      "Epoch [280/500], Loss: 0.6253\n",
      "Epoch [290/500], Loss: 0.6247\n",
      "Epoch [300/500], Loss: 0.6242\n",
      "Epoch [310/500], Loss: 0.6237\n",
      "Epoch [320/500], Loss: 0.6233\n",
      "Epoch [330/500], Loss: 0.6228\n",
      "Epoch [340/500], Loss: 0.6224\n",
      "Epoch [350/500], Loss: 0.6220\n",
      "Epoch [360/500], Loss: 0.6216\n",
      "Epoch [370/500], Loss: 0.6212\n",
      "Epoch [380/500], Loss: 0.6208\n",
      "Epoch [390/500], Loss: 0.6205\n",
      "Epoch [400/500], Loss: 0.6201\n",
      "Epoch [410/500], Loss: 0.6198\n",
      "Epoch [420/500], Loss: 0.6195\n",
      "Epoch [430/500], Loss: 0.6191\n",
      "Epoch [440/500], Loss: 0.6188\n",
      "Early stopping at epoch 441 due to minimal loss improvement.\n",
      "Train Error: 0.32882562277580074\n",
      "Test Error: 0.2958748221906117\n",
      "\n",
      "R2 Train: -0.451933271597482\n",
      "R2 Test: -0.35959088795908856\n",
      "\n",
      "NMF\n",
      "Run: 1\n",
      "\n",
      "Epoch [10/500], Loss: 0.5302\n",
      "Epoch [20/500], Loss: 0.5248\n",
      "Epoch [30/500], Loss: 0.5234\n",
      "Epoch [40/500], Loss: 0.5225\n",
      "Epoch [50/500], Loss: 0.5220\n",
      "Epoch [60/500], Loss: 0.5217\n",
      "Epoch [70/500], Loss: 0.5216\n",
      "Early stopping at epoch 79 due to minimal loss improvement.\n",
      "Train Error: 0.22206405693950182\n",
      "Test Error: 0.2446657183499289\n",
      "\n",
      "R2 Train: 0.01947363476533681\n",
      "R2 Test: -0.12427708042770802\n",
      "Run: 2\n",
      "\n",
      "Epoch [10/500], Loss: 0.5236\n",
      "Epoch [20/500], Loss: 0.5185\n",
      "Epoch [30/500], Loss: 0.5161\n",
      "Epoch [40/500], Loss: 0.5144\n",
      "Epoch [50/500], Loss: 0.5132\n",
      "Epoch [60/500], Loss: 0.5124\n",
      "Epoch [70/500], Loss: 0.5119\n",
      "Epoch [80/500], Loss: 0.5117\n",
      "Early stopping at epoch 82 due to minimal loss improvement.\n",
      "Train Error: 0.22455516014234878\n",
      "Test Error: 0.2560455192034139\n",
      "\n",
      "R2 Train: 0.008474140283537768\n",
      "R2 Test: -0.17656903765690357\n",
      "Run: 3\n",
      "\n",
      "Epoch [10/500], Loss: 0.5331\n",
      "Epoch [20/500], Loss: 0.5273\n",
      "Epoch [30/500], Loss: 0.5243\n",
      "Epoch [40/500], Loss: 0.5223\n",
      "Epoch [50/500], Loss: 0.5209\n",
      "Epoch [60/500], Loss: 0.5199\n",
      "Epoch [70/500], Loss: 0.5193\n",
      "Epoch [80/500], Loss: 0.5189\n",
      "Early stopping at epoch 82 due to minimal loss improvement.\n",
      "Train Error: 0.22206405693950182\n",
      "Test Error: 0.2588904694167852\n",
      "\n",
      "R2 Train: 0.01947363476533681\n",
      "R2 Test: -0.18964202696420251\n",
      "Run: 4\n",
      "\n",
      "Epoch [10/500], Loss: 0.5123\n",
      "Epoch [20/500], Loss: 0.5106\n",
      "Epoch [30/500], Loss: 0.5128\n",
      "Epoch [40/500], Loss: 0.5145\n",
      "Epoch [50/500], Loss: 0.5156\n",
      "Epoch [60/500], Loss: 0.5163\n",
      "Epoch [70/500], Loss: 0.5167\n",
      "Early stopping at epoch 80 due to minimal loss improvement.\n",
      "Train Error: 0.22455516014234878\n",
      "Test Error: 0.23755334281650076\n",
      "\n",
      "R2 Train: 0.008474140283537768\n",
      "R2 Test: -0.09159460715946066\n",
      "Run: 5\n",
      "\n",
      "Epoch [10/500], Loss: 0.5285\n",
      "Epoch [20/500], Loss: 0.5284\n",
      "Epoch [30/500], Loss: 0.5274\n",
      "Epoch [40/500], Loss: 0.5257\n",
      "Epoch [50/500], Loss: 0.5242\n",
      "Epoch [60/500], Loss: 0.5230\n",
      "Epoch [70/500], Loss: 0.5221\n",
      "Early stopping at epoch 80 due to minimal loss improvement.\n",
      "Train Error: 0.22099644128113882\n",
      "Test Error: 0.2446657183499289\n",
      "\n",
      "R2 Train: 0.024187703828965068\n",
      "R2 Test: -0.12427708042770802\n",
      "\n",
      "CTM\n",
      "Run: 1\n",
      "\n",
      "Epoch [10/500], Loss: 0.6515\n",
      "Epoch [20/500], Loss: 0.6476\n",
      "Epoch [30/500], Loss: 0.6444\n",
      "Epoch [40/500], Loss: 0.6419\n",
      "Epoch [50/500], Loss: 0.6398\n",
      "Early stopping at epoch 52 due to minimal loss improvement.\n",
      "Train Error: 0.3448398576512456\n",
      "Test Error: 0.3172119487908962\n",
      "\n",
      "R2 Train: -0.5226443075519047\n",
      "R2 Test: -0.45763830776383063\n",
      "Run: 2\n",
      "\n",
      "Epoch [10/500], Loss: 0.6499\n",
      "Epoch [20/500], Loss: 0.6454\n",
      "Epoch [30/500], Loss: 0.6424\n",
      "Epoch [40/500], Loss: 0.6403\n",
      "Epoch [50/500], Loss: 0.6387\n",
      "Epoch [60/500], Loss: 0.6375\n",
      "Epoch [70/500], Loss: 0.6365\n",
      "Early stopping at epoch 72 due to minimal loss improvement.\n",
      "Train Error: 0.3455516014234875\n",
      "Test Error: 0.3143669985775249\n",
      "\n",
      "R2 Train: -0.5257870202609902\n",
      "R2 Test: -0.4445653184565317\n",
      "Run: 3\n",
      "\n",
      "Epoch [10/500], Loss: 0.6552\n",
      "Epoch [20/500], Loss: 0.6545\n",
      "Epoch [30/500], Loss: 0.6541\n",
      "Epoch [40/500], Loss: 0.6539\n",
      "Epoch [50/500], Loss: 0.6539\n",
      "Early stopping at epoch 52 due to minimal loss improvement.\n",
      "Train Error: 0.3448398576512456\n",
      "Test Error: 0.3186344238975818\n",
      "\n",
      "R2 Train: -0.5226443075519047\n",
      "R2 Test: -0.4641748024174801\n",
      "Run: 4\n",
      "\n",
      "Epoch [10/500], Loss: 0.6553\n",
      "Epoch [20/500], Loss: 0.6546\n",
      "Epoch [30/500], Loss: 0.6542\n",
      "Epoch [40/500], Loss: 0.6540\n",
      "Epoch [50/500], Loss: 0.6539\n",
      "Early stopping at epoch 52 due to minimal loss improvement.\n",
      "Train Error: 0.3448398576512456\n",
      "Test Error: 0.3186344238975818\n",
      "\n",
      "R2 Train: -0.5226443075519047\n",
      "R2 Test: -0.4641748024174801\n",
      "Run: 5\n",
      "\n",
      "Epoch [10/500], Loss: 0.6543\n",
      "Epoch [20/500], Loss: 0.6527\n",
      "Epoch [30/500], Loss: 0.6514\n",
      "Epoch [40/500], Loss: 0.6503\n",
      "Epoch [50/500], Loss: 0.6495\n",
      "Early stopping at epoch 52 due to minimal loss improvement.\n",
      "Train Error: 0.3459074733096086\n",
      "Test Error: 0.32005689900426737\n",
      "\n",
      "R2 Train: -0.527358376615533\n",
      "R2 Test: -0.4707112970711296\n",
      "\n",
      "\n",
      "GensimLDA Average Train Error: 0.2758007117437722\n",
      "GensimLDA Average Test Error: 0.3021337126600285\n",
      "GensimLDA Average R2 Train: -0.21780117477061522\n",
      "GensimLDA Average R2 Test: -0.38835146443514623\n",
      "\n",
      "Mallet_LDA Average Train Error: 0.3170106761565836\n",
      "Mallet_LDA Average Test Error: 0.3223328591749644\n",
      "Mallet_LDA Average R2 Train: -0.3997642406266634\n",
      "Mallet_LDA Average R2 Test: -0.48116968851696873\n",
      "\n",
      "BERTopic Average Train Error: 0.31871886120996445\n",
      "BERTopic Average Test Error: 0.2995732574679943\n",
      "BERTopic Average R2 Train: -0.4073067511284685\n",
      "BERTopic Average R2 Test: -0.37658577405857724\n",
      "\n",
      "NMF Average Train Error: 0.22284697508896797\n",
      "NMF Average Test Error: 0.2483641536273115\n",
      "NMF Average R2 Train: 0.016016650785342847\n",
      "NMF Average R2 Test: -0.14127196652719656\n",
      "\n",
      "CTM Average Train Error: 0.34519572953736655\n",
      "CTM Average Test Error: 0.31778093883357045\n",
      "CTM Average R2 Train: -0.5242156639064475\n",
      "CTM Average R2 Test: -0.4602529056252904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# change this\n",
    "dataset = 'twitter' # folder and dataset name\n",
    "cols = ['gender'] # outcome columns\n",
    "outcome = 'gender'\n",
    "buckets = 4\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "train = pickle.load(open(dataset + '/BERTopic/run_1/train.pkl', 'rb'))\n",
    "test = pickle.load(open(dataset + '/BERTopic/run_1/test.pkl', 'rb'))\n",
    "\n",
    "temp = pd.concat([train, test]).reset_index(drop=True) # concatenating train and test datasets\n",
    "df = pd.read_csv(dataset + '.csv') # loading in actual dataset\n",
    "\n",
    "# df['age_group'] = pd.qcut(df['age'], q=buckets)\n",
    "# df['age_group'] = df['age_group'].astype(str)\n",
    "# df['age_group_bucket'] = df['age_group'].apply(lambda x: 'age_' + x.replace('(', '').replace(']', '').replace(', ', '-'))\n",
    "\n",
    "# df['age_group_label'] = label_encoder.fit_transform(df['age_group_bucket'])\n",
    "# df['politics'] = np.round(df['politics'])\n",
    "# df['politics'] = label_encoder.fit_transform(df['politics'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "\n",
    "merged = pd.merge(temp, df, how='inner', left_on = 'message_id', right_on = 'Unnamed: 0')[['message_id_x', 'user_id', 'message_x'] + cols]\n",
    "merged.columns = ['message_id', 'user_id', 'message'] + cols\n",
    "\n",
    "user_ids = {user_id : user_data.index.tolist() for user_id, user_data in merged.groupby('user_id')} # gets all users and their corresponding indices in the df\n",
    "all_user_ids = list(user_ids.keys())\n",
    "\n",
    "labels_data = merged[['user_id'] + cols] # df of user_ids and all the labels to consider\n",
    "user_ids_avg_dict = {} # populated to be user_id : average distribution\n",
    "y = [] # the outcome we care about\n",
    "\n",
    "for key in user_ids.keys():\n",
    "    y.append(labels_data[labels_data['user_id'] == key][outcome].iloc[0])\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "for model in ['GensimLDA', 'Mallet_LDA', 'BERTopic', 'NMF', 'CTM']:\n",
    "    print(model)\n",
    "\n",
    "    train_error_list = []\n",
    "    test_error_list = []\n",
    "\n",
    "    r2s_train = []\n",
    "    r2s_test = []\n",
    "\n",
    "    if model == 'GensimLDA':\n",
    "        lr = 0.5\n",
    "    elif model == 'BERTopic':\n",
    "        lr = 0.05\n",
    "    elif model == 'NMF':\n",
    "        lr = 0.05\n",
    "    elif model == 'Mallet_LDA':\n",
    "        lr = 0.1\n",
    "    else:\n",
    "        lr = 0.1\n",
    "\n",
    "    for run in range(1, 6):\n",
    "        print(\"Run: \" + str(run))\n",
    "        print()\n",
    "\n",
    "        topics = []\n",
    "        with open(dataset + '/' + model + '/run_' + str(run) + '/' + 'topics_100.txt', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                topic_list = [item.strip() for item in row if item.strip()]\n",
    "                topics.append(topic_list)\n",
    "\n",
    "        X = pickle.load(open(dataset + '/' + model + '/run_' + str(run) + '/' + model + '_avg_topic_distribution.pkl', 'rb'))\n",
    "\n",
    "        # # 80-20 split --> didn't use train-test-split function since its already shuffled\n",
    "        X_train = X[:round(0.80 * len(X))]\n",
    "        X_test = X[round(0.80 * len(X)):]\n",
    "\n",
    "        y_train = y[:round(0.80 * len(X))]\n",
    "        y_test = y[round(0.80 * len(X)):]\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        X_test = np.array(X_test)\n",
    "        y_train = np.array(y_train)\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "        train_users = all_user_ids[:round(0.80 * len(X))]\n",
    "        test_users = all_user_ids[round(0.80 * len(X)):]\n",
    "\n",
    "        # Convert arrays to torch tensors\n",
    "        X_train_tensor = torch.tensor(np.array(X_train).astype(np.float32))\n",
    "        y_train_tensor = torch.tensor(np.array(y_train).astype(np.longlong))  # Use long for classification\n",
    "        X_test_tensor = torch.tensor(np.array(X_test).astype(np.float32))\n",
    "        y_test_tensor = torch.tensor(np.array(y_test).astype(np.longlong))\n",
    "\n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Define the model\n",
    "        class LogisticRegressionModel(nn.Module):\n",
    "            def __init__(self, input_size, num_classes):\n",
    "                super(LogisticRegressionModel, self).__init__()\n",
    "                self.layer1 = nn.Linear(input_size, num_classes)\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.layer1(x)\n",
    "\n",
    "        input_size = X_train.shape[1]\n",
    "        num_classes = len(np.unique(y_train))  # Assuming y_train contains all classes\n",
    "        logit_model = LogisticRegressionModel(input_size, num_classes)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()  # This includes softmax\n",
    "        optimizer = optim.Adam(logit_model.parameters(), lr=lr)\n",
    "\n",
    "        loss_values = []\n",
    "        early_stopping_triggered = False\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = logit_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item() * inputs.size(0) \n",
    "            \n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            loss_values.append(epoch_loss)\n",
    "\n",
    "            if epoch >= 50:\n",
    "                # Calculate the percentage change in loss\n",
    "                loss_change = (loss_values[epoch - 50] - loss_values[epoch]) / loss_values[epoch - 50]\n",
    "                \n",
    "                # If change in loss is less than 1%, stop training\n",
    "                if abs(loss_change) < 0.005:\n",
    "                    print(f'Early stopping at epoch {epoch+1} due to minimal loss improvement.')\n",
    "                    early_stopping_triggered = True\n",
    "                    break\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Predict on the test set\n",
    "        logit_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = logit_model(X_train_tensor)\n",
    "            _, predicted_train = torch.max(y_pred_train.data, 1)\n",
    "\n",
    "            y_pred_test = logit_model(X_test_tensor)\n",
    "            _, predicted_test = torch.max(y_pred_test.data, 1)\n",
    "\n",
    "\n",
    "        train_error = 1 - accuracy_score(predicted_train, y_train)\n",
    "        test_error = 1 - accuracy_score(predicted_test, y_test)\n",
    "\n",
    "        train_error_list.append(train_error)\n",
    "        test_error_list.append(test_error)\n",
    "        \n",
    "        print(f'Train Error: {train_error}')\n",
    "        print(f'Test Error: {test_error}')\n",
    "\n",
    "        print()\n",
    "\n",
    "        r2_train = r2_score(y_train, predicted_train)\n",
    "        r2_test = r2_score(y_test, predicted_test)\n",
    "\n",
    "        r2s_train.append(r2_train)\n",
    "        r2s_test.append(r2_test)\n",
    "        \n",
    "        print(f'R2 Train: {r2_train}')\n",
    "        print(f'R2 Test: {r2_test}')\n",
    "\n",
    "        all_scores[model] = {\n",
    "            'Train Error': train_error_list,\n",
    "            'Test Error': test_error_list,\n",
    "            'R2 Train': r2s_train,\n",
    "            'R2 Test': r2s_test,\n",
    "\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': predicted_train.numpy(),\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': predicted_test.numpy()\n",
    "        }\n",
    "\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "for m in all_scores.keys():\n",
    "    print(f'{m} Average Train Error: {np.mean(all_scores[m][\"Train Error\"])}')\n",
    "    print(f'{m} Average Test Error: {np.mean(all_scores[m][\"Test Error\"])}')\n",
    "    print(f'{m} Average R2 Train: {np.mean(all_scores[m][\"R2 Train\"])}')\n",
    "    print(f'{m} Average R2 Test: {np.mean(all_scores[m][\"R2 Test\"])}')\n",
    "    print()\n",
    "\n",
    "with open('all_results/' + dataset + '_all_scores_' + outcome + '.pkl', 'wb') as f:\n",
    "     pickle.dump(all_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
