{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dlatkInterface.py -d arnav -t yelp_reviews -c message_id --add_ngrams -n 1\n",
      "\n",
      "./dlatkInterface.py -d arnav -t yelp_reviews -c message_id -f 'feat$1gram$yelp_reviews$message_id' --estimate_lda_topics --lda_lexicon_name mtm_100 --mallet_path /home/sharath/mallet-2.0.8/bin/mallet --num_stopwords 100 --num_topics 100 --language en --save_lda_files yelp_reviews/Mallet_LDA\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import os\n",
    "import errno\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from gensim.utils import deaccent\n",
    "import warnings\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "import abc\n",
    "import re\n",
    "import itertools\n",
    "from itertools import islice\n",
    "\n",
    "# change these accordingly\n",
    "text_column = 'text'\n",
    "label_column = 'label'\n",
    "id_column = 'Unnamed: 0'\n",
    "dataset_name = 'yelp_reviews'\n",
    "folder = dataset_name + '/'\n",
    "dataset_path = '/sandata/arnav/' + dataset_name + '.csv'\n",
    "perc = 1 # % of dataset used\n",
    "logit = False\n",
    "\n",
    "df = pd.read_csv(dataset_path)[[text_column]]\n",
    "df = df.reset_index()\n",
    "df.columns = ['message_id', 'message']\n",
    "\n",
    "# db = sqlalchemy.engine.url.URL(drivername='mysql', host='127.0.0.1', database='arnav', query={'read_default_file': '~/.my.cnf', 'charset':'utf8mb4'})\n",
    "# engine = sqlalchemy.create_engine(db)\n",
    "# df.to_sql(dataset_name, engine, if_exists='replace', index=False) \n",
    "\n",
    "one_grams = \"./dlatkInterface.py -d arnav -t \" + dataset_name + \" -c message_id --add_ngrams -n 1\"\n",
    "\n",
    "print(one_grams)\n",
    "print()\n",
    "\n",
    "try:\n",
    "    os.makedirs(folder + 'Mallet_LDA')\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "      raise\n",
    "\n",
    "mallet_lda = \"\"\"./dlatkInterface.py -d arnav -t \"\"\" + dataset_name + \"\"\" -c message_id -f 'feat$1gram$\"\"\" + dataset_name + \"\"\"$message_id' --estimate_lda_topics --lda_lexicon_name mtm_100 --mallet_path /home/sharath/mallet-2.0.8/bin/mallet --num_stopwords 100 --num_topics 100 --language en --save_lda_files \"\"\" + dataset_name + \"\"\"/Mallet_LDA\"\"\"\n",
    "\n",
    "print(mallet_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/arnav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "is_multi = False\n",
    "is_combined = not is_multi\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "language = 'en'\n",
    "stopwords = list(stop_words.words('english'))\n",
    "\n",
    "num_topics_word = 500\n",
    "num_topics_list = [100]\n",
    "\n",
    "dataset = pd.read_csv(dataset_path)[[id_column, text_column, label_column]].rename(columns = {id_column: 'message_id', text_column: 'message', label_column: 'label'}).dropna()\n",
    "\n",
    "# shuffles to avoid biases in data\n",
    "arr = sklearn.utils.shuffle(np.arange(len(dataset)), random_state=42)[0:int(len(dataset) * perc)]\n",
    "dataset = dataset.iloc[arr].reset_index(drop=True)\n",
    "\n",
    "train, test, message, test_message = train_test_split(dataset, dataset['message'], test_size = 0.20, random_state = 42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "message.reset_index(drop=True, inplace=True)\n",
    "test_message.reset_index(drop=True, inplace=True)\n",
    "\n",
    "message = list(message)\n",
    "test_message = list(test_message)\n",
    "\n",
    "class WhiteSpacePreprocessingStopwords():\n",
    "    \"\"\"\n",
    "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents, stopwords_list=None, vocabulary_size=2000, max_df=1.0, min_words=1,\n",
    "                 remove_numbers=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param documents: list of strings\n",
    "        :param stopwords_list: list of the stopwords to remove\n",
    "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
    "        :param max_df : float or int, default=1.0\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly higher than the given threshold (corpus-specific\n",
    "        stop words).\n",
    "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
    "        documents, integer absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "        :param min_words: int, default=1. Documents with less words than the parameter\n",
    "        will be removed\n",
    "        :param remove_numbers: bool, default=True. If true, numbers are removed from docs\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        if stopwords_list is not None:\n",
    "            self.stopwords = set(stopwords_list)\n",
    "        else:\n",
    "            self.stopwords = []\n",
    "\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.max_df = max_df\n",
    "        self.min_words = min_words\n",
    "        self.remove_numbers = remove_numbers\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
    "        list of unpreprocessed documents.\n",
    "\n",
    "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
    "        \"\"\"\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        if self.remove_numbers:\n",
    "            preprocessed_docs_tmp = [doc.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, max_df=self.max_df)\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0 and len(doc) >= self.min_words:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "                retained_indices.append(i)\n",
    "\n",
    "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
    "\n",
    "def get_bag_of_words(data, min_length):\n",
    "    \"\"\"\n",
    "    Creates the bag of words\n",
    "    \"\"\"\n",
    "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
    "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
    "\n",
    "    vect = scipy.sparse.csr_matrix(vect)\n",
    "    return vect\n",
    "\n",
    "\n",
    "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from an input file, assumes one document per line\n",
    "    \"\"\"\n",
    "\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "\n",
    "    if max_seq_length is not None:\n",
    "        model.max_seq_length = max_seq_length\n",
    "\n",
    "    with open(text_file, encoding=\"utf-8\") as filino:\n",
    "        texts = list(map(lambda x: x, filino.readlines()))\n",
    "\n",
    "    check_max_local_length(max_seq_length, texts)\n",
    "\n",
    "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from a list\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "\n",
    "    if max_seq_length is not None:\n",
    "        model.max_seq_length = max_seq_length\n",
    "\n",
    "    check_max_local_length(max_seq_length, texts)\n",
    "\n",
    "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "def check_max_local_length(max_seq_length, texts):\n",
    "    max_local_length = np.max([len(t.split()) for t in texts])\n",
    "    if max_local_length > max_seq_length:\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "\n",
    "\n",
    "class TopicModelDataPreparation:\n",
    "\n",
    "    def __init__(self, contextualized_model=None, show_warning=True, max_seq_length=128):\n",
    "        self.contextualized_model = contextualized_model\n",
    "        self.vocab = []\n",
    "        self.id2token = {}\n",
    "        self.vectorizer = None\n",
    "        self.label_encoder = None\n",
    "        self.show_warning = show_warning\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def load(self, contextualized_embeddings, bow_embeddings, id2token, labels=None):\n",
    "        return CTMDataset(\n",
    "            X_contextual=contextualized_embeddings, X_bow=bow_embeddings, idx2token=id2token, labels=labels)\n",
    "\n",
    "    def fit(self, text_for_contextual, text_for_bow, labels=None, custom_embeddings=None):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if custom_embeddings is not None:\n",
    "            assert len(text_for_contextual) == len(custom_embeddings)\n",
    "\n",
    "            if text_for_bow is not None:\n",
    "                assert len(custom_embeddings) == len(text_for_bow)\n",
    "\n",
    "            if type(custom_embeddings).__module__ != 'numpy':\n",
    "                raise TypeError(\"contextualized_embeddings must be a numpy.ndarray type object\")\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            assert len(text_for_contextual) == len(text_for_bow)\n",
    "\n",
    "        if self.contextualized_model is None and custom_embeddings is None:\n",
    "            raise Exception(\"A contextualized model or contextualized embeddings must be defined\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "\n",
    "        # if the user is passing custom embeddings we don't need to create the embeddings using the model\n",
    "\n",
    "        if custom_embeddings is None:\n",
    "            train_contextualized_embeddings = bert_embeddings_from_list(\n",
    "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
    "        else:\n",
    "            train_contextualized_embeddings = custom_embeddings\n",
    "        self.vocab = self.vectorizer.get_feature_names_out()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        if labels:\n",
    "            self.label_encoder = OneHotEncoder()\n",
    "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        return CTMDataset(\n",
    "            X_contextual=train_contextualized_embeddings, X_bow=train_bow_embeddings,\n",
    "            idx2token=self.id2token, labels=encoded_labels)\n",
    "\n",
    "    def transform(self, text_for_contextual, text_for_bow=None, custom_embeddings=None, labels=None):\n",
    "        \"\"\"\n",
    "        This method create the input for the prediction. Essentially, it creates the embeddings with the contextualized\n",
    "        model of choice and with trained vectorizer.\n",
    "\n",
    "        If text_for_bow is missing, it should be because we are using ZeroShotTM\n",
    "\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if custom_embeddings is not None:\n",
    "            assert len(text_for_contextual) == len(custom_embeddings)\n",
    "\n",
    "            if text_for_bow is not None:\n",
    "                assert len(custom_embeddings) == len(text_for_bow)\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            assert len(text_for_contextual) == len(text_for_bow)\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
    "        else:\n",
    "            # dummy matrix\n",
    "            if self.show_warning:\n",
    "                warnings.simplefilter('always', DeprecationWarning)\n",
    "                warnings.warn(\n",
    "                    \"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n",
    "                    \"are using ZeroShotTM in a cross-lingual setting\")\n",
    "\n",
    "            # we just need an object that is matrix-like so that pytorch does not complain\n",
    "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
    "\n",
    "        if custom_embeddings is None:\n",
    "            test_contextualized_embeddings = bert_embeddings_from_list(\n",
    "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
    "        else:\n",
    "            test_contextualized_embeddings = custom_embeddings\n",
    "\n",
    "        if labels:\n",
    "            encoded_labels = self.label_encoder.transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "\n",
    "        return CTMDataset(X_contextual=test_contextualized_embeddings, X_bow=test_bow_embeddings,\n",
    "                          idx2token=self.id2token, labels=encoded_labels)\n",
    "\n",
    "documents = [line.strip() for line in (message + test_message) if not isinstance(line, float)]\n",
    "test_documents = [line.strip() for line in test_message if not isinstance(line, float)]\n",
    "\n",
    "sp_train = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp_train.preprocess()\n",
    "labels = pd.concat([train, test]).reset_index()['label'][retained_indices]\n",
    "\n",
    "sp_test = WhiteSpacePreprocessingStopwords(test_documents, stopwords_list=stopwords)\n",
    "test_preprocessed_documents, test_unpreprocessed_corpus, test_vocab, test_retained_indices = sp_test.preprocess()\n",
    "test_labels = test['label'][test_retained_indices]\n",
    "\n",
    "mallet_stopwords = []\n",
    "loc = \"mallet_stopwords.txt\"\n",
    "with open(loc) as f:\n",
    "  for line in f:\n",
    "    mallet_stopwords.append(line.strip())\n",
    "\n",
    "lda_loglik = []\n",
    "loc2 = \"dlatk/\" + dataset_name + \"/Mallet_LDA/lda.topicGivenWord.csv\" \n",
    "with open(loc2) as f:\n",
    "  for line in f:\n",
    "    temp = line.strip().split('\"')\n",
    "    if len(temp) > 1:\n",
    "      for i in range(len(temp)):\n",
    "        if i % 2 != 0:\n",
    "          temp[i] = (\"\".join(temp[i].split(\",\")))\n",
    "      temp = [\"\".join(temp)]\n",
    "    lda_loglik.append(temp[0].split(\",\")[1:])\n",
    "\n",
    "lda_loglik = lda_loglik[1:]\n",
    "\n",
    "mallet_lda_topics = [[x for i, x in enumerate(j) if i % 2 == 0] for j in lda_loglik]\n",
    "\n",
    "mallet_lda_loglik = []\n",
    "for k in lda_loglik:\n",
    "    mallet_lda_loglik.append({k[j]: float(k[j+1]) for j in range(0, len(k) - 1, 2)})\n",
    "\n",
    "mallet_lda_loglik.append({i: 1.0 for i in mallet_stopwords})\n",
    "mallet_lda_topics.append(mallet_stopwords)\n",
    "\n",
    "def take(n, iterable):\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "for i in range(len(mallet_lda_loglik)):\n",
    "  if len(mallet_lda_loglik[i]) > num_topics_word:\n",
    "    mallet_lda_loglik[i] = dict(take(num_topics_word, mallet_lda_loglik[i].items()))\n",
    "\n",
    "def preprocess(documents, topics, weights):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "        documents: list of documents (each element in the list is a string) that the topics were extracted from\n",
    "        topics: list of list of topics from the model of choice\n",
    "        weights: list of dictionary mappings (word: weight)\n",
    "\n",
    "    RETURN\n",
    "        topic distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize distribution matrix\n",
    "    distribution = np.zeros((len(documents), len(topics)))\n",
    "\n",
    "    for i, document in enumerate(documents):\n",
    "        # Preprocess document\n",
    "        document = document.translate(str.maketrans('', '', string.punctuation)) # removing periods, commas, etc\n",
    "        document = document.split(' ') # split on spaces\n",
    "        document = [word.lower() for word in document if len(word.lower()) > 0] # lower case everything since all topics are lower case\n",
    "\n",
    "        for j, loglik_dict in enumerate(weights):\n",
    "            distribution[i][j] = np.sum([0 if word not in loglik_dict else loglik_dict[word] for word in document]) # if word exists then its weight else 0\n",
    "\n",
    "        # Normalize\n",
    "        distribution[i] /= (len(document) + 2) # +2 for some weird reason ?\n",
    "        \n",
    "    return distribution\n",
    "\n",
    "\n",
    "def train_regression_model(X, y):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "        X: distribution from preprocess() above\n",
    "        y: labels\n",
    "\n",
    "    RETURN\n",
    "        LR model   \n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "        model: model trained from train_regression_model()\n",
    "        X_test: distribution you want to make predictions on\n",
    "        y_test: the true labels for the X_test passed in\n",
    "\n",
    "    RETURN\n",
    "        MSE: MSE of the (X_test, y_test) data inputted\n",
    "        RMSE: RMSE of the (X_test, y_test) data inputted\n",
    "        R2: R2 of the (X_test, y_test) data inputted\n",
    "        MAE: MAE of the (X_test, y_test) data inputted\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=True)\n",
    "    rmse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=False)\n",
    "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    return mse, rmse, r2, mae\n",
    "\n",
    "\n",
    "def results(topics, weights, flag=False):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "        topics: list of list of topics\n",
    "        weights: list of dictionary mappings (word: weight)\n",
    "        flag: True if baseline (default is False\n",
    "\n",
    "    RETURN\n",
    "        r2_train: R2 on train set\n",
    "        mae_train: MAE on train set\n",
    "        mse_train: MSE on train set\n",
    "        rmse_train: RMSE on train set\n",
    "        r2_test: R2 on test set\n",
    "        mae_test: MAE on test set\n",
    "        mse_test: MSE on test set\n",
    "        rmse_test: RMSE on test set\n",
    "    \"\"\"\n",
    "    X = preprocess([line.strip() for line in message if not isinstance(line, float)], topics, weights)\n",
    "    X_test = preprocess([line.strip() for line in test_message if not isinstance(line, float)], topics, weights)\n",
    "\n",
    "    y = train['label']\n",
    "    y_test = test['label']\n",
    "    model = train_regression_model(X, y)\n",
    "\n",
    "    mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, X_test, y_test)\n",
    "    mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, X, y)\n",
    "\n",
    "    return r2_train, mae_train, mse_train, rmse_train, r2_test, mae_test, mse_test, rmse_test, model\n",
    "\n",
    "def proportion_unique_words(topics, topk=10):\n",
    "    \"\"\"\n",
    "    compute the proportion of unique words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity will be computed\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than '+str(topk))\n",
    "    else:\n",
    "        unique_words = set()\n",
    "        for topic in topics:\n",
    "            unique_words = unique_words.union(set(topic[:topk]))\n",
    "        puw = len(unique_words) / (topk * len(topics))\n",
    "        return puw\n",
    "\n",
    "class Coherence(abc.ABC):\n",
    "    \"\"\"\n",
    "    :param topics: a list of lists of the top-k words\n",
    "    :param texts: (list of lists of strings) represents the corpus on which\n",
    "     the empirical frequencies of words are computed\n",
    "    \"\"\"\n",
    "    def __init__(self, topics, texts):\n",
    "        self.topics = topics\n",
    "        self.texts = texts\n",
    "        self.dictionary = Dictionary(self.texts)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def score(self):\n",
    "        pass\n",
    "\n",
    "class CoherenceNPMI(Coherence):\n",
    "    def __init__(self, topics, texts):\n",
    "        super().__init__(topics, texts)\n",
    "\n",
    "    def score(self, topk=10, per_topic=False):\n",
    "        \"\"\"\n",
    "        :param topk: how many most likely words to consider in the evaluation\n",
    "        :param per_topic: if True, returns the coherence value for each topic\n",
    "         (default: False)\n",
    "        :return: NPMI coherence\n",
    "        \"\"\"\n",
    "        if topk > len(self.topics[0]):\n",
    "            raise Exception('Words in topics are less than topk')\n",
    "        else:\n",
    "            npmi = CoherenceModel(\n",
    "                topics=self.topics, texts=self.texts,\n",
    "                dictionary=self.dictionary,\n",
    "                coherence='c_npmi', topn=topk)\n",
    "            if per_topic:\n",
    "                return npmi.get_coherence_per_topic()\n",
    "            else:\n",
    "                return npmi.get_coherence()\n",
    "\n",
    "hyperparams = {\n",
    "  'num_topics': num_topics_list,\n",
    "  'num_top_words': num_topics_word,\n",
    "  'percentage_of_dataset': perc,\n",
    "  'text_column': text_column,\n",
    "  'label_column': label_column,\n",
    "  'id_column': id_column,\n",
    "}\n",
    "\n",
    "# write hyperparams to file\n",
    "import json\n",
    "with open(folder + 'Mallet_LDA/hyperparams.txt', 'w') as f:\n",
    "  f.write(json.dumps(hyperparams))\n",
    "\n",
    "# write stopwords to file\n",
    "with open(folder + 'Mallet_LDA/stopwords.txt', 'w') as f:\n",
    "    for item in mallet_stopwords:\n",
    "        f.write(item + \", \")\n",
    "\n",
    "nmf_topic_diversity = proportion_unique_words(mallet_lda_topics)\n",
    "tokenizer = lambda s: re.findall( '\\w+', s.lower() )\n",
    "texts = [tokenizer(t) for t in  documents]\n",
    "nmf_coherence = CoherenceNPMI(texts=texts, topics=mallet_lda_topics).score()\n",
    "\n",
    "if logit:\n",
    "    X = preprocess([line.strip() for line in message if not isinstance(line, float)], mallet_lda_topics, mallet_lda_loglik)\n",
    "    X_test = preprocess([line.strip() for line in test_message if not isinstance(line, float)], mallet_lda_topics, mallet_lda_loglik)\n",
    "\n",
    "    dummy_train = pd.get_dummies(train['label'], dtype=int)\n",
    "    dummy_test = pd.get_dummies(test['label'], dtype=int)\n",
    "    dummy_train.columns = [\"label \" + str(i) for i in range(len(dummy_train.columns))]\n",
    "    dummy_test.columns = [\"label \" + str(i) for i in range(len(dummy_test.columns))]\n",
    "\n",
    "    accuracies = []\n",
    "    for i, label_col in enumerate(list(dummy_train)):\n",
    "        lr_model = LogisticRegression().fit(X, dummy_train[label_col])\n",
    "        accuracies.append(lr_model.score(X_test, dummy_test[label_col]))\n",
    "    \n",
    "    accuracies = [np.mean(accuracies), nmf_topic_diversity, nmf_coherence] + accuracies\n",
    "    final_df = pd.concat([pd.Series(accuracies)], axis = 1).T\n",
    "    final_df.columns = ['Average Accuracy', 'Topic Diversity', 'Topic Coherence'] + [i + str(\" Accuracy\") for i in dummy_train.columns]\n",
    "\n",
    "else:\n",
    "    lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test, lda_regression = results(mallet_lda_topics, mallet_lda_loglik)\n",
    "    final_df = pd.concat([pd.Series([lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test, nmf_topic_diversity, nmf_coherence])], axis = 1).T\n",
    "    final_df.columns = ['Train R2', 'Train MAE', 'Train MSE', 'Train RMSE', 'Test R2', 'Test MAE', 'Test MSE', 'Test RMSE', 'Topic Diversity', 'Topic Coherence']\n",
    "\n",
    "    with open(folder + 'Mallet_LDA/LR_coefficients_' + str(num_topics_list[0]) + '.txt', 'w') as f:\n",
    "        for item in lda_regression.coef_:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "final_df.to_csv(folder + 'Mallet_LDA/df_' + str(num_topics_list[0]) + '.csv')\n",
    "\n",
    "with open(folder + 'Mallet_LDA/topics_' + str(num_topics_list[0]) + '.txt', 'w') as f:\n",
    "    for sublist in mallet_lda_topics:\n",
    "        for i in sublist:\n",
    "            if i not in mallet_stopwords:\n",
    "                f.write(i + \", \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(dataset_name + '/Mallet_LDA/df_100.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlatk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
