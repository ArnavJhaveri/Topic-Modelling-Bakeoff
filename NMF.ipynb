{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BUw5EQi8v9r1",
        "outputId": "4adb927e-4343-4795-96fe-f410b213e7ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/arnav/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-3-49b7c32da5e1>:67: DtypeWarning: Columns (0,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataset = pd.read_csv(dataset_path)[[id_column, text_column, label_column]].rename(columns = {id_column: 'message_id', text_column: 'message', label_column: 'label'}).dropna()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start 100\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train R2</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Train RMSE</th>\n",
              "      <th>Test R2</th>\n",
              "      <th>Test MAE</th>\n",
              "      <th>Test MSE</th>\n",
              "      <th>Test RMSE</th>\n",
              "      <th>Topic Diversity</th>\n",
              "      <th>Topic Coherence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.054405</td>\n",
              "      <td>8.71233</td>\n",
              "      <td>121.828375</td>\n",
              "      <td>11.037589</td>\n",
              "      <td>0.050949</td>\n",
              "      <td>8.901022</td>\n",
              "      <td>126.492498</td>\n",
              "      <td>11.246888</td>\n",
              "      <td>0.254</td>\n",
              "      <td>-0.358062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train R2  Train MAE   Train MSE  Train RMSE   Test R2  Test MAE  \\\n",
              "0  0.054405    8.71233  121.828375   11.037589  0.050949  8.901022   \n",
              "\n",
              "     Test MSE  Test RMSE  Topic Diversity  Topic Coherence  \n",
              "0  126.492498  11.246888            0.254        -0.358062  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End 100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "1. Dataset passed in must be a CSV file that can be opened using pd.read_csv()\n",
        "2. perc determines how much of the dataset is used\n",
        "3. num_topics_word is set to 500 top words\n",
        "4. num_topics_list is set to [100] but can be changed or appended to, to get more results\n",
        "5. text_column, label_column, and id_column must all be set according to your database\n",
        "  5.1 text_column is the actual text\n",
        "  5.2 label_column is what you regress over / predict\n",
        "  5.3 id_column is the unique identifier for each message\n",
        "6. dataset_path must be set to where the dataset is stored\n",
        "7. the name of the folder (be sure to include '/' at the end) -- usually the name of the dataset (i.e. twitter, amazon, etc.)\n",
        "\"\"\"\n",
        "\n",
        "# !pip install pyldavis\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords as stop_words\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "from gensim.utils import deaccent\n",
        "import warnings\n",
        "import scipy.sparse\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "from scipy.spatial.distance import cosine\n",
        "import abc\n",
        "import re\n",
        "from contextualized_topic_models.evaluation.rbo import rbo\n",
        "import itertools\n",
        "\n",
        "is_multi = False\n",
        "is_combined = not is_multi\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "language = 'en'\n",
        "stopwords = list(stop_words.words('english'))\n",
        "\n",
        "num_topics_word = 500\n",
        "num_topics_list = [100]\n",
        "\n",
        "# change these accordingly\n",
        "text_column = 'message'\n",
        "label_column = 'age'\n",
        "id_column = 'Unnamed: 0'\n",
        "dataset_path = '/sandata/arnav/twitter_df.csv'\n",
        "folder = 'twitter/'\n",
        "perc = 0.01 # % of dataset used\n",
        "\n",
        "dataset = pd.read_csv(dataset_path)[[id_column, text_column, label_column]].rename(columns = {id_column: 'message_id', text_column: 'message', label_column: 'label'}).dropna()\n",
        "\n",
        "# shuffles to avoid biases in data\n",
        "arr = sklearn.utils.shuffle(np.arange(len(dataset)), random_state=42)[0:int(len(dataset) * perc)]\n",
        "dataset = dataset.iloc[arr].reset_index(drop=True)\n",
        "\n",
        "train, test, message, test_message = train_test_split(dataset, dataset['message'], test_size = 0.20, random_state = 42)\n",
        "\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)\n",
        "message.reset_index(drop=True, inplace=True)\n",
        "test_message.reset_index(drop=True, inplace=True)\n",
        "\n",
        "message = list(message)\n",
        "test_message = list(test_message)\n",
        "\n",
        "class WhiteSpacePreprocessingStopwords():\n",
        "    \"\"\"\n",
        "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, documents, stopwords_list=None, vocabulary_size=2000, max_df=1.0, min_words=1,\n",
        "                 remove_numbers=True):\n",
        "        \"\"\"\n",
        "\n",
        "        :param documents: list of strings\n",
        "        :param stopwords_list: list of the stopwords to remove\n",
        "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
        "        :param max_df : float or int, default=1.0\n",
        "        When building the vocabulary ignore terms that have a document\n",
        "        frequency strictly higher than the given threshold (corpus-specific\n",
        "        stop words).\n",
        "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
        "        documents, integer absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "        :param min_words: int, default=1. Documents with less words than the parameter\n",
        "        will be removed\n",
        "        :param remove_numbers: bool, default=True. If true, numbers are removed from docs\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        if stopwords_list is not None:\n",
        "            self.stopwords = set(stopwords_list)\n",
        "        else:\n",
        "            self.stopwords = []\n",
        "\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.max_df = max_df\n",
        "        self.min_words = min_words\n",
        "        self.remove_numbers = remove_numbers\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"\n",
        "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
        "        list of unpreprocessed documents.\n",
        "\n",
        "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
        "        \"\"\"\n",
        "        preprocessed_docs_tmp = self.documents\n",
        "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [doc.translate(\n",
        "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
        "        if self.remove_numbers:\n",
        "            preprocessed_docs_tmp = [doc.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
        "                                     for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, max_df=self.max_df)\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
        "\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0 and len(doc) >= self.min_words:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(self.documents[i])\n",
        "                retained_indices.append(i)\n",
        "\n",
        "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
        "\n",
        "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
        "\n",
        "def get_bag_of_words(data, min_length):\n",
        "    \"\"\"\n",
        "    Creates the bag of words\n",
        "    \"\"\"\n",
        "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
        "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
        "\n",
        "    vect = scipy.sparse.csr_matrix(vect)\n",
        "    return vect\n",
        "\n",
        "\n",
        "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from an input file, assumes one document per line\n",
        "    \"\"\"\n",
        "\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    with open(text_file, encoding=\"utf-8\") as filino:\n",
        "        texts = list(map(lambda x: x, filino.readlines()))\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from a list\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def check_max_local_length(max_seq_length, texts):\n",
        "    max_local_length = np.max([len(t.split()) for t in texts])\n",
        "    if max_local_length > max_seq_length:\n",
        "        warnings.simplefilter('always', DeprecationWarning)\n",
        "        warnings.warn(f\"the longest document in your collection has {max_local_length} words, the model instead \"\n",
        "                      f\"truncates to {max_seq_length} tokens.\")\n",
        "\n",
        "\n",
        "class TopicModelDataPreparation:\n",
        "\n",
        "    def __init__(self, contextualized_model=None, show_warning=True, max_seq_length=128):\n",
        "        self.contextualized_model = contextualized_model\n",
        "        self.vocab = []\n",
        "        self.id2token = {}\n",
        "        self.vectorizer = None\n",
        "        self.label_encoder = None\n",
        "        self.show_warning = show_warning\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def load(self, contextualized_embeddings, bow_embeddings, id2token, labels=None):\n",
        "        return CTMDataset(\n",
        "            X_contextual=contextualized_embeddings, X_bow=bow_embeddings, idx2token=id2token, labels=labels)\n",
        "\n",
        "    def fit(self, text_for_contextual, text_for_bow, labels=None, custom_embeddings=None):\n",
        "        \"\"\"\n",
        "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "            if type(custom_embeddings).__module__ != 'numpy':\n",
        "                raise TypeError(\"contextualized_embeddings must be a numpy.ndarray type object\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None and custom_embeddings is None:\n",
        "            raise Exception(\"A contextualized model or contextualized embeddings must be defined\")\n",
        "\n",
        "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
        "\n",
        "        # if the user is passing custom embeddings we don't need to create the embeddings using the model\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            train_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            train_contextualized_embeddings = custom_embeddings\n",
        "        self.vocab = self.vectorizer.get_feature_names_out()\n",
        "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
        "\n",
        "        if labels:\n",
        "            self.label_encoder = OneHotEncoder()\n",
        "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "        return CTMDataset(\n",
        "            X_contextual=train_contextualized_embeddings, X_bow=train_bow_embeddings,\n",
        "            idx2token=self.id2token, labels=encoded_labels)\n",
        "\n",
        "    def transform(self, text_for_contextual, text_for_bow=None, custom_embeddings=None, labels=None):\n",
        "        \"\"\"\n",
        "        This method create the input for the prediction. Essentially, it creates the embeddings with the contextualized\n",
        "        model of choice and with trained vectorizer.\n",
        "\n",
        "        If text_for_bow is missing, it should be because we are using ZeroShotTM\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None:\n",
        "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
        "        else:\n",
        "            # dummy matrix\n",
        "            if self.show_warning:\n",
        "                warnings.simplefilter('always', DeprecationWarning)\n",
        "                warnings.warn(\n",
        "                    \"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n",
        "                    \"are using ZeroShotTM in a cross-lingual setting\")\n",
        "\n",
        "            # we just need an object that is matrix-like so that pytorch does not complain\n",
        "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            test_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            test_contextualized_embeddings = custom_embeddings\n",
        "\n",
        "        if labels:\n",
        "            encoded_labels = self.label_encoder.transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "\n",
        "        return CTMDataset(X_contextual=test_contextualized_embeddings, X_bow=test_bow_embeddings,\n",
        "                          idx2token=self.id2token, labels=encoded_labels)\n",
        "\n",
        "documents = [line.strip() for line in (message + test_message) if not isinstance(line, float)]\n",
        "test_documents = [line.strip() for line in test_message if not isinstance(line, float)]\n",
        "\n",
        "sp_train = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
        "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp_train.preprocess()\n",
        "labels = pd.concat([train, test]).reset_index()['label'][retained_indices]\n",
        "\n",
        "sp_test = WhiteSpacePreprocessingStopwords(test_documents, stopwords_list=stopwords)\n",
        "test_preprocessed_documents, test_unpreprocessed_corpus, test_vocab, test_retained_indices = sp_test.preprocess()\n",
        "test_labels = test['label'][test_retained_indices]\n",
        "\n",
        "mallet_stopwords = []\n",
        "loc3 = \"dlatk/amazon_eng/9f822b_stopwords\"\n",
        "with open(loc3) as f:\n",
        "  for line in f:\n",
        "    mallet_stopwords.append(line.strip())\n",
        "\n",
        "def preprocess(documents, topics, weights):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        documents: list of documents (each element in the list is a string) that the topics were extracted from\n",
        "        topics: list of list of topics from the model of choice\n",
        "        weights: list of dictionary mappings (word: weight)\n",
        "\n",
        "    RETURN\n",
        "        topic distribution\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize distribution matrix\n",
        "    distribution = np.zeros((len(documents), len(topics)))\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        # Preprocess document\n",
        "        document = document.translate(str.maketrans('', '', string.punctuation)) # removing periods, commas, etc\n",
        "        document = document.split(' ') # split on spaces\n",
        "        document = [word.lower() for word in document if len(word.lower()) > 0] # lower case everything since all topics are lower case\n",
        "\n",
        "        for j, loglik_dict in enumerate(weights):\n",
        "            distribution[i][j] = np.sum([0 if word not in loglik_dict else loglik_dict[word] for word in document]) # if word exists then its weight else 0\n",
        "\n",
        "        # Normalize\n",
        "        distribution[i] /= (len(document) + 2) # +2 for some weird reason ?\n",
        "        \n",
        "    return distribution\n",
        "\n",
        "\n",
        "def train_regression_model(X, y):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        X: distribution from preprocess() above\n",
        "        y: labels\n",
        "\n",
        "    RETURN\n",
        "        LR model   \n",
        "    \"\"\"\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        model: model trained from train_regression_model()\n",
        "        X_test: distribution you want to make predictions on\n",
        "        y_test: the true labels for the X_test passed in\n",
        "\n",
        "    RETURN\n",
        "        MSE: MSE of the (X_test, y_test) data inputted\n",
        "        RMSE: RMSE of the (X_test, y_test) data inputted\n",
        "        R2: R2 of the (X_test, y_test) data inputted\n",
        "        MAE: MAE of the (X_test, y_test) data inputted\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=True)\n",
        "    rmse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=False)\n",
        "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
        "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "    return mse, rmse, r2, mae\n",
        "\n",
        "\n",
        "def results(topics, weights, flag=False):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        topics: list of list of topics\n",
        "        weights: list of dictionary mappings (word: weight)\n",
        "        flag: True if baseline (default is False\n",
        "\n",
        "    RETURN\n",
        "        r2_train: R2 on train set\n",
        "        mae_train: MAE on train set\n",
        "        mse_train: MSE on train set\n",
        "        rmse_train: RMSE on train set\n",
        "        r2_test: R2 on test set\n",
        "        mae_test: MAE on test set\n",
        "        mse_test: MSE on test set\n",
        "        rmse_test: RMSE on test set\n",
        "    \"\"\"\n",
        "\n",
        "    if flag:\n",
        "        X = np.array([labels.mean()] * len(labels)).reshape(-1, 1)\n",
        "        X_test = np.array([test_labels.mean()] * len(test_labels)).reshape(-1, 1)\n",
        "\n",
        "    else:\n",
        "        X = preprocess([line.strip() for line in message if not isinstance(line, float)], topics, weights)\n",
        "        X_test = preprocess([line.strip() for line in test_message if not isinstance(line, float)], topics, weights)\n",
        "\n",
        "    y = train['label']\n",
        "    y_test = test['label']\n",
        "    model = train_regression_model(X, y)\n",
        "\n",
        "    mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, X_test, y_test)\n",
        "    mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, X, y)\n",
        "\n",
        "    return r2_train, mae_train, mse_train, rmse_train, r2_test, mae_test, mse_test, rmse_test, model\n",
        "\n",
        "def proportion_unique_words(topics, topk=10):\n",
        "    \"\"\"\n",
        "    compute the proportion of unique words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    topk: top k words on which the topic diversity will be computed\n",
        "    \"\"\"\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than '+str(topk))\n",
        "    else:\n",
        "        unique_words = set()\n",
        "        for topic in topics:\n",
        "            unique_words = unique_words.union(set(topic[:topk]))\n",
        "        puw = len(unique_words) / (topk * len(topics))\n",
        "        return puw\n",
        "\n",
        "class Coherence(abc.ABC):\n",
        "    \"\"\"\n",
        "    :param topics: a list of lists of the top-k words\n",
        "    :param texts: (list of lists of strings) represents the corpus on which\n",
        "     the empirical frequencies of words are computed\n",
        "    \"\"\"\n",
        "    def __init__(self, topics, texts):\n",
        "        self.topics = topics\n",
        "        self.texts = texts\n",
        "        self.dictionary = Dictionary(self.texts)\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def score(self):\n",
        "        pass\n",
        "\n",
        "class CoherenceNPMI(Coherence):\n",
        "    def __init__(self, topics, texts):\n",
        "        super().__init__(topics, texts)\n",
        "\n",
        "    def score(self, topk=10, per_topic=False):\n",
        "        \"\"\"\n",
        "        :param topk: how many most likely words to consider in the evaluation\n",
        "        :param per_topic: if True, returns the coherence value for each topic\n",
        "         (default: False)\n",
        "        :return: NPMI coherence\n",
        "        \"\"\"\n",
        "        if topk > len(self.topics[0]):\n",
        "            raise Exception('Words in topics are less than topk')\n",
        "        else:\n",
        "            npmi = CoherenceModel(\n",
        "                topics=self.topics, texts=self.texts,\n",
        "                dictionary=self.dictionary,\n",
        "                coherence='c_npmi', topn=topk)\n",
        "            if per_topic:\n",
        "                return npmi.get_coherence_per_topic()\n",
        "            else:\n",
        "                return npmi.get_coherence()\n",
        "\n",
        "hyperparams = {\n",
        "  'num_topics': num_topics_list,\n",
        "  'num_top_words': num_topics_word,\n",
        "  'percentage_of_dataset': perc,\n",
        "  'text_column': text_column,\n",
        "  'label_column': label_column,\n",
        "  'id_column': id_column,\n",
        "}\n",
        "\n",
        "# make new directory\n",
        "try:\n",
        "    os.makedirs(folder + 'NMF')\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "      raise\n",
        "\n",
        "# write hyperparams to file\n",
        "import json\n",
        "with open(folder + 'NMF/hyperparams.txt', 'w') as f:\n",
        "  f.write(json.dumps(hyperparams))\n",
        "\n",
        "# write stopwords to file\n",
        "with open(folder + 'NMF/stopwords.txt', 'w') as f:\n",
        "    for item in mallet_stopwords:\n",
        "        f.write(item + \", \")\n",
        "\n",
        "for num_topics in num_topics_list:\n",
        "    print(\"Start \" + str(num_topics))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=num_topics_word, min_df=10, stop_words='english')\n",
        "    X = vectorizer.fit_transform(documents + test_documents)\n",
        "    words = np.array(vectorizer.get_feature_names_out())\n",
        "    \n",
        "    nmf = NMF(n_components=num_topics, solver=\"mu\")\n",
        "    W = nmf.fit_transform(X)\n",
        "    H = nmf.components_\n",
        "\n",
        "    nmf_loglik = []\n",
        "    nmf_topics = []\n",
        "    for i in range(len(H)):\n",
        "        temp_words = words[H[i].argsort()]\n",
        "        temp_weights = sorted(H[i])\n",
        "\n",
        "        temp_dict = {}\n",
        "        temp_topics = []\n",
        "\n",
        "        for i in range(len(temp_words)):\n",
        "            if temp_words[i] not in mallet_stopwords:\n",
        "                temp_dict[temp_words[i]] = temp_weights[i]\n",
        "                temp_topics.append(temp_words[i])\n",
        "    \n",
        "        nmf_loglik.append(temp_dict)\n",
        "        nmf_topics.append(temp_topics)\n",
        "    \n",
        "    nmf_r2_train, nmf_mae_train, nmf_mse_train, nmf_rmse_train, nmf_r2_test, nmf_mae_test, nmf_mse_test, nmf_rmse_test, nmf_regression = results(nmf_topics, nmf_loglik)\n",
        "    \n",
        "    nmf_topic_diversity = proportion_unique_words(nmf_topics)\n",
        "\n",
        "    tokenizer = lambda s: re.findall( '\\w+', s.lower() )\n",
        "    texts = [tokenizer(t) for t in  documents]\n",
        "    nmf_coherence = CoherenceNPMI(texts=texts, topics=nmf_topics).score()\n",
        "\n",
        "    final_df = pd.concat([pd.Series([nmf_r2_train, nmf_mae_train, nmf_mse_train, nmf_rmse_train, nmf_r2_test, nmf_mae_test, nmf_mse_test, nmf_rmse_test, nmf_topic_diversity, nmf_coherence])], axis = 1).T\n",
        "    final_df.columns = ['Train R2', 'Train MAE', 'Train MSE', 'Train RMSE', 'Test R2', 'Test MAE', 'Test MSE', 'Test RMSE', 'Topic Diversity', 'Topic Coherence']\n",
        "    display(final_df)\n",
        "\n",
        "    with open(folder + 'NMF/LR_coefficients_' + str(num_topics) + '.txt', 'w') as f:\n",
        "        for item in nmf_regression.coef_:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "    \n",
        "    final_df.to_csv(folder + 'NMF/df_' + str(num_topics) + '.csv')\n",
        "\n",
        "    with open(folder + 'NMF/topics_' + str(num_topics) + '.txt', 'w') as f:\n",
        "        for sublist in nmf_topics:\n",
        "            for i in sublist:\n",
        "                if i not in mallet_stopwords:\n",
        "                    f.write(i + \", \")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(\"End \" + str(num_topics))\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GPJW1I8d0RLC",
        "jdrTu5G70cH0",
        "P6XQTwmDMyBY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09a11feda42f4d6ba5e008e767db9697": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16a84666a9642c882d8482ef9a63c20",
            "placeholder": "​",
            "style": "IPY_MODEL_db532f2eafbf4aaeb4596a938d606543",
            "value": " 3/0 [00:00&lt;00:00, 76.32 examples/s]"
          }
        },
        "1ea1810429f741709e31785ad5ff4722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30807e5b126248d2a445f0a0ee7e591b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8c5467e7f7994fdaa72fe05debdacf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30807e5b126248d2a445f0a0ee7e591b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c37c202c50564d5a8feefa849bba962e",
            "value": 1
          }
        },
        "a3f38945aa624fd992e9f6e81b56ea2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7893bbd0a80417186a1728ff855ac1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1a362062b94243946dd4d10a450c0f",
            "placeholder": "​",
            "style": "IPY_MODEL_1ea1810429f741709e31785ad5ff4722",
            "value": "Generating train split: "
          }
        },
        "c37c202c50564d5a8feefa849bba962e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c70668e3494c42bcbdc58563ec598c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7893bbd0a80417186a1728ff855ac1f",
              "IPY_MODEL_8c5467e7f7994fdaa72fe05debdacf78",
              "IPY_MODEL_09a11feda42f4d6ba5e008e767db9697"
            ],
            "layout": "IPY_MODEL_a3f38945aa624fd992e9f6e81b56ea2d"
          }
        },
        "db532f2eafbf4aaeb4596a938d606543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f16a84666a9642c882d8482ef9a63c20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1a362062b94243946dd4d10a450c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
