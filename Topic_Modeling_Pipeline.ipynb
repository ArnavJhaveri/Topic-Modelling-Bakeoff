{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMV7zTU1n9r"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex8TGwk0mTvd"
      },
      "source": [
        "1. Figure out what command BERTopic uses for (weight, word) pair in the topic and do the same thing as mentioned for LDA and CTM\n",
        "\n",
        "2. Why are the topics so monolingual -- this really needs to be answered --> maybe the embedding model could be changed --> take the vector embeddings for each language and subtract off the mean for all the sentence embeddings for that language\n",
        "\n",
        "\n",
        "Steps for the step 2 above:\n",
        "1. Embed each of my sentences (so we have an embedding for each of the sentences)\n",
        "2. Partition by language\n",
        "3. Take the average embedding for each language (centroid)\n",
        "4. When we retrieve the embedding for a given sentence, subtract off the average for that language\n",
        "5. Use this embedding for the clustering instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZc9P1Knz4xN"
      },
      "source": [
        "#Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BUw5EQi8v9r1",
        "outputId": "4adb927e-4343-4795-96fe-f410b213e7ca"
      },
      "outputs": [],
      "source": [
        "# !pip install contextualized-topic-models==2.3.0\n",
        "# !pip install datasets\n",
        "# !pip install pyldavis\n",
        "# !pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2OHuu51Y3opK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords as stop_words\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "from gensim.utils import deaccent\n",
        "import warnings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scipy.sparse\n",
        "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.models.ctm import ZeroShotTM\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO\n",
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lesAgL0U27xz"
      },
      "source": [
        "#Variables Changing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAo2LdVN29bK",
        "outputId": "3e91d32d-2f70-4d0f-aa6c-981a93d9a245"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/arnav/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "is_combined = True\n",
        "perc = 0.10 # 10% of dataset\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "language = 'en' if is_combined else 'all_languages'\n",
        "stopwords = list(stop_words.words('english')) if is_combined else list(stop_words.words(stop_words.fileids())) # from every language\n",
        "embedding_model = 'paraphrase-distilroberta-base-v2' if is_combined else 'paraphrase-multilingual-mpnet-base-v2'\n",
        "\n",
        "num_epochs_ctm = 50\n",
        "num_topics = 50\n",
        "num_topics_word = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1gfpXtmz67m"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PpVi-9I1TQ6U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset amazon_reviews_multi (/home/arnav/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc6d1496d0704811becac82e2e49e4ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset('amazon_reviews_multi', language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J8_SXkfKQYVq"
      },
      "outputs": [],
      "source": [
        "def data(part):\n",
        "  data = dataset[part].to_pandas()\n",
        "  review_body = list(data['review_body'])\n",
        "  return data, review_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "37O0Ez7WQ3Ac",
        "outputId": "47374d0a-28e0-48e9-b9d8-bfcfd1467ec9"
      },
      "outputs": [],
      "source": [
        "train, review_body = data('train')\n",
        "test, review_body_test = data('test')\n",
        "validation, review_body_validation = data('validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mBeI4ohCUt10"
      },
      "outputs": [],
      "source": [
        "def make_review_body(data, perc):\n",
        "  arr = sklearn.utils.shuffle(np.arange(len(data)), random_state=42)[0:int(len(data) * perc)]\n",
        "  temp = data.iloc[arr].reset_index(drop=True)\n",
        "  review_body = list(temp['review_body'])\n",
        "  temp = temp.reset_index()\n",
        "  temp = temp.rename(columns = {'index': 'message_id', 'review_body': 'message'})\n",
        "  return temp, review_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kxKTimTHU-9H"
      },
      "outputs": [],
      "source": [
        "temp_train, review_body = make_review_body(train, perc)\n",
        "temp_test, test_review_body = make_review_body(test, 1)\n",
        "temp_validation, validation_review_body = make_review_body(validation, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# temp_train = pd.concat([temp_train, temp_test])\n",
        "# review_body = pd.concat([pd.Series(review_body), pd.Series(test_review_body)])\n",
        "\n",
        "# temp_train_to_save = pd.concat([temp_train, temp_test])[['message_id', 'message']]\n",
        "# temp_test_to_save = temp_test[['message_id', 'message']]\n",
        "# temp_validation_to_save = temp_validation[['message_id', 'message']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# temp_train_to_save.to_csv('amazon_multi_train_ten.csv')\n",
        "# temp_test_to_save.to_csv('amazon_multi_test_ten.csv')\n",
        "# temp_validation_to_save.to_csv('amazon_multi_validation_ten.csv')\n",
        "\n",
        "# temp_train_to_save.to_csv('amazon_eng_train_ten.csv')\n",
        "# temp_test_to_save.to_csv('amazon_eng_test_ten.csv')\n",
        "# temp_validation_to_save.to_csv('amazon_eng_validation_ten.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import sqlalchemy\n",
        "\n",
        "# temp_train_from_save = pd.read_csv('amazon_eng_train_ten.csv')\n",
        "# temp_train_from_save = temp_train_from_save[['message_id', 'message']]\n",
        "\n",
        "# # temp_test_from_save = pd.read_csv('amazon_eng_test_ten.csv')\n",
        "# # temp_test_from_save = temp_test_from_save[['message_id', 'message']]\n",
        "\n",
        "# # temp_validation_from_save = pd.read_csv('amazon_eng_validation_ten.csv')\n",
        "# # temp_validation_from_save = temp_validation_from_save[['message_id', 'message']]\n",
        "\n",
        "# db = sqlalchemy.engine.url.URL(drivername='mysql', host='127.0.0.1', database='arnav', query={'read_default_file': '~/.my.cnf', 'charset':'utf8mb4'})\n",
        "# engine = sqlalchemy.create_engine(db)\n",
        "\n",
        "# temp_train_from_save.to_sql('amazon_eng_train_ten', engine, if_exists='replace', index=False) \n",
        "# # temp_test_from_save.to_sql('amazon_eng_test_ten', engine, if_exists='replace', index=False) \n",
        "# # temp_validation_from_save.to_sql('amazon_eng_validation_ten', engine, if_exists='replace', index=False) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPJW1I8d0RLC"
      },
      "source": [
        "#Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sh3oNwBqXh_G"
      },
      "outputs": [],
      "source": [
        "class WhiteSpacePreprocessingStopwords():\n",
        "    \"\"\"\n",
        "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, documents, stopwords_list=None, vocabulary_size=2000, max_df=1.0, min_words=1,\n",
        "                 remove_numbers=True):\n",
        "        \"\"\"\n",
        "\n",
        "        :param documents: list of strings\n",
        "        :param stopwords_list: list of the stopwords to remove\n",
        "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
        "        :param max_df : float or int, default=1.0\n",
        "        When building the vocabulary ignore terms that have a document\n",
        "        frequency strictly higher than the given threshold (corpus-specific\n",
        "        stop words).\n",
        "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
        "        documents, integer absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "        :param min_words: int, default=1. Documents with less words than the parameter\n",
        "        will be removed\n",
        "        :param remove_numbers: bool, default=True. If true, numbers are removed from docs\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        if stopwords_list is not None:\n",
        "            self.stopwords = set(stopwords_list)\n",
        "        else:\n",
        "            self.stopwords = []\n",
        "\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.max_df = max_df\n",
        "        self.min_words = min_words\n",
        "        self.remove_numbers = remove_numbers\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"\n",
        "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
        "        list of unpreprocessed documents.\n",
        "\n",
        "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
        "        \"\"\"\n",
        "        preprocessed_docs_tmp = self.documents\n",
        "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [doc.translate(\n",
        "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
        "        if self.remove_numbers:\n",
        "            preprocessed_docs_tmp = [doc.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
        "                                     for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, max_df=self.max_df)\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
        "\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0 and len(doc) >= self.min_words:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(self.documents[i])\n",
        "                retained_indices.append(i)\n",
        "\n",
        "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
        "\n",
        "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdrTu5G70cH0"
      },
      "source": [
        "#TopicModelDataPreparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dcpoTEeEZDs-"
      },
      "outputs": [],
      "source": [
        "def get_bag_of_words(data, min_length):\n",
        "    \"\"\"\n",
        "    Creates the bag of words\n",
        "    \"\"\"\n",
        "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
        "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
        "\n",
        "    vect = scipy.sparse.csr_matrix(vect)\n",
        "    return vect\n",
        "\n",
        "\n",
        "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from an input file, assumes one document per line\n",
        "    \"\"\"\n",
        "\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    with open(text_file, encoding=\"utf-8\") as filino:\n",
        "        texts = list(map(lambda x: x, filino.readlines()))\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from a list\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def check_max_local_length(max_seq_length, texts):\n",
        "    max_local_length = np.max([len(t.split()) for t in texts])\n",
        "    if max_local_length > max_seq_length:\n",
        "        warnings.simplefilter('always', DeprecationWarning)\n",
        "        warnings.warn(f\"the longest document in your collection has {max_local_length} words, the model instead \"\n",
        "                      f\"truncates to {max_seq_length} tokens.\")\n",
        "\n",
        "\n",
        "class TopicModelDataPreparation:\n",
        "\n",
        "    def __init__(self, contextualized_model=None, show_warning=True, max_seq_length=128):\n",
        "        self.contextualized_model = contextualized_model\n",
        "        self.vocab = []\n",
        "        self.id2token = {}\n",
        "        self.vectorizer = None\n",
        "        self.label_encoder = None\n",
        "        self.show_warning = show_warning\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def load(self, contextualized_embeddings, bow_embeddings, id2token, labels=None):\n",
        "        return CTMDataset(\n",
        "            X_contextual=contextualized_embeddings, X_bow=bow_embeddings, idx2token=id2token, labels=labels)\n",
        "\n",
        "    def fit(self, text_for_contextual, text_for_bow, labels=None, custom_embeddings=None):\n",
        "        \"\"\"\n",
        "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "            if type(custom_embeddings).__module__ != 'numpy':\n",
        "                raise TypeError(\"contextualized_embeddings must be a numpy.ndarray type object\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None and custom_embeddings is None:\n",
        "            raise Exception(\"A contextualized model or contextualized embeddings must be defined\")\n",
        "\n",
        "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
        "\n",
        "        # if the user is passing custom embeddings we don't need to create the embeddings using the model\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            train_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            train_contextualized_embeddings = custom_embeddings\n",
        "        self.vocab = self.vectorizer.get_feature_names_out()\n",
        "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
        "\n",
        "        if labels:\n",
        "            self.label_encoder = OneHotEncoder()\n",
        "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "        return CTMDataset(\n",
        "            X_contextual=train_contextualized_embeddings, X_bow=train_bow_embeddings,\n",
        "            idx2token=self.id2token, labels=encoded_labels)\n",
        "\n",
        "    def transform(self, text_for_contextual, text_for_bow=None, custom_embeddings=None, labels=None):\n",
        "        \"\"\"\n",
        "        This method create the input for the prediction. Essentially, it creates the embeddings with the contextualized\n",
        "        model of choice and with trained vectorizer.\n",
        "\n",
        "        If text_for_bow is missing, it should be because we are using ZeroShotTM\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None:\n",
        "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
        "        else:\n",
        "            # dummy matrix\n",
        "            if self.show_warning:\n",
        "                warnings.simplefilter('always', DeprecationWarning)\n",
        "                warnings.warn(\n",
        "                    \"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n",
        "                    \"are using ZeroShotTM in a cross-lingual setting\")\n",
        "\n",
        "            # we just need an object that is matrix-like so that pytorch does not complain\n",
        "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            test_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            test_contextualized_embeddings = custom_embeddings\n",
        "\n",
        "        if labels:\n",
        "            encoded_labels = self.label_encoder.transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "\n",
        "        return CTMDataset(X_contextual=test_contextualized_embeddings, X_bow=test_bow_embeddings,\n",
        "                          idx2token=self.id2token, labels=encoded_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W02I4GhA0ihe"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lYOlwvGDn8DB"
      },
      "outputs": [],
      "source": [
        "documents = [line.strip() for line in review_body]\n",
        "test_documents = [line.strip() for line in test_review_body]\n",
        "\n",
        "sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
        "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
        "labels = temp_train['stars'][retained_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6XQTwmDMyBY"
      },
      "source": [
        "##ETM (not working)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "P2YVeiB67q1Z"
      },
      "outputs": [],
      "source": [
        "# from embedded_topic_model.utils import embedding\n",
        "# embeddings_mapping = embedding.create_word2vec_embedding_from_dataset(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0A5eHDsDHE"
      },
      "source": [
        "##BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "ts8S70BRceXW"
      },
      "outputs": [],
      "source": [
        "if perc > 0.01:\n",
        "  bert_topic_model = BERTopic(nr_topics=num_topics) if language!='en' else BERTopic(nr_topics=num_topics, language = 'English').fit(preprocessed_documents)\n",
        "else:\n",
        "  bert_topic_model = BERTopic(nr_topics=num_topics, calculate_probabilities=True).fit(preprocessed_documents) if language!='en' else BERTopic(nr_topics=num_topics, language = 'English', calculate_probabilities=True).fit(preprocessed_documents)\n",
        "# bert_topics = list(bert_topic_model.get_topic_info()['Representation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_topic_distribution, _ = bert_topic_model.approximate_distribution(preprocessed_documents) # take this to be the result from preprocess?\n",
        "bert_topic_distribution_test, _ = bert_topic_model.approximate_distribution(test_preprocessed_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('book', 0.05594157274261299),\n",
              " ('story', 0.03711343058052864),\n",
              " ('read', 0.031694403680537254),\n",
              " ('characters', 0.027079891063082135),\n",
              " ('books', 0.025984729672777325),\n",
              " ('series', 0.019556978708388566),\n",
              " ('author', 0.01935942805415595),\n",
              " ('pages', 0.018397262689030556),\n",
              " ('reading', 0.017153381712940962),\n",
              " ('enjoyed', 0.015696921281808312)]"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_topic_model.get_topics()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfGZ5Kc70sV"
      },
      "source": [
        "##CTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhWE-DOeXhd3"
      },
      "outputs": [],
      "source": [
        "tp = TopicModelDataPreparation(embedding_model)\n",
        "training_dataset = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-Ii3ivKQa-6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: [21/50]\t Seen Samples: [1667967/3971350]\tTrain Loss: 126.76303104005511\tTime: 0:02:18.294483: : 21it [45:21, 129.36s/it]"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, n_components=num_topics, num_epochs=num_epochs_ctm) if is_combined else ZeroShotTM(bow_size=len(tp.vocab), contextual_size=768, n_components=num_topics, num_epochs=num_epochs_ctm)\n",
        "ctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, n_components=num_topics, num_epochs=num_epochs_ctm) # testing something out\n",
        "ctm.fit(training_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxcKgjbx3V2o"
      },
      "outputs": [],
      "source": [
        "ctm_topics = ctm.get_topic_lists(num_topics_word)\n",
        "documents = preprocessed_documents\n",
        "labels = temp_train['stars'][retained_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqblvyh5E99L"
      },
      "outputs": [],
      "source": [
        "ctm_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438cbcf6"
      },
      "source": [
        "##LDA (GS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48fbb3d1"
      },
      "outputs": [],
      "source": [
        "split_preprocessed_documents = [d.split() for d in preprocessed_documents]\n",
        "dictionary = Dictionary(split_preprocessed_documents)\n",
        "corpus = [dictionary.doc2bow(text) for text in split_preprocessed_documents]\n",
        "\n",
        "lda = LdaModel(corpus, num_topics=num_topics, iterations=500, random_state=42, minimum_probability = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a676a15"
      },
      "outputs": [],
      "source": [
        "def get_topics_lda(topk=10):\n",
        "  topic_terms = []\n",
        "  for i in range(num_topics):\n",
        "      topic_words_list = []\n",
        "      for word_tuple in lda.get_topic_terms(i, topk):\n",
        "          topic_words_list.append(dictionary[word_tuple[0]])\n",
        "      topic_terms.append(topic_words_list)\n",
        "  return topic_terms\n",
        "\n",
        "lda_topics = get_topics_lda(num_topics_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIp-b3ifE6Dz"
      },
      "outputs": [],
      "source": [
        "lda_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECKEgHx1EUy"
      },
      "source": [
        "#Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PARAMS\n",
        "# documents: list of documents (each element in the list is a string) that the topics were extracted from\n",
        "# topics: list of list of topics from the model of choice\n",
        "# weights: list of dictionary mappings (word: weight)\n",
        "\n",
        "# RETURN\n",
        "# topic distribution\n",
        "\n",
        "def preprocess(documents, topics, weights):\n",
        "    # Initialize distribution matrix\n",
        "    distribution = np.zeros((len(documents), len(topics)))\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        # Preprocess document\n",
        "        document = document.translate(str.maketrans('', '', string.punctuation)) # removing periods, commas, etc\n",
        "        document = document.split(' ') # split on spaces\n",
        "        document = [word.lower() for word in document if len(word.lower()) > 0] # lower case everything since all topics are lower case\n",
        "\n",
        "        for j, loglik_dict in enumerate(weights):\n",
        "            distribution[i][j] = np.sum([0 if word not in loglik_dict else loglik_dict[word] for word in document]) # if word exists then its weight else 0\n",
        "\n",
        "        # Normalize\n",
        "        distribution[i] /= (len(document) + 2) # +2 for some weird reason ?\n",
        "        \n",
        "    return distribution\n",
        "\n",
        "\n",
        "# PARAMS\n",
        "# X: distribution from preprocess() above\n",
        "# y: stars ('labels')\n",
        "\n",
        "# RETURN\n",
        "# LR model\n",
        "\n",
        "def train_regression_model(X, y):\n",
        "    model = LinearRegression()\n",
        "    # model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "# PARAMS\n",
        "# model: model trained from train_regression_model()\n",
        "# X_test: distribution you want to make predictions on\n",
        "# y_test: the true labels for the X_test passed in\n",
        "\n",
        "# RETURN\n",
        "# MSE: MSE of the (X_test, y_test) data inputted\n",
        "# RMSE: RMSE of the (X_test, y_test) data inputted\n",
        "# R2: R2 of the (X_test, y_test) data inputted\n",
        "# MAE: MAE of the (X_test, y_test) data inputted\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # from sklearn.metrics import accuracy_score\n",
        "    # return accuracy_score(y_test, y_pred)\n",
        "\n",
        "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=True)\n",
        "    rmse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=False)\n",
        "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
        "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "    return mse, rmse, r2, mae\n",
        "\n",
        "\n",
        "# PARAMS\n",
        "# data: testing dataset\n",
        "\n",
        "# RETURN\n",
        "# preprocessed_documents: documents after passed through preprocessing steps\n",
        "# labels: corresponding labels to ^\n",
        "\n",
        "def test_ready(data):\n",
        "  documents = [line.strip() for line in data['review_body']]\n",
        "  sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
        "  preprocessed_documents, _, _, retained_indices = sp.preprocess()\n",
        "  labels = temp_test['stars'][retained_indices]\n",
        "  return preprocessed_documents, labels\n",
        "\n",
        "test_preprocessed_documents, test_labels = test_ready(test)\n",
        "\n",
        "\n",
        "# PARAMS\n",
        "# topics: list of list of topics\n",
        "# weights: list of dictionary mappings (word: weight)\n",
        "# flag: True if baseline (default is False\n",
        "\n",
        "# RETURN\n",
        "# r2_train: R2 on train set\n",
        "# mae_train: MAE on train set\n",
        "# mse_train: MSE on train set\n",
        "# rmse_train: RMSE on train set\n",
        "# r2_test: R2 on test set\n",
        "# mae_test: MAE on test set\n",
        "# mse_test: MSE on test set\n",
        "# rmse_test: RMSE on test set\n",
        "\n",
        "def results(topics, weights, flag=False):\n",
        "  if flag:\n",
        "    X = np.array([labels.mean()] * len(labels)).reshape(-1, 1)\n",
        "    X_test = np.array([test_labels.mean()] * len(test_labels)).reshape(-1, 1)\n",
        "\n",
        "  else:\n",
        "    # X = preprocess(preprocessed_documents, topics, weights)\n",
        "    X = preprocess([line.strip() for line in review_body], topics, weights)\n",
        "    # X_test = preprocess(test_preprocessed_documents, topics, weights)\n",
        "    X_test = preprocess([line.strip() for line in test_review_body], topics, weights)\n",
        "\n",
        "  # y = labels\n",
        "  y = temp_train['stars']\n",
        "  # y_test = test_labels\n",
        "  y_test = temp_test['stars']\n",
        "  model = train_regression_model(X, y)\n",
        "\n",
        "  mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, X_test, y_test)\n",
        "  mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, X, y)\n",
        "\n",
        "  # train_acc = evaluate_model(model, X, y)\n",
        "  # test_acc = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "  # print(\"Train Accuracy: \", train_acc)\n",
        "  # print(\"Test Accuracy: \", test_acc)\n",
        "\n",
        "  # return train_acc, test_acc\n",
        "\n",
        "  print(\"Train\")\n",
        "  print(\"R2:\", r2_train)\n",
        "  print(\"MAE:\", mae_train)\n",
        "  print(\"MSE:\", mse_train)\n",
        "  print(\"RMSE:\", rmse_train)\n",
        "\n",
        "  print()\n",
        "\n",
        "  print(\"Test\")\n",
        "  print(\"R2:\", r2_test)\n",
        "  print(\"MAE:\", mae_test)\n",
        "  print(\"MSE:\", mse_test)\n",
        "  print(\"RMSE:\", rmse_test)\n",
        "\n",
        "  return r2_train, mae_train, mse_train, rmse_train, r2_test, mae_test, mse_test, rmse_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MALLET LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_loglik = []\n",
        "with open(\"dlatk/amazon_eng/lda.topicGivenWord.csv\") as f:\n",
        "  for line in f:\n",
        "    temp = line.strip().split('\"')\n",
        "    if len(temp) > 1:\n",
        "      for i in range(len(temp)):\n",
        "        if i % 2 != 0:\n",
        "          temp[i] = (\"\".join(temp[i].split(\",\")))\n",
        "      temp = [\"\".join(temp)]\n",
        "    lda_loglik.append(temp[0].split(\",\")[1:])\n",
        "\n",
        "lda_loglik = lda_loglik[1:]\n",
        "\n",
        "mallet_lda_topics = [[x for i, x in enumerate(j) if i % 2 == 0] for j in lda_loglik]\n",
        "\n",
        "mallet_lda_loglik = []\n",
        "for k in lda_loglik:\n",
        "    mallet_lda_loglik.append({k[j]: float(k[j+1]) for j in range(0, len(k) - 1, 2)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train\n",
            "R2: 0.28878903518138765\n",
            "MAE: 0.9945251291336966\n",
            "MSE: 1.4338984404898703\n",
            "RMSE: 1.1974549847446752\n",
            "\n",
            "Test\n",
            "R2: 0.2709251968971472\n",
            "MAE: 1.0147344275351098\n",
            "MSE: 1.4581496062057056\n",
            "RMSE: 1.2075386561951984\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.28878903518138765,\n",
              " 0.9945251291336966,\n",
              " 1.4338984404898703,\n",
              " 1.1974549847446752,\n",
              " 0.2709251968971472,\n",
              " 1.0147344275351098,\n",
              " 1.4581496062057056,\n",
              " 1.2075386561951984)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results(mallet_lda_topics, mallet_lda_loglik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCroeZXlgcMv"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Toou_cqtlWv"
      },
      "outputs": [],
      "source": [
        "b_r2_train, b_mae_train, b_mse_train, b_rmse_train, b_r2_test, b_mae_test, b_mse_test, b_rmse_test = results(lda_topics, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seKgX_3d17z9"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vHvXo6MsGpJ"
      },
      "outputs": [],
      "source": [
        "lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test = results(lda_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBcId2fqd9qh"
      },
      "outputs": [],
      "source": [
        "pd.concat([pd.Series([lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test])], axis = 1).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwdN-c_U14mH"
      },
      "source": [
        "##CTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYX04Sx3sa7a"
      },
      "outputs": [],
      "source": [
        "ctm_r2_train, ctm_mae_train, ctm_mse_train, ctm_rmse_train, ctm_r2_test, ctm_mae_test, ctm_mse_test, ctm_rmse_test = results(ctm_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B43HKbgWexe0"
      },
      "outputs": [],
      "source": [
        "pd.concat([pd.Series([ctm_r2_train, ctm_mae_train, ctm_mse_train, ctm_rmse_train, ctm_r2_test, ctm_mae_test, ctm_mse_test, ctm_rmse_test])], axis = 1).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nya0LM5OkBHs"
      },
      "source": [
        "Using paraphrase-multilingual-mpnet-base-v2, 200 topics, 50 epochs (best one so far):\n",
        "\n",
        "* MSE: 1.0463393882784946\n",
        "* RMSE: 1.0229073214512128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt60-STspjK3"
      },
      "source": [
        "##BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.03982321, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.08145821, 0.49982888, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_topic_distribution_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train\n",
            "R2: 0.21279826767450016\n",
            "MAE: 1.054235544333614\n",
            "MSE: 1.5870375219991502\n",
            "RMSE: 1.259776774670477\n",
            "\n",
            "Test\n",
            "R2: -0.19955535661612278\n",
            "MAE: 1.3092946685216322\n",
            "MSE: 2.39886926289336\n",
            "RMSE: 1.5488283516559735\n"
          ]
        }
      ],
      "source": [
        "y = labels\n",
        "y_test = test_labels\n",
        "model = train_regression_model(bert_topic_distribution, y)\n",
        "\n",
        "mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, bert_topic_distribution_test, y_test)\n",
        "mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, bert_topic_distribution, y)\n",
        "\n",
        "print(\"Train\")\n",
        "print(\"R2:\", r2_train)\n",
        "print(\"MAE:\", mae_train)\n",
        "print(\"MSE:\", mse_train)\n",
        "print(\"RMSE:\", rmse_train)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Test\")\n",
        "print(\"R2:\", r2_test)\n",
        "print(\"MAE:\", mae_test)\n",
        "print(\"MSE:\", mse_test)\n",
        "print(\"RMSE:\", rmse_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ll74AMdph-s"
      },
      "outputs": [],
      "source": [
        "bert_r2_train, bert_mae_train, bert_mse_train, bert_rmse_train, bert_r2_test, bert_mae_test, bert_mse_test, bert_rmse_test = results(bert_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-onlxv-7e2kk"
      },
      "outputs": [],
      "source": [
        "pd.concat([pd.Series([bert_r2_train, bert_mae_train, bert_mse_train, bert_rmse_train, bert_r2_test, bert_mae_test, bert_mse_test, bert_rmse_test])], axis = 1).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9jWv1XuJnMv"
      },
      "source": [
        "#Printing out topics and LR coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjQ-NsWxNzSf"
      },
      "outputs": [],
      "source": [
        "def topic_LR_coef(model, topics):\n",
        "  x1 = []\n",
        "  x2 = pd.Series(model.coef_)\n",
        "\n",
        "  for i, topic in enumerate(topics):\n",
        "    x1.append(\", \".join(topic))\n",
        "\n",
        "  x1 = pd.Series(x1)\n",
        "  return (pd.concat([x1, x2], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCu4BygLCR7e"
      },
      "outputs": [],
      "source": [
        "lda_model = train_regression_model(preprocess(preprocessed_documents, lda_topics), labels)\n",
        "ctm_model = train_regression_model(preprocess(preprocessed_documents, ctm_topics), labels)\n",
        "bert_model = train_regression_model(preprocess(preprocessed_documents, bert_topics), labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrYRf_95GGHE"
      },
      "outputs": [],
      "source": [
        "lda_LR_coef = topic_LR_coef(lda_model, lda_topics)\n",
        "ctm_LR_coef = topic_LR_coef(ctm_model, ctm_topics)\n",
        "bert_LR_coef = topic_LR_coef(bert_model, bert_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EbxEc3hDSM-"
      },
      "outputs": [],
      "source": [
        "lda_LR_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzb5_OnwN3_f"
      },
      "outputs": [],
      "source": [
        "ctm_LR_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R0ZKUQBN4vf"
      },
      "outputs": [],
      "source": [
        "bert_LR_coef"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GPJW1I8d0RLC",
        "jdrTu5G70cH0",
        "P6XQTwmDMyBY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09a11feda42f4d6ba5e008e767db9697": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16a84666a9642c882d8482ef9a63c20",
            "placeholder": "",
            "style": "IPY_MODEL_db532f2eafbf4aaeb4596a938d606543",
            "value": " 3/0 [00:00&lt;00:00, 76.32 examples/s]"
          }
        },
        "1ea1810429f741709e31785ad5ff4722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30807e5b126248d2a445f0a0ee7e591b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8c5467e7f7994fdaa72fe05debdacf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30807e5b126248d2a445f0a0ee7e591b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c37c202c50564d5a8feefa849bba962e",
            "value": 1
          }
        },
        "a3f38945aa624fd992e9f6e81b56ea2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7893bbd0a80417186a1728ff855ac1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1a362062b94243946dd4d10a450c0f",
            "placeholder": "",
            "style": "IPY_MODEL_1ea1810429f741709e31785ad5ff4722",
            "value": "Generating train split: "
          }
        },
        "c37c202c50564d5a8feefa849bba962e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c70668e3494c42bcbdc58563ec598c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7893bbd0a80417186a1728ff855ac1f",
              "IPY_MODEL_8c5467e7f7994fdaa72fe05debdacf78",
              "IPY_MODEL_09a11feda42f4d6ba5e008e767db9697"
            ],
            "layout": "IPY_MODEL_a3f38945aa624fd992e9f6e81b56ea2d"
          }
        },
        "db532f2eafbf4aaeb4596a938d606543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f16a84666a9642c882d8482ef9a63c20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1a362062b94243946dd4d10a450c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
