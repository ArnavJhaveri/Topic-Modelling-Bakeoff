{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMV7zTU1n9r"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex8TGwk0mTvd"
      },
      "source": [
        "1. Run Mallet LDA -- will give actual weights on all the words for every topic (give the word the weight for that topic instead of a 1 or 0 in the evaluation script) -- at the moment, the weight for a topic is the number of words from that topic / total number of words in the document. INSTEAD: going to have every topic be a sum of the weights of the words (weights for that topic) in the document and then normalize that by the sum over all the topics\n",
        "\n",
        "2. Figure out what command BERTopic uses for (weight, word) pair in the topic and do the same thing as mentioned for LDA and CTM\n",
        "\n",
        "3. Why are the topics so monolingual -- this really needs to be answered --> maybe the embedding model could be changed --> take the vector embeddings for each language and subtract off the mean for all the sentence embeddings for that language\n",
        "\n",
        "\n",
        "Steps for the step 3 above:\n",
        "1. Embed each of my sentences (so we have an embedding for each of the sentences)\n",
        "2. Partition by language\n",
        "3. Take the average embedding for each language (centroid)\n",
        "4. When we retrieve the embedding for a given sentence, subtract off the average for that language\n",
        "5. Use this embedding for the clustering instead\n",
        "\n",
        "\n",
        "---------------------------------------\n",
        "\n",
        "Stick with 20 topics\n",
        "\n",
        "CTM should give weights, Gensim should give weights\n",
        "\n",
        "BERTopic seems to have more multilingual topics but still not the best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZc9P1Knz4xN"
      },
      "source": [
        "#Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BUw5EQi8v9r1",
        "outputId": "4adb927e-4343-4795-96fe-f410b213e7ca"
      },
      "outputs": [],
      "source": [
        "# !pip install contextualized-topic-models==2.3.0\n",
        "# !pip install datasets\n",
        "# !pip install pyldavis\n",
        "# !pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2OHuu51Y3opK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "/home/arnav/.conda/envs/BERTopic_1/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords as stop_words\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "from gensim.utils import deaccent\n",
        "import warnings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scipy.sparse\n",
        "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.models.ctm import ZeroShotTM\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO\n",
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lesAgL0U27xz"
      },
      "source": [
        "#Variables Changing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAo2LdVN29bK",
        "outputId": "3e91d32d-2f70-4d0f-aa6c-981a93d9a245"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/arnav/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "is_combined = True\n",
        "perc = 0.10 # 10% of dataset\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "language = 'en' if is_combined else 'all_languages'\n",
        "stopwords = list(stop_words.words('english')) if is_combined else list(stop_words.words(stop_words.fileids())) # from every language\n",
        "embedding_model = 'paraphrase-distilroberta-base-v2' if is_combined else 'paraphrase-multilingual-mpnet-base-v2'\n",
        "\n",
        "num_epochs_ctm = 50\n",
        "num_topics = 200\n",
        "num_topics_word = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1gfpXtmz67m"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PpVi-9I1TQ6U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset amazon_reviews_multi (/home/arnav/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c246b9278ae42589a393459bf55e3d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset('amazon_reviews_multi', language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J8_SXkfKQYVq"
      },
      "outputs": [],
      "source": [
        "def data(part):\n",
        "  data = dataset[part].to_pandas()\n",
        "  review_body = list(data['review_body'])\n",
        "  return data, review_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "37O0Ez7WQ3Ac",
        "outputId": "47374d0a-28e0-48e9-b9d8-bfcfd1467ec9"
      },
      "outputs": [],
      "source": [
        "train, review_body = data('train')\n",
        "test, review_body_test = data('test')\n",
        "validation, review_body_validation = data('validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combined_train = pd.concat([train, test])\n",
        "# combined_review_body = pd.concat([pd.Series(review_body), pd.Series(review_body_test)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mBeI4ohCUt10"
      },
      "outputs": [],
      "source": [
        "def make_review_body(data, perc):\n",
        "  arr = sklearn.utils.shuffle(np.arange(len(data)), random_state=42)[0:int(len(data) * perc)]\n",
        "  temp = data.iloc[arr].reset_index(drop=True)\n",
        "  review_body = list(temp['review_body'])\n",
        "  temp = temp.reset_index()\n",
        "  temp = temp.rename(columns = {'index': 'message_id', 'review_body': 'message'})\n",
        "  return temp, review_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kxKTimTHU-9H"
      },
      "outputs": [],
      "source": [
        "temp_train, review_body = make_review_body(train, perc)\n",
        "temp_test, test_review_body = make_review_body(test, 1)\n",
        "temp_validation, validation_review_body = make_review_body(validation, 1)\n",
        "\n",
        "# temp_train = pd.concat([temp_train, temp_test])\n",
        "# review_body = pd.concat([pd.Series(review_body), pd.Series(test_review_body)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# temp_train_to_save = pd.concat([temp_train, temp_test])[['message_id', 'message']]\n",
        "# temp_test_to_save = temp_test[['message_id', 'message']]\n",
        "# temp_validation_to_save = temp_validation[['message_id', 'message']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# temp_train_to_save.to_csv('amazon_multi_train_ten.csv')\n",
        "# temp_test_to_save.to_csv('amazon_multi_test_ten.csv')\n",
        "# temp_validation_to_save.to_csv('amazon_multi_validation_ten.csv')\n",
        "\n",
        "# temp_train_to_save.to_csv('amazon_eng_train_ten.csv')\n",
        "# temp_test_to_save.to_csv('amazon_eng_test_ten.csv')\n",
        "# temp_validation_to_save.to_csv('amazon_eng_validation_ten.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import sqlalchemy\n",
        "\n",
        "# temp_train_from_save = pd.read_csv('amazon_eng_train_ten.csv')\n",
        "# temp_train_from_save = temp_train_from_save[['message_id', 'message']]\n",
        "\n",
        "# # temp_test_from_save = pd.read_csv('amazon_eng_test_ten.csv')\n",
        "# # temp_test_from_save = temp_test_from_save[['message_id', 'message']]\n",
        "\n",
        "# # temp_validation_from_save = pd.read_csv('amazon_eng_validation_ten.csv')\n",
        "# # temp_validation_from_save = temp_validation_from_save[['message_id', 'message']]\n",
        "\n",
        "# db = sqlalchemy.engine.url.URL(drivername='mysql', host='127.0.0.1', database='arnav', query={'read_default_file': '~/.my.cnf', 'charset':'utf8mb4'})\n",
        "# engine = sqlalchemy.create_engine(db)\n",
        "\n",
        "# temp_train_from_save.to_sql('amazon_eng_train_ten', engine, if_exists='replace', index=False) \n",
        "# # temp_test_from_save.to_sql('amazon_eng_test_ten', engine, if_exists='replace', index=False) \n",
        "# # temp_validation_from_save.to_sql('amazon_eng_validation_ten', engine, if_exists='replace', index=False) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPJW1I8d0RLC"
      },
      "source": [
        "#Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sh3oNwBqXh_G"
      },
      "outputs": [],
      "source": [
        "class WhiteSpacePreprocessingStopwords():\n",
        "    \"\"\"\n",
        "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, documents, stopwords_list=None, vocabulary_size=2000, max_df=1.0, min_words=1,\n",
        "                 remove_numbers=True):\n",
        "        \"\"\"\n",
        "\n",
        "        :param documents: list of strings\n",
        "        :param stopwords_list: list of the stopwords to remove\n",
        "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
        "        :param max_df : float or int, default=1.0\n",
        "        When building the vocabulary ignore terms that have a document\n",
        "        frequency strictly higher than the given threshold (corpus-specific\n",
        "        stop words).\n",
        "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
        "        documents, integer absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "        :param min_words: int, default=1. Documents with less words than the parameter\n",
        "        will be removed\n",
        "        :param remove_numbers: bool, default=True. If true, numbers are removed from docs\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        if stopwords_list is not None:\n",
        "            self.stopwords = set(stopwords_list)\n",
        "        else:\n",
        "            self.stopwords = []\n",
        "\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.max_df = max_df\n",
        "        self.min_words = min_words\n",
        "        self.remove_numbers = remove_numbers\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"\n",
        "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
        "        list of unpreprocessed documents.\n",
        "\n",
        "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
        "        \"\"\"\n",
        "        preprocessed_docs_tmp = self.documents\n",
        "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [doc.translate(\n",
        "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
        "        if self.remove_numbers:\n",
        "            preprocessed_docs_tmp = [doc.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
        "                                     for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, max_df=self.max_df)\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
        "\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0 and len(doc) >= self.min_words:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(self.documents[i])\n",
        "                retained_indices.append(i)\n",
        "\n",
        "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
        "\n",
        "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdrTu5G70cH0"
      },
      "source": [
        "#TopicModelDataPreparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dcpoTEeEZDs-"
      },
      "outputs": [],
      "source": [
        "def get_bag_of_words(data, min_length):\n",
        "    \"\"\"\n",
        "    Creates the bag of words\n",
        "    \"\"\"\n",
        "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
        "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
        "\n",
        "    vect = scipy.sparse.csr_matrix(vect)\n",
        "    return vect\n",
        "\n",
        "\n",
        "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from an input file, assumes one document per line\n",
        "    \"\"\"\n",
        "\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    with open(text_file, encoding=\"utf-8\") as filino:\n",
        "        texts = list(map(lambda x: x, filino.readlines()))\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from a list\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def check_max_local_length(max_seq_length, texts):\n",
        "    max_local_length = np.max([len(t.split()) for t in texts])\n",
        "    if max_local_length > max_seq_length:\n",
        "        warnings.simplefilter('always', DeprecationWarning)\n",
        "        warnings.warn(f\"the longest document in your collection has {max_local_length} words, the model instead \"\n",
        "                      f\"truncates to {max_seq_length} tokens.\")\n",
        "\n",
        "\n",
        "class TopicModelDataPreparation:\n",
        "\n",
        "    def __init__(self, contextualized_model=None, show_warning=True, max_seq_length=128):\n",
        "        self.contextualized_model = contextualized_model\n",
        "        self.vocab = []\n",
        "        self.id2token = {}\n",
        "        self.vectorizer = None\n",
        "        self.label_encoder = None\n",
        "        self.show_warning = show_warning\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def load(self, contextualized_embeddings, bow_embeddings, id2token, labels=None):\n",
        "        return CTMDataset(\n",
        "            X_contextual=contextualized_embeddings, X_bow=bow_embeddings, idx2token=id2token, labels=labels)\n",
        "\n",
        "    def fit(self, text_for_contextual, text_for_bow, labels=None, custom_embeddings=None):\n",
        "        \"\"\"\n",
        "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "            if type(custom_embeddings).__module__ != 'numpy':\n",
        "                raise TypeError(\"contextualized_embeddings must be a numpy.ndarray type object\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None and custom_embeddings is None:\n",
        "            raise Exception(\"A contextualized model or contextualized embeddings must be defined\")\n",
        "\n",
        "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
        "\n",
        "        # if the user is passing custom embeddings we don't need to create the embeddings using the model\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            train_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            train_contextualized_embeddings = custom_embeddings\n",
        "        self.vocab = self.vectorizer.get_feature_names_out()\n",
        "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
        "\n",
        "        if labels:\n",
        "            self.label_encoder = OneHotEncoder()\n",
        "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "        return CTMDataset(\n",
        "            X_contextual=train_contextualized_embeddings, X_bow=train_bow_embeddings,\n",
        "            idx2token=self.id2token, labels=encoded_labels)\n",
        "\n",
        "    def transform(self, text_for_contextual, text_for_bow=None, custom_embeddings=None, labels=None):\n",
        "        \"\"\"\n",
        "        This method create the input for the prediction. Essentially, it creates the embeddings with the contextualized\n",
        "        model of choice and with trained vectorizer.\n",
        "\n",
        "        If text_for_bow is missing, it should be because we are using ZeroShotTM\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None:\n",
        "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
        "        else:\n",
        "            # dummy matrix\n",
        "            if self.show_warning:\n",
        "                warnings.simplefilter('always', DeprecationWarning)\n",
        "                warnings.warn(\n",
        "                    \"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n",
        "                    \"are using ZeroShotTM in a cross-lingual setting\")\n",
        "\n",
        "            # we just need an object that is matrix-like so that pytorch does not complain\n",
        "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            test_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            test_contextualized_embeddings = custom_embeddings\n",
        "\n",
        "        if labels:\n",
        "            encoded_labels = self.label_encoder.transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "\n",
        "        return CTMDataset(X_contextual=test_contextualized_embeddings, X_bow=test_bow_embeddings,\n",
        "                          idx2token=self.id2token, labels=encoded_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W02I4GhA0ihe"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lYOlwvGDn8DB"
      },
      "outputs": [],
      "source": [
        "documents = [line.strip() for line in review_body]\n",
        "sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
        "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
        "labels = temp_train['stars'][retained_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6XQTwmDMyBY"
      },
      "source": [
        "##ETM (not working)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "P2YVeiB67q1Z"
      },
      "outputs": [],
      "source": [
        "# from embedded_topic_model.utils import embedding\n",
        "# embeddings_mapping = embedding.create_word2vec_embedding_from_dataset(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0A5eHDsDHE"
      },
      "source": [
        "##BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "ts8S70BRceXW"
      },
      "outputs": [],
      "source": [
        "if perc > 0.01:\n",
        "  bert_topic_model = BERTopic(nr_topics=num_topics) if language!='en' else BERTopic(nr_topics=num_topics, language = 'English').fit(preprocessed_documents)\n",
        "else:\n",
        "  bert_topic_model = BERTopic(nr_topics=num_topics, calculate_probabilities=True).fit(preprocessed_documents) if language!='en' else BERTopic(nr_topics=num_topics, language = 'English', calculate_probabilities=True).fit(preprocessed_documents)\n",
        "# bert_topics = list(bert_topic_model.get_topic_info()['Representation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_topic_distribution, _ = bert_topic_model.approximate_distribution(preprocessed_documents) # take this to be the result from preprocess?\n",
        "bert_topic_distribution_test, _ = bert_topic_model.approximate_distribution(test_preprocessed_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(199,)"
            ]
          },
          "execution_count": 216,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_topic_distrbution[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('book', 0.05594157274261299),\n",
              " ('story', 0.03711343058052864),\n",
              " ('read', 0.031694403680537254),\n",
              " ('characters', 0.027079891063082135),\n",
              " ('books', 0.025984729672777325),\n",
              " ('series', 0.019556978708388566),\n",
              " ('author', 0.01935942805415595),\n",
              " ('pages', 0.018397262689030556),\n",
              " ('reading', 0.017153381712940962),\n",
              " ('enjoyed', 0.015696921281808312)]"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_topic_model.get_topics()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = preprocess(preprocessed_documents, mallet_lda_topics, lda_loglik)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "nasZeLl_pY1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.36743472, 0.31394564, 0.        , ..., 0.07406133, 0.        ,\n",
              "        0.4226438 ],\n",
              "       [0.22536142, 0.2094417 , 0.03427784, ..., 0.        , 0.11566492,\n",
              "        0.23229864],\n",
              "       [0.        , 0.        , 0.        , ..., 0.27111899, 0.        ,\n",
              "        1.48087119],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.16695646,\n",
              "        0.44055639],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.0686175 , 0.        , 0.45647005, ..., 0.        , 0.33401495,\n",
              "        0.10151003]])"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfGZ5Kc70sV"
      },
      "source": [
        "##CTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BhWE-DOeXhd3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96444e0a2f3547a987470a9d67da14c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-27ee19964229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTopicModelDataPreparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_for_contextual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpreprocessed_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_for_bow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessed_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-bea9a5e354e7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, text_for_contextual, text_for_bow, labels, custom_embeddings)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcustom_embeddings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             train_contextualized_embeddings = bert_embeddings_from_list(\n\u001b[0m\u001b[1;32m    101\u001b[0m                 text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-bea9a5e354e7>\u001b[0m in \u001b[0;36mbert_embeddings_from_list\u001b[0;34m(texts, sbert_model_to_load, batch_size, max_seq_length)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcheck_max_local_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'token_embeddings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         )\n\u001b[0;32m--> 852\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    853\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 )\n\u001b[1;32m    526\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    528\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         )\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.conda/envs/BERTopic_1/lib/python3.11/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tp = TopicModelDataPreparation(embedding_model)\n",
        "training_dataset = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-Ii3ivKQa-6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: [21/50]\t Seen Samples: [1667967/3971350]\tTrain Loss: 126.76303104005511\tTime: 0:02:18.294483: : 21it [45:21, 129.36s/it]"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, n_components=num_topics, num_epochs=num_epochs_ctm) if is_combined else ZeroShotTM(bow_size=len(tp.vocab), contextual_size=768, n_components=num_topics, num_epochs=num_epochs_ctm)\n",
        "ctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, n_components=num_topics, num_epochs=num_epochs_ctm) # testing something out\n",
        "ctm.fit(training_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxcKgjbx3V2o"
      },
      "outputs": [],
      "source": [
        "ctm_topics = ctm.get_topic_lists(num_topics_word)\n",
        "documents = preprocessed_documents\n",
        "labels = temp_train['stars'][retained_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqblvyh5E99L"
      },
      "outputs": [],
      "source": [
        "ctm_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438cbcf6"
      },
      "source": [
        "##LDA (GS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48fbb3d1"
      },
      "outputs": [],
      "source": [
        "split_preprocessed_documents = [d.split() for d in preprocessed_documents]\n",
        "dictionary = Dictionary(split_preprocessed_documents)\n",
        "corpus = [dictionary.doc2bow(text) for text in split_preprocessed_documents]\n",
        "\n",
        "lda = LdaModel(corpus, num_topics=num_topics, iterations=500, random_state=42, minimum_probability = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a676a15"
      },
      "outputs": [],
      "source": [
        "def get_topics_lda(topk=10):\n",
        "  topic_terms = []\n",
        "  for i in range(num_topics):\n",
        "      topic_words_list = []\n",
        "      for word_tuple in lda.get_topic_terms(i, topk):\n",
        "          topic_words_list.append(dictionary[word_tuple[0]])\n",
        "      topic_terms.append(topic_words_list)\n",
        "  return topic_terms\n",
        "\n",
        "lda_topics = get_topics_lda(num_topics_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIp-b3ifE6Dz"
      },
      "outputs": [],
      "source": [
        "lda_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECKEgHx1EUy"
      },
      "source": [
        "#Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOEggvBBvT6i"
      },
      "outputs": [],
      "source": [
        "# old version EXAMPLE\n",
        "\n",
        "# doc = \"we walked the dog yesterday\"\n",
        "\n",
        "# t1: you, we, I\n",
        "# t2: the, an\n",
        "# t3: dog, walked\n",
        "# t4: burger\n",
        "\n",
        "# #           t1   t2   t3  t4\n",
        "# X_doc_1 = [0.2, 0.2, 0.4, 0]\n",
        "\n",
        "\n",
        "\n",
        "# new version EXAMPLE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BFE6RrcyxySY"
      },
      "outputs": [],
      "source": [
        "def preprocess(documents, topics, weights):\n",
        "    # Initialize distribution matrix\n",
        "    distribution = np.zeros((len(documents), len(topics)))\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        # Preprocess document\n",
        "        document = document.split(' ')\n",
        "        document = [word.lower() for word in document if len(word.lower()) > 0]\n",
        "\n",
        "        loglik_dicts = []\n",
        "        for k in weights:\n",
        "            loglik_dicts.append({k[j]: float(k[j+1]) for j in range(0, len(k) - 1, 2)})\n",
        "\n",
        "        for j, loglik_dict in enumerate(loglik_dicts):\n",
        "            distribution[i][j] = np.sum([0 if word not in loglik_dict else loglik_dict[word] for word in document])\n",
        "\n",
        "        # Normalize\n",
        "        distribution[i] /= len(document)\n",
        "\n",
        "    return distribution\n",
        "\n",
        "\n",
        "def train_regression_model(X, y):\n",
        "    model = LinearRegression()\n",
        "    # model = Ridge(alpha = 0.1)\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=True)\n",
        "    rmse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=False)\n",
        "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
        "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "    return mse, rmse, r2, mae\n",
        "\n",
        "\n",
        "def test_ready(data):\n",
        "  documents = [line.strip() for line in data['review_body']]\n",
        "  sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
        "  preprocessed_documents, _, _, retained_indices = sp.preprocess()\n",
        "  labels = temp_test['stars'][retained_indices]\n",
        "\n",
        "  return preprocessed_documents, labels\n",
        "\n",
        "test_preprocessed_documents, test_labels = test_ready(test)\n",
        "\n",
        "def results(topics, weights, flag=False):\n",
        "  if flag:\n",
        "    X = np.array([labels.mean()] * len(labels)).reshape(-1, 1)\n",
        "    X_test = np.array([test_labels.mean()] * len(test_labels)).reshape(-1, 1)\n",
        "\n",
        "  else:\n",
        "    # X = preprocess(preprocessed_documents, topics, weights)\n",
        "    # X = preprocess([line.strip() for line in review_body], topics, weights)\n",
        "    X = mallet_lda_train_distribution\n",
        "    # X_test = preprocess(test_preprocessed_documents, topics, weights)\n",
        "    # X_test = preprocess([line.strip() for line in test_review_body], topics, weights)\n",
        "    X_test = mallet_lda_test_distribution\n",
        "\n",
        "  # y = labels\n",
        "  y = temp_train['stars']\n",
        "  # y_test = test_labels\n",
        "  y_test = temp_test['stars']\n",
        "  model = train_regression_model(X, y)\n",
        "\n",
        "  mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, X_test, y_test)\n",
        "  mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, X, y)\n",
        "\n",
        "  print(\"Train\")\n",
        "  print(\"R2:\", r2_train)\n",
        "  print(\"MAE:\", mae_train)\n",
        "  print(\"MSE:\", mse_train)\n",
        "  print(\"RMSE:\", rmse_train)\n",
        "\n",
        "  print()\n",
        "\n",
        "  print(\"Test\")\n",
        "  print(\"R2:\", r2_test)\n",
        "  print(\"MAE:\", mae_test)\n",
        "  print(\"MSE:\", mse_test)\n",
        "  print(\"RMSE:\", rmse_test)\n",
        "\n",
        "  return r2_train, mae_train, mse_train, rmse_train, r2_test, mae_test, mse_test, rmse_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MALLET LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_loglik = []\n",
        "# with open(\"dlatk/amazon_eng/lda.loglik.csv\") as f:\n",
        "with open(\"dlatk/amazon_eng/lda.wordGivenTopic.csv\") as f:\n",
        "  for line in f:\n",
        "    temp = line.strip().split('\"')\n",
        "    if len(temp) > 1:\n",
        "      for i in range(len(temp)):\n",
        "        if i % 2 != 0:\n",
        "          temp[i] = (\"\".join(temp[i].split(\",\")))\n",
        "      temp = [\"\".join(temp)]\n",
        "    lda_loglik.append(temp[0].split(\",\")[1:])\n",
        "\n",
        "lda_loglik = lda_loglik[1:]\n",
        "# lda_loglik_df = pd.DataFrame(lda_loglik)\n",
        "# lda_loglik_df.columns = ['word ' + str(int(i / 2)) if i % 2 == 0 else  'weight ' + str(int(i / 2)) for i in range(lda_loglik_df.shape[1])]\n",
        "\n",
        "mallet_lda_topics = [[x for i, x in enumerate(j) if i % 2 == 0] for j in lda_loglik]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(documents, topics, weights):\n",
        "    # Initialize distribution matrix\n",
        "    distribution = np.zeros((len(documents), len(topics)))\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        # Preprocess document\n",
        "        document = document.translate(str.maketrans('', '', string.punctuation))\n",
        "        document = document.split(' ')\n",
        "        document = [word.lower() for word in document if len(word.lower()) > 0]\n",
        "\n",
        "        loglik_dicts = []\n",
        "        for k in weights:\n",
        "            loglik_dicts.append({k[j]: float(k[j+1]) for j in range(0, len(k) - 1, 2)})\n",
        "\n",
        "        for j, loglik_dict in enumerate(loglik_dicts):\n",
        "            distribution[i][j] = np.sum([0 if word not in loglik_dict else loglik_dict[word] for word in document])\n",
        "\n",
        "        # Normalize\n",
        "        distribution[i] /= len(document)\n",
        "\n",
        "    return distribution\n",
        "\n",
        "mallet_lda_train_distribution = preprocess([line.strip() for line in review_body], mallet_lda_topics, lda_loglik)\n",
        "mallet_lda_test_distribution = preprocess([line.strip() for line in test_review_body], mallet_lda_topics, lda_loglik)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_regression_model(X, y):\n",
        "    model = LinearRegression()\n",
        "    # model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=True)\n",
        "    rmse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=False)\n",
        "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
        "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "    return mse, rmse, r2, mae\n",
        "\n",
        "    # from sklearn.metrics import accuracy_score\n",
        "    # return accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "def test_ready(data):\n",
        "  documents = [line.strip() for line in data['review_body']]\n",
        "  sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
        "  preprocessed_documents, _, _, retained_indices = sp.preprocess()\n",
        "  labels = temp_test['stars'][retained_indices]\n",
        "  return preprocessed_documents, labels\n",
        "\n",
        "test_preprocessed_documents, test_labels = test_ready(test)\n",
        "\n",
        "def results(topics, weights, flag=False):\n",
        "  if flag:\n",
        "    X = np.array([labels.mean()] * len(labels)).reshape(-1, 1)\n",
        "    X_test = np.array([test_labels.mean()] * len(test_labels)).reshape(-1, 1)\n",
        "\n",
        "  else:\n",
        "    # X = preprocess(preprocessed_documents, topics, weights)\n",
        "    # X = preprocess([line.strip() for line in review_body], topics, weights)\n",
        "    X = mallet_lda_train_distribution\n",
        "    # X_test = preprocess(test_preprocessed_documents, topics, weights)\n",
        "    # X_test = preprocess([line.strip() for line in test_review_body], topics, weights)\n",
        "    X_test = mallet_lda_test_distribution\n",
        "\n",
        "  # y = labels\n",
        "  y = temp_train['stars']\n",
        "  # y_test = test_labels\n",
        "  y_test = temp_test['stars']\n",
        "  model = train_regression_model(X, y)\n",
        "\n",
        "  mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, X_test, y_test)\n",
        "  mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, X, y)\n",
        "\n",
        "  # train_acc = evaluate_model(model, X, y)\n",
        "  # test_acc = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "  # print(\"Train Accuracy: \", train_acc)\n",
        "  # print(\"Test Accuracy: \", test_acc)\n",
        "\n",
        "  # return train_acc, test_acc\n",
        "\n",
        "  print(\"Train\")\n",
        "  print(\"R2:\", r2_train)\n",
        "  print(\"MAE:\", mae_train)\n",
        "  print(\"MSE:\", mse_train)\n",
        "  print(\"RMSE:\", rmse_train)\n",
        "\n",
        "  print()\n",
        "\n",
        "  print(\"Test\")\n",
        "  print(\"R2:\", r2_test)\n",
        "  print(\"MAE:\", mae_test)\n",
        "  print(\"MSE:\", mse_test)\n",
        "  print(\"RMSE:\", rmse_test)\n",
        "\n",
        "  return r2_train, mae_train, mse_train, rmse_train, r2_test, mae_test, mse_test, rmse_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "# R2: 0.2887257405209036\n",
        "# MAE: 1.00275569803334\n",
        "# MSE: 1.4340260511700327\n",
        "# RMSE: 1.1975082676833728\n",
        "\n",
        "# Test\n",
        "# R2: 0.26284721220188745\n",
        "# MAE: 1.0191325835730143\n",
        "# MSE: 1.474305575596225\n",
        "# RMSE: 1.2142098564894888"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train\n",
            "R2: 0.2887257405209036\n",
            "MAE: 1.00275569803334\n",
            "MSE: 1.4340260511700327\n",
            "RMSE: 1.1975082676833728\n",
            "\n",
            "Test\n",
            "R2: 0.26284721220188745\n",
            "MAE: 1.0191325835730143\n",
            "MSE: 1.474305575596225\n",
            "RMSE: 1.2142098564894888\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.2887257405209036,\n",
              " 1.00275569803334,\n",
              " 1.4340260511700327,\n",
              " 1.1975082676833728,\n",
              " 0.26284721220188745,\n",
              " 1.0191325835730143,\n",
              " 1.474305575596225,\n",
              " 1.2142098564894888)"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results(mallet_lda_topics, lda_loglik)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Accuracy:  0.2865\n",
        "# Test Accuracy:  0.2804"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy:  0.2865\n",
            "Test Accuracy:  0.2804\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.2865, 0.2804)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results(mallet_lda_topics, lda_loglik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCroeZXlgcMv"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Toou_cqtlWv"
      },
      "outputs": [],
      "source": [
        "b_r2_train, b_mae_train, b_mse_train, b_rmse_train, b_r2_test, b_mae_test, b_mse_test, b_rmse_test = results(lda_topics, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seKgX_3d17z9"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vHvXo6MsGpJ"
      },
      "outputs": [],
      "source": [
        "lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test = results(lda_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBcId2fqd9qh"
      },
      "outputs": [],
      "source": [
        "pd.concat([pd.Series([lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test])], axis = 1).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwdN-c_U14mH"
      },
      "source": [
        "##CTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYX04Sx3sa7a"
      },
      "outputs": [],
      "source": [
        "ctm_r2_train, ctm_mae_train, ctm_mse_train, ctm_rmse_train, ctm_r2_test, ctm_mae_test, ctm_mse_test, ctm_rmse_test = results(ctm_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B43HKbgWexe0"
      },
      "outputs": [],
      "source": [
        "pd.concat([pd.Series([ctm_r2_train, ctm_mae_train, ctm_mse_train, ctm_rmse_train, ctm_r2_test, ctm_mae_test, ctm_mse_test, ctm_rmse_test])], axis = 1).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nya0LM5OkBHs"
      },
      "source": [
        "Using paraphrase-multilingual-mpnet-base-v2, 200 topics, 50 epochs (best one so far):\n",
        "\n",
        "* MSE: 1.0463393882784946\n",
        "* RMSE: 1.0229073214512128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt60-STspjK3"
      },
      "source": [
        "##BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.03982321, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.08145821, 0.49982888, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_topic_distribution_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train\n",
            "R2: 0.21279826767450016\n",
            "MAE: 1.054235544333614\n",
            "MSE: 1.5870375219991502\n",
            "RMSE: 1.259776774670477\n",
            "\n",
            "Test\n",
            "R2: -0.19955535661612278\n",
            "MAE: 1.3092946685216322\n",
            "MSE: 2.39886926289336\n",
            "RMSE: 1.5488283516559735\n"
          ]
        }
      ],
      "source": [
        "y = labels\n",
        "y_test = test_labels\n",
        "model = train_regression_model(bert_topic_distribution, y)\n",
        "\n",
        "mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, bert_topic_distribution_test, y_test)\n",
        "mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, bert_topic_distribution, y)\n",
        "\n",
        "print(\"Train\")\n",
        "print(\"R2:\", r2_train)\n",
        "print(\"MAE:\", mae_train)\n",
        "print(\"MSE:\", mse_train)\n",
        "print(\"RMSE:\", rmse_train)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Test\")\n",
        "print(\"R2:\", r2_test)\n",
        "print(\"MAE:\", mae_test)\n",
        "print(\"MSE:\", mse_test)\n",
        "print(\"RMSE:\", rmse_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ll74AMdph-s"
      },
      "outputs": [],
      "source": [
        "bert_r2_train, bert_mae_train, bert_mse_train, bert_rmse_train, bert_r2_test, bert_mae_test, bert_mse_test, bert_rmse_test = results(bert_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-onlxv-7e2kk"
      },
      "outputs": [],
      "source": [
        "pd.concat([pd.Series([bert_r2_train, bert_mae_train, bert_mse_train, bert_rmse_train, bert_r2_test, bert_mae_test, bert_mse_test, bert_rmse_test])], axis = 1).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9jWv1XuJnMv"
      },
      "source": [
        "#Printing out topics and LR coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjQ-NsWxNzSf"
      },
      "outputs": [],
      "source": [
        "def topic_LR_coef(model, topics):\n",
        "  x1 = []\n",
        "  x2 = pd.Series(model.coef_)\n",
        "\n",
        "  for i, topic in enumerate(topics):\n",
        "    x1.append(\", \".join(topic))\n",
        "\n",
        "  x1 = pd.Series(x1)\n",
        "  return (pd.concat([x1, x2], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCu4BygLCR7e"
      },
      "outputs": [],
      "source": [
        "lda_model = train_regression_model(preprocess(preprocessed_documents, lda_topics), labels)\n",
        "ctm_model = train_regression_model(preprocess(preprocessed_documents, ctm_topics), labels)\n",
        "bert_model = train_regression_model(preprocess(preprocessed_documents, bert_topics), labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrYRf_95GGHE"
      },
      "outputs": [],
      "source": [
        "lda_LR_coef = topic_LR_coef(lda_model, lda_topics)\n",
        "ctm_LR_coef = topic_LR_coef(ctm_model, ctm_topics)\n",
        "bert_LR_coef = topic_LR_coef(bert_model, bert_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EbxEc3hDSM-"
      },
      "outputs": [],
      "source": [
        "lda_LR_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzb5_OnwN3_f"
      },
      "outputs": [],
      "source": [
        "ctm_LR_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R0ZKUQBN4vf"
      },
      "outputs": [],
      "source": [
        "bert_LR_coef"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GPJW1I8d0RLC",
        "jdrTu5G70cH0",
        "P6XQTwmDMyBY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09a11feda42f4d6ba5e008e767db9697": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16a84666a9642c882d8482ef9a63c20",
            "placeholder": "​",
            "style": "IPY_MODEL_db532f2eafbf4aaeb4596a938d606543",
            "value": " 3/0 [00:00&lt;00:00, 76.32 examples/s]"
          }
        },
        "1ea1810429f741709e31785ad5ff4722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30807e5b126248d2a445f0a0ee7e591b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8c5467e7f7994fdaa72fe05debdacf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30807e5b126248d2a445f0a0ee7e591b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c37c202c50564d5a8feefa849bba962e",
            "value": 1
          }
        },
        "a3f38945aa624fd992e9f6e81b56ea2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7893bbd0a80417186a1728ff855ac1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1a362062b94243946dd4d10a450c0f",
            "placeholder": "​",
            "style": "IPY_MODEL_1ea1810429f741709e31785ad5ff4722",
            "value": "Generating train split: "
          }
        },
        "c37c202c50564d5a8feefa849bba962e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c70668e3494c42bcbdc58563ec598c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7893bbd0a80417186a1728ff855ac1f",
              "IPY_MODEL_8c5467e7f7994fdaa72fe05debdacf78",
              "IPY_MODEL_09a11feda42f4d6ba5e008e767db9697"
            ],
            "layout": "IPY_MODEL_a3f38945aa624fd992e9f6e81b56ea2d"
          }
        },
        "db532f2eafbf4aaeb4596a938d606543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f16a84666a9642c882d8482ef9a63c20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1a362062b94243946dd4d10a450c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
