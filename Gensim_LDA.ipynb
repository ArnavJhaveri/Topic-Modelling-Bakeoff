{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BUw5EQi8v9r1",
        "outputId": "4adb927e-4343-4795-96fe-f410b213e7ca"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Dataset passed in must be a CSV file that can be opened using pd.read_csv()\n",
        "2. perc determines how much of the dataset is used\n",
        "3. num_topics_word is set to 500 top words\n",
        "4. num_topics_list is set to [100] but can be changed or appended to, to get more results\n",
        "5. text_column, label_column, and id_column must all be set according to your database\n",
        "  5.1 text_column is the actual text\n",
        "  5.2 label_column is what you regress over / predict\n",
        "  5.3 id_column is the unique identifier for each message\n",
        "6. dataset_path must be set to where the dataset is stored\n",
        "7. the name of the folder (be sure to include '/' at the end) -- usually the name of the dataset (i.e. twitter, amazon, etc.)\n",
        "\"\"\"\n",
        "\n",
        "# !pip install datasets\n",
        "# !pip install pyldavis\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords as stop_words\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "from gensim.utils import deaccent\n",
        "import warnings\n",
        "import scipy.sparse\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "is_multi = False\n",
        "is_combined = not is_multi\n",
        "perc = 0.01 # % of dataset used\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "language = 'en'\n",
        "stopwords = list(stop_words.words('english'))\n",
        "\n",
        "num_topics_word = 500\n",
        "num_topics_list = [100]\n",
        "\n",
        "# change these accordingly\n",
        "text_column = 'message'\n",
        "label_column = 'age'\n",
        "id_column = 'Unnamed: 0'\n",
        "dataset_path = '/sandata/arnav/twitter_df.csv'\n",
        "folder = 'twitter/'\n",
        "\n",
        "dataset = pd.read_csv(dataset_path)[[id_column, text_column, label_column]].rename(columns = {id_column: 'message_id', text_column: 'message', label_column: 'label'}).dropna()\n",
        "\n",
        "# shuffles to avoid biases in data\n",
        "arr = sklearn.utils.shuffle(np.arange(len(dataset)), random_state=42)[0:int(len(dataset) * perc)]\n",
        "dataset = dataset.iloc[arr].reset_index(drop=True)\n",
        "\n",
        "train, test, message, test_message = train_test_split(dataset, dataset['message'], test_size = 0.20, random_state = 42)\n",
        "\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)\n",
        "message.reset_index(drop=True, inplace=True)\n",
        "test_message.reset_index(drop=True, inplace=True)\n",
        "\n",
        "message = list(message)\n",
        "test_message = list(test_message)\n",
        "\n",
        "class WhiteSpacePreprocessingStopwords():\n",
        "    \"\"\"\n",
        "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, documents, stopwords_list=None, vocabulary_size=2000, max_df=1.0, min_words=1,\n",
        "                 remove_numbers=True):\n",
        "        \"\"\"\n",
        "\n",
        "        :param documents: list of strings\n",
        "        :param stopwords_list: list of the stopwords to remove\n",
        "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
        "        :param max_df : float or int, default=1.0\n",
        "        When building the vocabulary ignore terms that have a document\n",
        "        frequency strictly higher than the given threshold (corpus-specific\n",
        "        stop words).\n",
        "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
        "        documents, integer absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "        :param min_words: int, default=1. Documents with less words than the parameter\n",
        "        will be removed\n",
        "        :param remove_numbers: bool, default=True. If true, numbers are removed from docs\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        if stopwords_list is not None:\n",
        "            self.stopwords = set(stopwords_list)\n",
        "        else:\n",
        "            self.stopwords = []\n",
        "\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.max_df = max_df\n",
        "        self.min_words = min_words\n",
        "        self.remove_numbers = remove_numbers\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"\n",
        "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
        "        list of unpreprocessed documents.\n",
        "\n",
        "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
        "        \"\"\"\n",
        "        preprocessed_docs_tmp = self.documents\n",
        "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [doc.translate(\n",
        "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
        "        if self.remove_numbers:\n",
        "            preprocessed_docs_tmp = [doc.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
        "                                     for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, max_df=self.max_df)\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
        "\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0 and len(doc) >= self.min_words:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(self.documents[i])\n",
        "                retained_indices.append(i)\n",
        "\n",
        "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
        "\n",
        "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
        "\n",
        "def get_bag_of_words(data, min_length):\n",
        "    \"\"\"\n",
        "    Creates the bag of words\n",
        "    \"\"\"\n",
        "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
        "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
        "\n",
        "    vect = scipy.sparse.csr_matrix(vect)\n",
        "    return vect\n",
        "\n",
        "\n",
        "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from an input file, assumes one document per line\n",
        "    \"\"\"\n",
        "\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    with open(text_file, encoding=\"utf-8\") as filino:\n",
        "        texts = list(map(lambda x: x, filino.readlines()))\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Creates SBERT Embeddings from a list\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(sbert_model_to_load)\n",
        "\n",
        "    if max_seq_length is not None:\n",
        "        model.max_seq_length = max_seq_length\n",
        "\n",
        "    check_max_local_length(max_seq_length, texts)\n",
        "\n",
        "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
        "\n",
        "\n",
        "def check_max_local_length(max_seq_length, texts):\n",
        "    max_local_length = np.max([len(t.split()) for t in texts])\n",
        "    if max_local_length > max_seq_length:\n",
        "        warnings.simplefilter('always', DeprecationWarning)\n",
        "        warnings.warn(f\"the longest document in your collection has {max_local_length} words, the model instead \"\n",
        "                      f\"truncates to {max_seq_length} tokens.\")\n",
        "\n",
        "\n",
        "class TopicModelDataPreparation:\n",
        "\n",
        "    def __init__(self, contextualized_model=None, show_warning=True, max_seq_length=128):\n",
        "        self.contextualized_model = contextualized_model\n",
        "        self.vocab = []\n",
        "        self.id2token = {}\n",
        "        self.vectorizer = None\n",
        "        self.label_encoder = None\n",
        "        self.show_warning = show_warning\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def load(self, contextualized_embeddings, bow_embeddings, id2token, labels=None):\n",
        "        return CTMDataset(\n",
        "            X_contextual=contextualized_embeddings, X_bow=bow_embeddings, idx2token=id2token, labels=labels)\n",
        "\n",
        "    def fit(self, text_for_contextual, text_for_bow, labels=None, custom_embeddings=None):\n",
        "        \"\"\"\n",
        "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "            if type(custom_embeddings).__module__ != 'numpy':\n",
        "                raise TypeError(\"contextualized_embeddings must be a numpy.ndarray type object\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None and custom_embeddings is None:\n",
        "            raise Exception(\"A contextualized model or contextualized embeddings must be defined\")\n",
        "\n",
        "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
        "\n",
        "        # if the user is passing custom embeddings we don't need to create the embeddings using the model\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            train_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            train_contextualized_embeddings = custom_embeddings\n",
        "        self.vocab = self.vectorizer.get_feature_names_out()\n",
        "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
        "\n",
        "        if labels:\n",
        "            self.label_encoder = OneHotEncoder()\n",
        "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "        return CTMDataset(\n",
        "            X_contextual=train_contextualized_embeddings, X_bow=train_bow_embeddings,\n",
        "            idx2token=self.id2token, labels=encoded_labels)\n",
        "\n",
        "    def transform(self, text_for_contextual, text_for_bow=None, custom_embeddings=None, labels=None):\n",
        "        \"\"\"\n",
        "        This method create the input for the prediction. Essentially, it creates the embeddings with the contextualized\n",
        "        model of choice and with trained vectorizer.\n",
        "\n",
        "        If text_for_bow is missing, it should be because we are using ZeroShotTM\n",
        "\n",
        "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
        "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
        "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
        "        :param labels: list of labels associated with each document (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if custom_embeddings is not None:\n",
        "            assert len(text_for_contextual) == len(custom_embeddings)\n",
        "\n",
        "            if text_for_bow is not None:\n",
        "                assert len(custom_embeddings) == len(text_for_bow)\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            assert len(text_for_contextual) == len(text_for_bow)\n",
        "\n",
        "        if self.contextualized_model is None:\n",
        "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
        "\n",
        "        if text_for_bow is not None:\n",
        "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
        "        else:\n",
        "            # dummy matrix\n",
        "            if self.show_warning:\n",
        "                warnings.simplefilter('always', DeprecationWarning)\n",
        "                warnings.warn(\n",
        "                    \"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n",
        "                    \"are using ZeroShotTM in a cross-lingual setting\")\n",
        "\n",
        "            # we just need an object that is matrix-like so that pytorch does not complain\n",
        "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
        "\n",
        "        if custom_embeddings is None:\n",
        "            test_contextualized_embeddings = bert_embeddings_from_list(\n",
        "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
        "        else:\n",
        "            test_contextualized_embeddings = custom_embeddings\n",
        "\n",
        "        if labels:\n",
        "            encoded_labels = self.label_encoder.transform(np.array([labels]).reshape(-1, 1))\n",
        "        else:\n",
        "            encoded_labels = None\n",
        "\n",
        "        return CTMDataset(X_contextual=test_contextualized_embeddings, X_bow=test_bow_embeddings,\n",
        "                          idx2token=self.id2token, labels=encoded_labels)\n",
        "\n",
        "documents = [line.strip() for line in (message + test_message) if not isinstance(line, float)]\n",
        "test_documents = [line.strip() for line in test_message if not isinstance(line, float)]\n",
        "\n",
        "sp_train = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords)\n",
        "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp_train.preprocess()\n",
        "labels = pd.concat([train, test]).reset_index()['label'][retained_indices]\n",
        "\n",
        "sp_test = WhiteSpacePreprocessingStopwords(test_documents, stopwords_list=stopwords)\n",
        "test_preprocessed_documents, test_unpreprocessed_corpus, test_vocab, test_retained_indices = sp_test.preprocess()\n",
        "test_labels = test['label'][test_retained_indices]\n",
        "\n",
        "mallet_stopwords = []\n",
        "loc3 = \"dlatk/amazon_eng/9f822b_stopwords\"\n",
        "with open(loc3) as f:\n",
        "  for line in f:\n",
        "    mallet_stopwords.append(line.strip())\n",
        "\n",
        "def preprocess(documents, topics, weights):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        documents: list of documents (each element in the list is a string) that the topics were extracted from\n",
        "        topics: list of list of topics from the model of choice\n",
        "        weights: list of dictionary mappings (word: weight)\n",
        "\n",
        "    RETURN\n",
        "        topic distribution\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize distribution matrix\n",
        "    distribution = np.zeros((len(documents), len(topics)))\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        # Preprocess document\n",
        "        document = document.translate(str.maketrans('', '', string.punctuation)) # removing periods, commas, etc\n",
        "        document = document.split(' ') # split on spaces\n",
        "        document = [word.lower() for word in document if len(word.lower()) > 0] # lower case everything since all topics are lower case\n",
        "\n",
        "        for j, loglik_dict in enumerate(weights):\n",
        "            distribution[i][j] = np.sum([0 if word not in loglik_dict else loglik_dict[word] for word in document]) # if word exists then its weight else 0\n",
        "\n",
        "        # Normalize\n",
        "        distribution[i] /= (len(document) + 2) # +2 for some weird reason ?\n",
        "        \n",
        "    return distribution\n",
        "\n",
        "\n",
        "def train_regression_model(X, y):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        X: distribution from preprocess() above\n",
        "        y: labels\n",
        "\n",
        "    RETURN\n",
        "        LR model   \n",
        "    \"\"\"\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        model: model trained from train_regression_model()\n",
        "        X_test: distribution you want to make predictions on\n",
        "        y_test: the true labels for the X_test passed in\n",
        "\n",
        "    RETURN\n",
        "        MSE: MSE of the (X_test, y_test) data inputted\n",
        "        RMSE: RMSE of the (X_test, y_test) data inputted\n",
        "        R2: R2 of the (X_test, y_test) data inputted\n",
        "        MAE: MAE of the (X_test, y_test) data inputted\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=True)\n",
        "    rmse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=False)\n",
        "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
        "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "    return mse, rmse, r2, mae\n",
        "\n",
        "\n",
        "def results(topics, weights, flag=False):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "        topics: list of list of topics\n",
        "        weights: list of dictionary mappings (word: weight)\n",
        "        flag: True if baseline (default is False\n",
        "\n",
        "    RETURN\n",
        "        r2_train: R2 on train set\n",
        "        mae_train: MAE on train set\n",
        "        mse_train: MSE on train set\n",
        "        rmse_train: RMSE on train set\n",
        "        r2_test: R2 on test set\n",
        "        mae_test: MAE on test set\n",
        "        mse_test: MSE on test set\n",
        "        rmse_test: RMSE on test set\n",
        "    \"\"\"\n",
        "\n",
        "    if flag:\n",
        "        X = np.array([labels.mean()] * len(labels)).reshape(-1, 1)\n",
        "        X_test = np.array([test_labels.mean()] * len(test_labels)).reshape(-1, 1)\n",
        "\n",
        "    else:\n",
        "        X = preprocess([line.strip() for line in message if not isinstance(line, float)], topics, weights)\n",
        "        X_test = preprocess([line.strip() for line in test_message if not isinstance(line, float)], topics, weights)\n",
        "\n",
        "    y = train['label']\n",
        "    y_test = test['label']\n",
        "    model = train_regression_model(X, y)\n",
        "\n",
        "    mse_test, rmse_test, r2_test, mae_test = evaluate_model(model, X_test, y_test)\n",
        "    mse_train, rmse_train, r2_train, mae_train = evaluate_model(model, X, y)\n",
        "\n",
        "    return r2_train, mae_train, mse_train, rmse_train, r2_test, mae_test, mse_test, rmse_test\n",
        "\n",
        "hyperparams = {\n",
        "  'num_topics': num_topics_list,\n",
        "  'num_top_words': num_topics_word,\n",
        "  'percentage_of_dataset': perc,\n",
        "  'text_column': text_column,\n",
        "  'label_column': label_column,\n",
        "  'id_column': id_column,\n",
        "}\n",
        "\n",
        "# make new directory\n",
        "try:\n",
        "    os.makedirs(folder + 'GensimLDA')\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "      raise\n",
        "\n",
        "# write hyperparams to file\n",
        "import json\n",
        "with open(folder + 'GensimLDA/hyperparams.txt', 'w') as f:\n",
        "  f.write(json.dumps(hyperparams))\n",
        "\n",
        "# write stopwords to file\n",
        "with open(folder + 'GensimLDA/stopwords.txt', 'w') as f:\n",
        "    for item in mallet_stopwords:\n",
        "        f.write(item + \", \")\n",
        "\n",
        "for num_topics in num_topics_list:\n",
        "    print(\"Start \" + str(num_topics))\n",
        "\n",
        "    split_preprocessed_documents = [d.split() for d in preprocessed_documents]\n",
        "    dictionary = Dictionary(split_preprocessed_documents)\n",
        "    corpus = [dictionary.doc2bow(text) for text in split_preprocessed_documents]\n",
        "\n",
        "    lda = LdaModel(corpus, num_topics=num_topics, iterations=500, random_state=42, minimum_probability = 0)\n",
        "\n",
        "    def get_topics_lda(topk=10):\n",
        "        topic_terms = []\n",
        "        for i in range(num_topics):\n",
        "            topic_words_dict = {}\n",
        "            for word_tuple in lda.get_topic_terms(i, topk):\n",
        "                if dictionary[word_tuple[0]] not in mallet_stopwords:\n",
        "                    topic_words_dict[dictionary[word_tuple[0]]] = word_tuple[1]\n",
        "            topic_terms.append(topic_words_dict)\n",
        "        return topic_terms\n",
        "\n",
        "    lda_loglik = get_topics_lda(num_topics_word)\n",
        "    lda_topics = [list(lda_loglik[i].keys()) for i in range(len(lda_loglik))]\n",
        "    \n",
        "    lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test = results(lda_topics, lda_loglik)\n",
        "\n",
        "    final_df = pd.concat([pd.Series([lda_r2_train, lda_mae_train, lda_mse_train, lda_rmse_train, lda_r2_test, lda_mae_test, lda_mse_test, lda_rmse_test])], axis = 1).T\n",
        "    final_df.columns = ['Train R2', 'Train MAE', 'Train MSE', 'Train RMSE', 'Test R2', 'Test MAE', 'Test MSE', 'Test RMSE']\n",
        "    display(final_df)\n",
        "\n",
        "    lda_regression = train_regression_model(preprocess([line.strip() for line in message], lda_topics, lda_loglik), train['label'])\n",
        "\n",
        "    with open(folder + 'GensimLDA/LR_coefficients_' + str(num_topics) + '.txt', 'w') as f:\n",
        "        for item in lda_regression.coef_:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "    \n",
        "    final_df.to_csv(folder + 'GensimLDA/GensimLDA_df_' + str(num_topics) + '.csv')\n",
        "\n",
        "    with open(folder + 'GensimLDA/topics_' + str(num_topics) + '.txt', 'w') as f:\n",
        "        for sublist in lda_topics:\n",
        "            for i in sublist:\n",
        "                if i not in mallet_stopwords:\n",
        "                    f.write(i + \", \")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(\"End \" + str(num_topics))\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GPJW1I8d0RLC",
        "jdrTu5G70cH0",
        "P6XQTwmDMyBY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09a11feda42f4d6ba5e008e767db9697": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16a84666a9642c882d8482ef9a63c20",
            "placeholder": "​",
            "style": "IPY_MODEL_db532f2eafbf4aaeb4596a938d606543",
            "value": " 3/0 [00:00&lt;00:00, 76.32 examples/s]"
          }
        },
        "1ea1810429f741709e31785ad5ff4722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30807e5b126248d2a445f0a0ee7e591b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8c5467e7f7994fdaa72fe05debdacf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30807e5b126248d2a445f0a0ee7e591b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c37c202c50564d5a8feefa849bba962e",
            "value": 1
          }
        },
        "a3f38945aa624fd992e9f6e81b56ea2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7893bbd0a80417186a1728ff855ac1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1a362062b94243946dd4d10a450c0f",
            "placeholder": "​",
            "style": "IPY_MODEL_1ea1810429f741709e31785ad5ff4722",
            "value": "Generating train split: "
          }
        },
        "c37c202c50564d5a8feefa849bba962e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c70668e3494c42bcbdc58563ec598c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7893bbd0a80417186a1728ff855ac1f",
              "IPY_MODEL_8c5467e7f7994fdaa72fe05debdacf78",
              "IPY_MODEL_09a11feda42f4d6ba5e008e767db9697"
            ],
            "layout": "IPY_MODEL_a3f38945aa624fd992e9f6e81b56ea2d"
          }
        },
        "db532f2eafbf4aaeb4596a938d606543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f16a84666a9642c882d8482ef9a63c20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1a362062b94243946dd4d10a450c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
